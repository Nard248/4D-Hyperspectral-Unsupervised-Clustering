{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T17:30:43.465754Z",
     "start_time": "2025-04-26T17:30:42.043880Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def update_hyperspectral_loader_class():\n",
    "    \"\"\"\n",
    "    Apply a patch to the HyperspectralDataLoader class to update the apply_spectral_cutoff method.\n",
    "    This will replace the existing method with the dual cutoff version.\n",
    "\n",
    "    Must be called before using the HyperspectralDataLoader class.\n",
    "    \"\"\"\n",
    "    from HyperspectralDataLoader import HyperspectralDataLoader\n",
    "\n",
    "    # Replace the apply_spectral_cutoff method with our new implementation\n",
    "    def new_apply_spectral_cutoff(self, data, wavelengths, excitation):\n",
    "        \"\"\"\n",
    "        Apply both Rayleigh and second-order spectral cutoffs.\n",
    "        \"\"\"\n",
    "        # Convert wavelengths to numpy array if it's not already\n",
    "        wavelengths_arr = np.array(wavelengths)\n",
    "\n",
    "        # Create a mask to keep valid wavelengths\n",
    "        keep_mask = np.ones(len(wavelengths_arr), dtype=bool)\n",
    "\n",
    "        # 1. Apply Rayleigh cutoff - remove wavelengths below (excitation + rayleigh_offset)\n",
    "        rayleigh_cutoff = excitation + self.cutoff_offset  # Use the same offset parameter for both cutoffs\n",
    "        rayleigh_mask = wavelengths_arr >= rayleigh_cutoff\n",
    "        keep_mask = np.logical_and(keep_mask, rayleigh_mask)\n",
    "\n",
    "        # 2. Apply second-order cutoff - remove wavelengths in (2*excitation ± cutoff_offset)\n",
    "        second_order_min = 2 * excitation - self.cutoff_offset\n",
    "        second_order_max = 2 * excitation + self.cutoff_offset\n",
    "        second_order_mask = np.logical_or(wavelengths_arr < second_order_min, wavelengths_arr > second_order_max)\n",
    "        keep_mask = np.logical_and(keep_mask, second_order_mask)\n",
    "\n",
    "        # Apply the combined mask to the third dimension (emission wavelengths)\n",
    "        filtered_data = data[:, :, keep_mask]\n",
    "        filtered_wavelengths = wavelengths_arr[keep_mask].tolist()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Applied dual cutoff for excitation {excitation}nm\")\n",
    "            print(f\"Removed wavelengths below {rayleigh_cutoff}nm (Rayleigh cutoff)\")\n",
    "            print(f\"Removed wavelengths between {second_order_min}nm and {second_order_max}nm (second-order cutoff)\")\n",
    "            print(f\"Original data shape: {data.shape}, filtered shape: {filtered_data.shape}\")\n",
    "\n",
    "        return filtered_data, filtered_wavelengths\n",
    "\n",
    "    # Patch the method in the class\n",
    "    HyperspectralDataLoader.apply_spectral_cutoff = new_apply_spectral_cutoff\n",
    "\n",
    "    print(\"HyperspectralDataLoader.apply_spectral_cutoff method has been updated to apply dual cutoff\")\n",
    "\n",
    "\n",
    "def read_laser_power_excel(excel_path: str) -> Dict[float, float]:\n",
    "    \"\"\"\n",
    "    Read laser power measurements from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        excel_path: Path to the Excel file containing excitation wavelengths and power measurements\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping excitation wavelengths to power values\n",
    "    \"\"\"\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Extract the columns (adjust column names if needed)\n",
    "    if \"Excitation Wavelength (nm)\" in df.columns and \"Average Power (W)\" in df.columns:\n",
    "        excitation_col = \"Excitation Wavelength (nm)\"\n",
    "        power_col = \"Average Power (W)\"\n",
    "    elif len(df.columns) >= 2:\n",
    "        # If column names don't match, use the first two columns\n",
    "        excitation_col = df.columns[0]\n",
    "        power_col = df.columns[1]\n",
    "    else:\n",
    "        raise ValueError(\"Excel file doesn't have expected columns\")\n",
    "\n",
    "    # Create dictionary mapping wavelengths to power\n",
    "    power_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        excitation = float(row[excitation_col])\n",
    "        power = float(row[power_col])\n",
    "        power_dict[excitation] = power\n",
    "\n",
    "    return power_dict\n",
    "\n",
    "\n",
    "def normalize_hyperspectral_data_by_laser_power(\n",
    "    data_dict: Dict,\n",
    "    laser_powers: Dict[float, float],\n",
    "    reference_type: str = 'max',\n",
    "    output_file: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Normalize hyperspectral data based on laser powers from Excel file.\n",
    "\n",
    "    Args:\n",
    "        data_dict: Dictionary containing hyperspectral data\n",
    "        laser_powers: Dictionary mapping excitation wavelengths to power values\n",
    "        reference_type: Type of reference to use ('max', 'min', 'mean', or a float value)\n",
    "        output_file: Path to save the normalized data pickle file (optional)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing normalized hyperspectral data\n",
    "    \"\"\"\n",
    "    print(\"Normalizing hyperspectral data based on laser power...\")\n",
    "\n",
    "    # Create a deep copy of the data to avoid modifying the original\n",
    "    import copy\n",
    "    normalized_data = copy.deepcopy(data_dict)\n",
    "\n",
    "    # Determine the reference power\n",
    "    if reference_type == 'max':\n",
    "        reference_power = max(laser_powers.values())\n",
    "        print(f\"Using maximum laser power as reference: {reference_power:.8f} W\")\n",
    "    elif reference_type == 'min':\n",
    "        reference_power = min(laser_powers.values())\n",
    "        print(f\"Using minimum laser power as reference: {reference_power:.8f} W\")\n",
    "    elif reference_type == 'mean':\n",
    "        reference_power = sum(laser_powers.values()) / len(laser_powers)\n",
    "        print(f\"Using mean laser power as reference: {reference_power:.8f} W\")\n",
    "    elif isinstance(reference_type, (int, float)):\n",
    "        reference_power = float(reference_type)\n",
    "        print(f\"Using provided power as reference: {reference_power:.8f} W\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reference_type. Use 'max', 'min', 'mean', or a float value.\")\n",
    "\n",
    "    # Store the normalization information in metadata\n",
    "    if 'metadata' not in normalized_data:\n",
    "        normalized_data['metadata'] = {}\n",
    "\n",
    "    normalized_data['metadata']['laser_power_normalization'] = {\n",
    "        'reference_type': reference_type,\n",
    "        'reference_power': reference_power,\n",
    "        'laser_powers': laser_powers\n",
    "    }\n",
    "\n",
    "    # Plot powers and normalization factors for visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: Original laser powers\n",
    "    plt.subplot(2, 1, 1)\n",
    "    x = sorted(laser_powers.keys())\n",
    "    y = [laser_powers[k] for k in x]\n",
    "    plt.plot(x, y, 'o-', color='blue')\n",
    "    plt.xlabel('Excitation Wavelength (nm)')\n",
    "    plt.ylabel('Laser Power (W)')\n",
    "    plt.title(f'Laser Power vs Excitation Wavelength')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Normalization factors\n",
    "    plt.subplot(2, 1, 2)\n",
    "    factors = [reference_power / laser_powers[k] for k in x]\n",
    "    plt.plot(x, factors, 'o-', color='red')\n",
    "    plt.xlabel('Excitation Wavelength (nm)')\n",
    "    plt.ylabel('Normalization Factor')\n",
    "    plt.title(f'Normalization Factors ({reference_type} reference)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output file is provided\n",
    "    if output_file:\n",
    "        plot_file = str(Path(output_file).with_suffix('.png'))\n",
    "        plt.savefig(plot_file)\n",
    "        print(f\"Normalization plot saved to: {plot_file}\")\n",
    "\n",
    "    # Normalize each data cube\n",
    "    print(\"Normalizing data cubes...\")\n",
    "    for ex_str in data_dict['data'].keys():\n",
    "        excitation = float(ex_str)\n",
    "\n",
    "        # Check if we have laser power for this excitation\n",
    "        if excitation in laser_powers:\n",
    "            laser_power = laser_powers[excitation]\n",
    "\n",
    "            # Calculate normalization factor: reference_power / laser_power\n",
    "            # This compensates for variations in laser power\n",
    "            normalization_factor = reference_power / laser_power\n",
    "\n",
    "            # Apply normalization to the data cube\n",
    "            original_cube = data_dict['data'][ex_str]['cube']\n",
    "            normalized_data['data'][ex_str]['cube'] = original_cube * normalization_factor\n",
    "\n",
    "            # Store normalization factor in metadata\n",
    "            normalized_data['data'][ex_str]['laser_power_normalization_factor'] = normalization_factor\n",
    "\n",
    "            print(f\"  Normalized excitation {ex_str}nm (Power: {laser_power:.8f} W, Factor: {normalization_factor:.4f})\")\n",
    "        else:\n",
    "            print(f\"  ⚠ No laser power data for excitation {ex_str}nm\")\n",
    "\n",
    "    # Save the normalized data if output file is provided\n",
    "    if output_file:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(normalized_data, f)\n",
    "        print(f\"Normalized data saved to {output_file}\")\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def process_with_dual_cutoff_and_power_normalization(\n",
    "    data_path: str,\n",
    "    metadata_path: str,\n",
    "    laser_power_excel: str,\n",
    "    cutoff_offset: int = 30,\n",
    "    reference_types: List[str] = ['max', 'min', 'mean'],\n",
    "    output_dir: Optional[str] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Process hyperspectral data with both Rayleigh and second-order cutoffs\n",
    "    and normalize by laser power from Excel file using multiple reference types.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the directory containing .im3 files\n",
    "        metadata_path: Path to the Excel file with exposure metadata\n",
    "        laser_power_excel: Path to the Excel file with laser power measurements\n",
    "        cutoff_offset: Offset in nm for both Rayleigh and second-order cutoffs\n",
    "        reference_types: List of reference types for normalization ('max', 'min', 'mean')\n",
    "        output_dir: Directory to save the output files (default: 'processed_data')\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping reference types to their output file paths\n",
    "    \"\"\"\n",
    "    # First update the HyperspectralDataLoader class to use our new dual cutoff method\n",
    "    update_hyperspectral_loader_class()\n",
    "\n",
    "    from HyperspectralDataLoader import HyperspectralDataLoader\n",
    "\n",
    "    # Create output directory if needed\n",
    "    if output_dir is None:\n",
    "        output_dir = Path(\"processed_data\")\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. Process data with dual cutoff\n",
    "    print(f\"Processing data with dual cutoff (offset: {cutoff_offset}nm)...\")\n",
    "\n",
    "    loader = HyperspectralDataLoader(\n",
    "        data_path=data_path,\n",
    "        metadata_path=metadata_path,\n",
    "        cutoff_offset=cutoff_offset,\n",
    "        use_fiji=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # This will now use our patched method that applies both cutoffs\n",
    "    loader.load_data(apply_cutoff=True)\n",
    "\n",
    "    # Summary of the processed data\n",
    "    loader.print_summary()\n",
    "\n",
    "    # Save the data with the dual cutoff\n",
    "    cutoff_file = output_dir / f\"data_dual_cutoff_{cutoff_offset}nm.pkl\"\n",
    "    loader.save_to_pkl(str(cutoff_file))\n",
    "\n",
    "    print(f\"Data with dual cutoff (offset: {cutoff_offset}nm) saved to: {cutoff_file}\")\n",
    "\n",
    "    # 2. Normalize by laser power using different reference types\n",
    "    print(\"\\nNormalizing data based on laser power...\")\n",
    "\n",
    "    # Load laser powers from Excel file\n",
    "    laser_powers = read_laser_power_excel(laser_power_excel)\n",
    "    print(f\"Found laser powers for {len(laser_powers)} excitation wavelengths\")\n",
    "\n",
    "    # Load the data we just saved\n",
    "    with open(cutoff_file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    # Process with each reference type\n",
    "    output_files = {'cutoff': str(cutoff_file)}\n",
    "\n",
    "    for ref_type in reference_types:\n",
    "        print(f\"\\nNormalizing with {ref_type} reference...\")\n",
    "        power_file = output_dir / f\"data_dual_cutoff_{cutoff_offset}nm_power_normalized_{ref_type}.pkl\"\n",
    "\n",
    "        normalized_data = normalize_hyperspectral_data_by_laser_power(\n",
    "            data_dict,\n",
    "            laser_powers,\n",
    "            reference_type=ref_type,\n",
    "            output_file=str(power_file)\n",
    "        )\n",
    "\n",
    "        output_files[ref_type] = str(power_file)\n",
    "\n",
    "        # Create dataframe for this normalization\n",
    "        from HyperspectralDataLoader import load_data_and_create_df, save_dataframe\n",
    "\n",
    "        # Create dataframe from power normalized data\n",
    "        df_power = load_data_and_create_df(str(power_file))\n",
    "        power_parquet = str(power_file).replace('.pkl', '.parquet')\n",
    "        save_dataframe(df_power, power_parquet)\n",
    "\n",
    "        print(f\"Power normalized dataframe ({ref_type} reference) saved to: {power_parquet}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"1. Data with dual cutoff (offset: {cutoff_offset}nm): {cutoff_file}\")\n",
    "\n",
    "    for ref_type in reference_types:\n",
    "        print(f\"2. Data normalized by laser power ({ref_type} reference): {output_files[ref_type]}\")\n",
    "\n",
    "    return output_files"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = '../Data/Kiwi'\n",
    "metadata_path = '../Data/Kiwi/metadata.xlsx'\n",
    "laser_power_excel = \"../Data/Kiwi/TLS Scans/wavelength_power_data.xlsx\"\n",
    "\n",
    "output_files = process_with_dual_cutoff_and_power_normalization(\n",
    "    data_path=data_path,\n",
    "    metadata_path=metadata_path,\n",
    "    laser_power_excel=laser_power_excel,\n",
    "    cutoff_offset=30,\n",
    "    reference_types=['max','min','mean'],\n",
    "    output_dir='Data/Kiwi Experiment/pickles'\n",
    ")\n",
    "\n",
    "print(\"\\nCreating dataframes...\")\n",
    "\n",
    "from HyperspectralDataLoader import load_data_and_create_df, save_dataframe\n",
    "\n",
    "df_power = load_data_and_create_df(power_file)\n",
    "power_parquet = power_file.replace('.pkl', '.parquet')\n",
    "save_dataframe(df_power, power_parquet)\n",
    "\n",
    "print(f\"Power normalized dataframe saved to: {power_parquet}\")"
   ],
   "id": "e974f3c427de6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = '../Data/Kiwi'\n",
    "metadata_path = '../Data/Kiwi/metadata.xlsx'\n",
    "laser_power_excel = \"../Data/Kiwi/TLS Scans/wavelength_power_data.xlsx\"\n",
    "\n",
    "output_files = process_with_dual_cutoff_and_power_normalization(\n",
    "    data_path=data_path,\n",
    "    metadata_path=metadata_path,\n",
    "    laser_power_excel=laser_power_excel,\n",
    "    cutoff_offset=30,\n",
    "    reference_types=['max','min','mean'],\n",
    "    output_dir='Data/Kiwi Experiment/pickles'\n",
    ")\n",
    "\n",
    "print(\"\\nCreating dataframes...\")\n",
    "\n",
    "from HyperspectralDataLoader import load_data_and_create_df, save_dataframe\n",
    "\n",
    "df_power = load_data_and_create_df(power_file)\n",
    "power_parquet = power_file.replace('.pkl', '.parquet')\n",
    "save_dataframe(df_power, power_parquet)\n",
    "\n",
    "print(f\"Power normalized dataframe saved to: {power_parquet}\")"
   ],
   "id": "24f111da5d5ff474",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
