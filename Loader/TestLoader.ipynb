{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from HyperspectralDataLoader import HyperspectralDataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "data_path = '../Data/Kiwi'\n",
    "metadata_path = '../Data/Kiwi/metadata.xlsx'\n",
    "\n",
    "loader = HyperspectralDataLoader(\n",
    "    data_path=data_path,\n",
    "    metadata_path=metadata_path,\n",
    "    cutoff_offset=20,\n",
    "    use_fiji=True,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loader.load_data(apply_cutoff=True)\n",
    "\n",
    "loader.print_summary()\n",
    "\n",
    "loader.save_to_pkl('kiwi_processed.pkl')"
   ],
   "id": "40d17e0bfc09e7b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_excitation_emission_dataframe(data_dict: Dict,\n",
    "                                        sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform 4D hyperspectral data into a 2D dataframe.\n",
    "\n",
    "    Args:\n",
    "        data_dict: Dictionary containing hyperspectral data\n",
    "        sample_size: Optional number of random pixels to sample (for large datasets)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with x, y coordinates and intensity values for each valid excitation-emission combination\n",
    "    \"\"\"\n",
    "    # First, collect all valid excitation-emission combinations\n",
    "    valid_combinations = []\n",
    "    all_excitations = []\n",
    "\n",
    "    # Check what excitations we actually have in the data\n",
    "    for ex_str in data_dict['data'].keys():\n",
    "        excitation = float(ex_str)\n",
    "        all_excitations.append(excitation)\n",
    "\n",
    "        # Get the valid emission wavelengths for this excitation\n",
    "        emissions = data_dict['data'][ex_str]['wavelengths']\n",
    "\n",
    "        # Add all valid combinations to our list\n",
    "        for emission in emissions:\n",
    "            col_name = f\"{int(emission)}-{int(excitation)}\"\n",
    "            valid_combinations.append((excitation, emission, col_name))\n",
    "\n",
    "    print(f\"Found {len(all_excitations)} excitation wavelengths\")\n",
    "    print(f\"Generated {len(valid_combinations)} valid excitation-emission combinations\")\n",
    "\n",
    "    # Create an empty dataframe with x, y coordinates\n",
    "    # First, determine the dimensions of our data\n",
    "    first_ex = str(all_excitations[0])\n",
    "    cube_shape = data_dict['data'][first_ex]['cube'].shape\n",
    "    height, width = cube_shape[0], cube_shape[1]\n",
    "\n",
    "    print(f\"Image dimensions: {height} x {width} pixels\")\n",
    "\n",
    "    # Initialize the dataframe with columns for x and y coordinates\n",
    "    total_pixels = height * width\n",
    "\n",
    "    # Create coordinate arrays - this is the correct way to flatten spatial dimensions\n",
    "    # Create a meshgrid of coordinates\n",
    "    y_coords, x_coords = np.mgrid[0:height, 0:width]\n",
    "\n",
    "    # Flatten the coordinates\n",
    "    x_coords = x_coords.flatten()\n",
    "    y_coords = y_coords.flatten()\n",
    "\n",
    "    # Create initial dataframe with coordinates\n",
    "    df = pd.DataFrame({\n",
    "        'x': x_coords,\n",
    "        'y': y_coords\n",
    "    })\n",
    "\n",
    "    # If sample_size is provided, take a random sample of pixels\n",
    "    if sample_size is not None and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Sampled {sample_size} pixels out of {total_pixels}\")\n",
    "\n",
    "    print(f\"Created initial dataframe with {len(df)} rows\")\n",
    "\n",
    "    # Now, fill in the intensity values for each valid combination\n",
    "    for excitation, emission, col_name in valid_combinations:\n",
    "        # Get the data cube for this excitation\n",
    "        ex_str = str(excitation)\n",
    "        cube = data_dict['data'][ex_str]['cube']\n",
    "        wavelengths = data_dict['data'][ex_str]['wavelengths']\n",
    "\n",
    "        # Find the index of this emission wavelength\n",
    "        try:\n",
    "            em_idx = wavelengths.index(emission)\n",
    "\n",
    "            # Extract the intensity values for this emission wavelength\n",
    "            # For the sampled rows only\n",
    "            if sample_size is not None and sample_size < total_pixels:\n",
    "                # Get the x, y coordinates of the sampled pixels\n",
    "                sampled_coords = df[['x', 'y']].values\n",
    "                # Extract intensity values for these coordinates\n",
    "                intensities = [cube[y, x, em_idx] for x, y in zip(sampled_coords[:, 0], sampled_coords[:, 1])]\n",
    "                df[col_name] = intensities\n",
    "            else:\n",
    "                # Extract for all pixels - flatten in the same order as the coordinates\n",
    "                intensities = cube[:, :, em_idx].flatten()\n",
    "                df[col_name] = intensities\n",
    "\n",
    "        except ValueError:\n",
    "            # This emission wavelength doesn't exist for this excitation\n",
    "            # We're skipping it as requested instead of adding NaN values\n",
    "            continue\n",
    "\n",
    "    print(f\"Final dataframe has {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "def load_data_and_create_df(pickle_file: str, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from pickle file and create the dataframe\n",
    "\n",
    "    Args:\n",
    "        pickle_file: Path to the pickle file\n",
    "        sample_size: Optional number of random pixels to sample\n",
    "\n",
    "    Returns:\n",
    "        Transformed dataframe\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    # Create the dataframe\n",
    "    return create_excitation_emission_dataframe(data_dict, sample_size)\n",
    "\n",
    "def save_dataframe(df: pd.DataFrame, output_file: str) -> None:\n",
    "    \"\"\"Save the dataframe to a file\"\"\"\n",
    "    print(f\"Saving dataframe to {output_file}\")\n",
    "\n",
    "    # Determine file extension and save accordingly\n",
    "    ext = Path(output_file).suffix\n",
    "    if ext == '.csv':\n",
    "        df.to_csv(output_file, index=False)\n",
    "    elif ext == '.parquet':\n",
    "        df.to_parquet(output_file, index=False)\n",
    "    elif ext == '.pkl' or ext == '.pickle':\n",
    "        df.to_pickle(output_file)\n",
    "    else:\n",
    "        print(f\"Unrecognized extension {ext}, saving as pickle\")\n",
    "        df.to_pickle(output_file)\n",
    "\n",
    "    print(f\"Saved dataframe with {len(df)} rows and {len(df.columns)} columns\")"
   ],
   "id": "2ff2026bb6d9cc66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pickle_file = \"Data/Normalized/kiwi_processed_normalized_exposure_up.pkl\"  # Update with your file path\n",
    "\n",
    "# 2. Create the dataframe (sample 1000 pixels for large datasets)\n",
    "df = load_data_and_create_df(pickle_file)\n",
    "\n",
    "# 3. Save the result\n",
    "save_dataframe(df, \"Data/parquet-data/kiwi_processed_normalized_exposure_up.parquet\")\n",
    "\n",
    "# 4. Show a sample of the result\n",
    "print(\"\\nSample of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# 5. Show some statistics\n",
    "print(\"\\nDataframe statistics:\")\n",
    "print(f\"Total rows (pixels): {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")"
   ],
   "id": "17c07a9bb779ab43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pickle_file = \"Data/Normalized/kiwi_processed_normalized_exposure_down.pkl\"  # Update with your file path\n",
    "\n",
    "# 2. Create the dataframe (sample 1000 pixels for large datasets)\n",
    "df = load_data_and_create_df(pickle_file)\n",
    "\n",
    "# 3. Save the result\n",
    "save_dataframe(df, \"Data/parquet-data/kiwi_processed_normalized_exposure_down.parquet\")\n",
    "\n",
    "# 4. Show a sample of the result\n",
    "print(\"\\nSample of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# 5. Show some statistics\n",
    "print(\"\\nDataframe statistics:\")\n",
    "print(f\"Total rows (pixels): {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")"
   ],
   "id": "161e173c8339d5b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "def get_intensity(data_dict: Dict, x: int, y: int,\n",
    "                 excitation: float, emission: float) -> float:\n",
    "    \"\"\"\n",
    "    Extract the intensity value for a specific pixel and wavelength combination\n",
    "    from the original hyperspectral data.\n",
    "\n",
    "    Args:\n",
    "        data_dict: The original hyperspectral data dictionary\n",
    "        x: X coordinate (column) of the pixel\n",
    "        y: Y coordinate (row) of the pixel\n",
    "        excitation: Excitation wavelength\n",
    "        emission: Emission wavelength\n",
    "\n",
    "    Returns:\n",
    "        The intensity value at the specified position\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the excitation or emission wavelength is not found\n",
    "    \"\"\"\n",
    "    # Convert excitation to string key\n",
    "    ex_str = str(excitation)\n",
    "\n",
    "    # Check if excitation exists in the data\n",
    "    if ex_str not in data_dict['data']:\n",
    "        raise ValueError(f\"Excitation wavelength {excitation}nm not found in data\")\n",
    "\n",
    "    # Get the data cube and wavelengths for this excitation\n",
    "    cube = data_dict['data'][ex_str]['cube']\n",
    "    wavelengths = data_dict['data'][ex_str]['wavelengths']\n",
    "\n",
    "    # Check if the pixel coordinates are within bounds\n",
    "    height, width, _ = cube.shape\n",
    "    if x < 0 or x >= width or y < 0 or y >= height:\n",
    "        raise ValueError(f\"Pixel coordinates ({x},{y}) out of bounds for image of size {width}x{height}\")\n",
    "\n",
    "    # Find the emission wavelength index\n",
    "    try:\n",
    "        em_idx = wavelengths.index(emission)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Emission wavelength {emission}nm not found for excitation {excitation}nm\")\n",
    "\n",
    "    # Return the intensity value\n",
    "    return cube[y, x, em_idx]\n",
    "\n",
    "def validate_dataframe(data_dict: Dict, df: pd.DataFrame, num_samples: int = 10) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the transformed dataframe against the original data by comparing\n",
    "    random samples.\n",
    "\n",
    "    Args:\n",
    "        data_dict: The original hyperspectral data dictionary\n",
    "        df: The transformed dataframe\n",
    "        num_samples: Number of random samples to validate\n",
    "\n",
    "    Returns:\n",
    "        True if all validations pass, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Validating dataframe with {num_samples} random samples...\")\n",
    "\n",
    "    # Get a list of all excitation-emission combination columns\n",
    "    combination_cols = [col for col in df.columns if '-' in col]\n",
    "\n",
    "    # Function to extract excitation and emission from column name\n",
    "    def parse_column_name(col_name):\n",
    "        emission, excitation = map(float, col_name.split('-'))\n",
    "        return emission, excitation\n",
    "\n",
    "    # Randomly select rows and columns to validate\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    sample_indices = np.random.randint(0, len(df), num_samples)\n",
    "\n",
    "    all_passed = True\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = df.iloc[idx]\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "\n",
    "        # Randomly select a combination column\n",
    "        col = np.random.choice(combination_cols)\n",
    "        emission, excitation = parse_column_name(col)\n",
    "\n",
    "        # Get the value from the dataframe\n",
    "        df_value = row[col]\n",
    "\n",
    "        try:\n",
    "            # Get the value from the original data\n",
    "            original_value = get_intensity(data_dict, x, y, excitation, emission)\n",
    "\n",
    "            # Compare values (allow for small floating-point differences)\n",
    "            if abs(df_value - original_value) < 1e-6:\n",
    "                print(f\"✓ Validation passed for pixel ({x},{y}), Ex={excitation}nm, Em={emission}nm: {df_value} == {original_value}\")\n",
    "            else:\n",
    "                print(f\"✗ Validation failed for pixel ({x},{y}), Ex={excitation}nm, Em={emission}nm: {df_value} != {original_value}\")\n",
    "                all_passed = False\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"⚠ Validation error: {e}\")\n",
    "            all_passed = False\n",
    "\n",
    "    if all_passed:\n",
    "        print(\"All validations passed! The dataframe transformation is correct.\")\n",
    "    else:\n",
    "        print(\"Some validations failed. Please check your transformation code.\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "def compare_specific_pixel(data_dict: Dict, df: pd.DataFrame, x: int, y: int,\n",
    "                         excitation: float, emission: float) -> None:\n",
    "    \"\"\"\n",
    "    Compare a specific pixel's value between the original data and the transformed dataframe.\n",
    "\n",
    "    Args:\n",
    "        data_dict: The original hyperspectral data dictionary\n",
    "        df: The transformed dataframe\n",
    "        x: X coordinate of the pixel\n",
    "        y: Y coordinate of the pixel\n",
    "        excitation: Excitation wavelength\n",
    "        emission: Emission wavelength\n",
    "    \"\"\"\n",
    "    # Find the row in the dataframe for this pixel\n",
    "    pixel_row = df[(df['x'] == x) & (df['y'] == y)]\n",
    "\n",
    "    if len(pixel_row) == 0:\n",
    "        print(f\"Pixel ({x},{y}) not found in the dataframe\")\n",
    "        return\n",
    "\n",
    "    # Get the column name for this excitation-emission combination\n",
    "    col_name = f\"{int(emission)}-{int(excitation)}\"\n",
    "\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"Column {col_name} not found in the dataframe\")\n",
    "        return\n",
    "\n",
    "    # Get the value from the dataframe\n",
    "    df_value = pixel_row[col_name].values[0]\n",
    "\n",
    "    try:\n",
    "        # Get the value from the original data\n",
    "        original_value = get_intensity(data_dict, x, y, excitation, emission)\n",
    "\n",
    "        # Compare values\n",
    "        if abs(df_value - original_value) < 1e-6:\n",
    "            print(f\"✓ Values match for pixel ({x},{y}), Ex={excitation}nm, Em={emission}nm\")\n",
    "            print(f\"  Original value: {original_value}\")\n",
    "            print(f\"  Dataframe value: {df_value}\")\n",
    "        else:\n",
    "            print(f\"✗ Values do not match for pixel ({x},{y}), Ex={excitation}nm, Em={emission}nm\")\n",
    "            print(f\"  Original value: {original_value}\")\n",
    "            print(f\"  Dataframe value: {df_value}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "\n",
    "def visualize_spectrum_comparison(data_dict: Dict, df: pd.DataFrame, x: int, y: int,\n",
    "                                excitation: float) -> None:\n",
    "    \"\"\"\n",
    "    Visualize and compare the emission spectrum for a specific pixel and excitation\n",
    "    between the original data and the transformed dataframe.\n",
    "\n",
    "    Args:\n",
    "        data_dict: The original hyperspectral data dictionary\n",
    "        df: The transformed dataframe\n",
    "        x: X coordinate of the pixel\n",
    "        y: Y coordinate of the pixel\n",
    "        excitation: Excitation wavelength\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert excitation to string key\n",
    "    ex_str = str(excitation)\n",
    "\n",
    "    # Check if excitation exists in the data\n",
    "    if ex_str not in data_dict['data']:\n",
    "        print(f\"Excitation wavelength {excitation}nm not found in data\")\n",
    "        return\n",
    "\n",
    "    # Get the wavelengths and data cube for this excitation\n",
    "    wavelengths = data_dict['data'][ex_str]['wavelengths']\n",
    "    cube = data_dict['data'][ex_str]['cube']\n",
    "\n",
    "    # Extract the spectrum from the original data\n",
    "    original_spectrum = cube[y, x, :]\n",
    "\n",
    "    # Find the row in the dataframe for this pixel\n",
    "    pixel_row = df[(df['x'] == x) & (df['y'] == y)]\n",
    "\n",
    "    if len(pixel_row) == 0:\n",
    "        print(f\"Pixel ({x},{y}) not found in the dataframe\")\n",
    "        return\n",
    "\n",
    "    # Extract values from the dataframe for this excitation\n",
    "    df_values = []\n",
    "    df_wavelengths = []\n",
    "\n",
    "    for emission in wavelengths:\n",
    "        col_name = f\"{int(emission)}-{int(excitation)}\"\n",
    "        if col_name in df.columns:\n",
    "            df_values.append(pixel_row[col_name].values[0])\n",
    "            df_wavelengths.append(emission)\n",
    "\n",
    "    # Plot the comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(wavelengths, original_spectrum, 'o-', label='Original Data')\n",
    "    plt.plot(df_wavelengths, df_values, 'x--', label='Transformed DataFrame')\n",
    "    plt.xlabel('Emission Wavelength (nm)')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.title(f'Spectrum Comparison for Pixel ({x},{y}) at Excitation {excitation}nm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate some statistics\n",
    "    original_mean = np.mean(original_spectrum)\n",
    "    df_mean = np.mean(df_values)\n",
    "\n",
    "    print(f\"Original data mean intensity: {original_mean:.2f}\")\n",
    "    print(f\"DataFrame mean intensity: {df_mean:.2f}\")\n",
    "    print(f\"Difference: {abs(original_mean - df_mean):.2f}\")"
   ],
   "id": "bc6167941bdb0160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pickle_file = \"Data/pickle-data/kiwi_processed.pkl\"  # Update with your file path\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "df_file = \"Data/parquet-data/hyperspectral_2d.parquet\"  # Update with your file path\n",
    "df = pd.read_parquet(df_file)\n",
    "\n",
    "# Validate the dataframe (check 20 random samples)\n",
    "validate_dataframe(data_dict, df, num_samples=20)\n",
    "\n",
    "# Check a specific pixel\n",
    "x, y = 100, 200  # Example coordinates\n",
    "excitation = 350.0\n",
    "emission = 480.0\n",
    "\n",
    "compare_specific_pixel(data_dict, df, x, y, excitation, emission)\n",
    "\n",
    "# Visualize a spectrum comparison\n",
    "visualize_spectrum_comparison(data_dict, df, x, y, excitation)"
   ],
   "id": "5d7f7e457ab92465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "def normalize_hyperspectral_data(\n",
    "    data_dict: Dict,\n",
    "    reference_type: str = 'min',\n",
    "    output_file: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Normalize hyperspectral data based on exposure time.\n",
    "\n",
    "    Args:\n",
    "        data_dict: Dictionary containing hyperspectral data with exposure time in metadata\n",
    "        reference_type: Type of reference exposure time ('min', 'max', or float value)\n",
    "        output_file: Path to save the normalized data pickle file (optional)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing normalized hyperspectral data\n",
    "    \"\"\"\n",
    "    print(f\"Normalizing hyperspectral data using {reference_type} exposure as reference...\")\n",
    "\n",
    "    # Create a deep copy of the data to avoid modifying the original\n",
    "    normalized_data = copy.deepcopy(data_dict)\n",
    "\n",
    "    # Extract exposure times for each excitation wavelength\n",
    "    exposure_times = {}\n",
    "\n",
    "    for ex_str in data_dict['data'].keys():\n",
    "        # Try to get exposure time from different possible locations in the data structure\n",
    "        if 'raw' in data_dict['data'][ex_str] and 'expos_val' in data_dict['data'][ex_str]['raw']:\n",
    "            exposure_times[ex_str] = data_dict['data'][ex_str]['raw']['expos_val']\n",
    "        elif 'expos_val' in data_dict['data'][ex_str]:\n",
    "            exposure_times[ex_str] = data_dict['data'][ex_str]['expos_val']\n",
    "\n",
    "    if not exposure_times:\n",
    "        raise ValueError(\"Could not find exposure time information in the data\")\n",
    "\n",
    "    print(f\"Found exposure times for {len(exposure_times)} excitation wavelengths\")\n",
    "\n",
    "    # Determine the reference exposure time\n",
    "    if reference_type == 'min':\n",
    "        reference_exposure = min(exposure_times.values())\n",
    "        print(f\"Using minimum exposure time as reference: {reference_exposure}\")\n",
    "    elif reference_type == 'max':\n",
    "        reference_exposure = max(exposure_times.values())\n",
    "        print(f\"Using maximum exposure time as reference: {reference_exposure}\")\n",
    "    elif isinstance(reference_type, (int, float)):\n",
    "        reference_exposure = float(reference_type)\n",
    "        print(f\"Using provided exposure time as reference: {reference_exposure}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reference_type. Use 'min', 'max', or a float value.\")\n",
    "\n",
    "    # Store the normalization information in metadata\n",
    "    if 'metadata' not in normalized_data:\n",
    "        normalized_data['metadata'] = {}\n",
    "\n",
    "    normalized_data['metadata']['normalization'] = {\n",
    "        'reference_type': reference_type,\n",
    "        'reference_exposure': reference_exposure,\n",
    "        'original_exposures': exposure_times\n",
    "    }\n",
    "\n",
    "    # Normalize each data cube\n",
    "    print(\"Normalizing data cubes...\")\n",
    "    for ex_str, exposure in exposure_times.items():\n",
    "        # Calculate normalization factor: E₁/E₂\n",
    "        normalization_factor = reference_exposure / exposure\n",
    "\n",
    "        # Apply normalization to the data cube\n",
    "        original_cube = data_dict['data'][ex_str]['cube']\n",
    "\n",
    "        # Normalize: I_ij^norm = I_ij × (E₁/E₂)\n",
    "        normalized_data['data'][ex_str]['cube'] = original_cube * normalization_factor\n",
    "\n",
    "        # Store normalization factor in metadata\n",
    "        normalized_data['data'][ex_str]['normalization_factor'] = normalization_factor\n",
    "\n",
    "        print(f\"  Normalized excitation {ex_str}nm (Exposure: {exposure}, Factor: {normalization_factor:.4f})\")\n",
    "\n",
    "    # Save the normalized data if output file is provided\n",
    "    if output_file:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(normalized_data, f)\n",
    "        print(f\"Normalized data saved to {output_file}\")\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "def print_exposure_info(data_dict: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Print exposure time information from the data dictionary.\n",
    "\n",
    "    Args:\n",
    "        data_dict: Dictionary containing hyperspectral data\n",
    "    \"\"\"\n",
    "    print(\"\\nExposure Time Information:\")\n",
    "\n",
    "    exposure_times = {}\n",
    "\n",
    "    for ex_str in data_dict['data'].keys():\n",
    "        # Try to get exposure time from different possible locations\n",
    "        if 'raw' in data_dict['data'][ex_str] and 'expos_val' in data_dict['data'][ex_str]['raw']:\n",
    "            exposure_times[ex_str] = data_dict['data'][ex_str]['raw']['expos_val']\n",
    "        elif 'expos_val' in data_dict['data'][ex_str]:\n",
    "            exposure_times[ex_str] = data_dict['data'][ex_str]['expos_val']\n",
    "\n",
    "    if not exposure_times:\n",
    "        print(\"No exposure time information found in the data\")\n",
    "        return\n",
    "\n",
    "    # Convert to sorted list of tuples\n",
    "    sorted_exposures = sorted([(float(ex), exp) for ex, exp in exposure_times.items()])\n",
    "\n",
    "    print(f\"{'Excitation (nm)':<15} {'Exposure Time':<15}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for ex, exp in sorted_exposures:\n",
    "        print(f\"{ex:<15.1f} {exp:<15}\")\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Minimum exposure: {min(exposure_times.values())}\")\n",
    "    print(f\"Maximum exposure: {max(exposure_times.values())}\")\n",
    "    print(f\"Ratio max/min: {max(exposure_times.values()) / min(exposure_times.values()):.2f}\")\n",
    "\n",
    "def normalize_and_save_both_versions(\n",
    "    input_file: str,\n",
    "    output_dir: Optional[str] = None\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load data, normalize it using both min and max exposure times, and save both versions.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to the input pickle file\n",
    "        output_dir: Directory to save the output files (default: same as input file)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (up_normalized_data, down_normalized_data)\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    with open(input_file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    # Print exposure information\n",
    "    print_exposure_info(data_dict)\n",
    "\n",
    "    # Set up output directory\n",
    "    input_path = Path(input_file)\n",
    "    if output_dir is None:\n",
    "        output_dir = input_path.parent\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create output file names\n",
    "    base_name = input_path.stem\n",
    "    up_output_file = output_dir / f\"{base_name}_normalized_exposure_up.pkl\"\n",
    "    down_output_file = output_dir / f\"{base_name}_normalized_exposure_down.pkl\"\n",
    "\n",
    "    # Normalize up (using max exposure as reference)\n",
    "    up_normalized_data = normalize_hyperspectral_data(\n",
    "        data_dict,\n",
    "        reference_type='max',\n",
    "        output_file=str(up_output_file)\n",
    "    )\n",
    "\n",
    "    # Normalize down (using min exposure as reference)\n",
    "    down_normalized_data = normalize_hyperspectral_data(\n",
    "        data_dict,\n",
    "        reference_type='min',\n",
    "        output_file=str(down_output_file)\n",
    "    )\n",
    "\n",
    "    print(\"\\nNormalization complete!\")\n",
    "    print(f\"Up-normalized data (max exposure reference) saved to: {up_output_file}\")\n",
    "    print(f\"Down-normalized data (min exposure reference) saved to: {down_output_file}\")\n",
    "\n",
    "    return up_normalized_data, down_normalized_data"
   ],
   "id": "dcdb21d3b4cc9fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "normalize_and_save_both_versions('Data/pickle-data/kiwi_processed.pkl', 'Data/Kiwi Experiment/pickles')",
   "id": "e866b40f6257f150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "97ecbb45cc71c817",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
