{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap\n",
    "from typing import Tuple, List, Optional, Dict, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class HyperspectralClustering:\n",
    "    \"\"\"\n",
    "    Class for unsupervised clustering of hyperspectral data and visualizing results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, image_shape: Optional[Tuple[int, int]] = None):\n",
    "        \"\"\"\n",
    "        Initialize with the flattened hyperspectral dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing flattened hyperspectral data with x, y coordinates\n",
    "            image_shape: Tuple of (height, width) of the original image. If None, inferred from data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        # Extract spatial coordinates and feature columns\n",
    "        self.x_col = 'x'\n",
    "        self.y_col = 'y'\n",
    "\n",
    "        # Validate that x and y columns exist\n",
    "        if self.x_col not in df.columns or self.y_col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{self.x_col}' and '{self.y_col}' columns\")\n",
    "\n",
    "        # Get feature columns (all columns except x and y)\n",
    "        self.feature_cols = [col for col in df.columns if col not in [self.x_col, self.y_col]]\n",
    "\n",
    "        if len(self.feature_cols) == 0:\n",
    "            raise ValueError(\"No feature columns found in DataFrame\")\n",
    "\n",
    "        print(f\"Found {len(self.feature_cols)} feature columns in the data\")\n",
    "\n",
    "        # Determine image shape if not provided\n",
    "        if image_shape is None:\n",
    "            self.height = int(df[self.y_col].max()) + 1\n",
    "            self.width = int(df[self.x_col].max()) + 1\n",
    "        else:\n",
    "            self.height, self.width = image_shape\n",
    "\n",
    "        print(f\"Image shape: {self.height} Ã— {self.width} pixels\")\n",
    "\n",
    "        # Initialize model and results\n",
    "        self.model = None\n",
    "        self.labels = None\n",
    "        self.feature_scaler = None\n",
    "        self.scaled_features = None\n",
    "\n",
    "    def preprocess(self, handle_nan: str = 'drop_feature'):\n",
    "        \"\"\"\n",
    "        Preprocess the data for clustering.\n",
    "\n",
    "        Args:\n",
    "            handle_nan: Strategy for handling NaN values ('drop_feature', 'fill_zero', 'fill_mean')\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Extract features\n",
    "        features = self.df[self.feature_cols].copy()\n",
    "\n",
    "        # Count missing values\n",
    "        nan_counts = features.isna().sum()\n",
    "        nan_features = nan_counts[nan_counts > 0]\n",
    "\n",
    "        if len(nan_features) > 0:\n",
    "            print(f\"Found {len(nan_features)} features with missing values\")\n",
    "            print(f\"Top 5 features with most NaNs: {nan_features.sort_values(ascending=False).head()}\")\n",
    "\n",
    "            # Handle missing values based on strategy\n",
    "            if handle_nan == 'drop_feature':\n",
    "                # Drop features with any NaN values\n",
    "                good_features = [col for col in self.feature_cols if nan_counts[col] == 0]\n",
    "                features = self.df[good_features].copy()\n",
    "                print(f\"Dropped {len(self.feature_cols) - len(good_features)} features with NaNs\")\n",
    "                self.feature_cols = good_features\n",
    "\n",
    "            elif handle_nan == 'fill_zero':\n",
    "                # Replace NaN with zeros\n",
    "                features.fillna(0, inplace=True)\n",
    "                print(\"Filled NaN values with zeros\")\n",
    "\n",
    "            elif handle_nan == 'fill_mean':\n",
    "                # Replace NaN with feature means\n",
    "                features.fillna(features.mean(), inplace=True)\n",
    "                print(\"Filled NaN values with feature means\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown handle_nan strategy: {handle_nan}\")\n",
    "\n",
    "        # Scale the features\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.scaled_features = self.feature_scaler.fit_transform(features)\n",
    "        print(f\"Scaled {self.scaled_features.shape[1]} features\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def find_optimal_clusters(self, max_clusters: int = 10, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using silhouette score and inertia.\n",
    "\n",
    "        Args:\n",
    "            max_clusters: Maximum number of clusters to try\n",
    "            random_state: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (silhouette_scores, inertia_values)\n",
    "        \"\"\"\n",
    "        print(f\"Finding optimal number of clusters (max={max_clusters})...\")\n",
    "\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        # Try different numbers of clusters\n",
    "        silhouette_scores = []\n",
    "        inertia_values = []\n",
    "\n",
    "        # Start from 2 clusters (silhouette score requires at least 2)\n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fit KMeans\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(self.scaled_features)\n",
    "\n",
    "            # Calculate metrics\n",
    "            silhouette_avg = silhouette_score(self.scaled_features, cluster_labels)\n",
    "            inertia = kmeans.inertia_\n",
    "\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            inertia_values.append(inertia)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "            print(f\"  {n_clusters} clusters: silhouette={silhouette_avg:.4f}, inertia={inertia:.4f} (took {duration:.2f}s)\")\n",
    "\n",
    "        return silhouette_scores, inertia_values\n",
    "\n",
    "    def plot_cluster_metrics(self,\n",
    "                             silhouette_scores: List[float],\n",
    "                             inertia_values: List[float],\n",
    "                             max_clusters: int = 10):\n",
    "        \"\"\"\n",
    "        Plot metrics to help select the optimal number of clusters.\n",
    "\n",
    "        Args:\n",
    "            silhouette_scores: List of silhouette scores for different numbers of clusters\n",
    "            inertia_values: List of inertia values for different numbers of clusters\n",
    "            max_clusters: Maximum number of clusters tried\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Plot silhouette scores\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        ax1.plot(cluster_range, silhouette_scores, 'o-', linewidth=2, markersize=8)\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Silhouette Score')\n",
    "        ax1.set_title('Silhouette Score vs Number of Clusters')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Find optimal according to silhouette\n",
    "        optimal_idx = np.argmax(silhouette_scores)\n",
    "        optimal_clusters = cluster_range[optimal_idx]\n",
    "        ax1.axvline(x=optimal_clusters, color='r', linestyle='--',\n",
    "                   label=f'Optimal: {optimal_clusters} clusters')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot inertia (elbow method)\n",
    "        ax2.plot(cluster_range, inertia_values, 'o-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Number of Clusters')\n",
    "        ax2.set_ylabel('Inertia')\n",
    "        ax2.set_title('Inertia vs Number of Clusters (Elbow Method)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def fit_kmeans(self, n_clusters: int = 5, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Fit K-means clustering on the hyperspectral data.\n",
    "\n",
    "        Args:\n",
    "            n_clusters: Number of clusters\n",
    "            random_state: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(f\"Fitting K-means with {n_clusters} clusters...\")\n",
    "\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        # Fit the model\n",
    "        self.model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        self.labels = self.model.fit_predict(self.scaled_features)\n",
    "\n",
    "        # Add cluster labels to dataframe\n",
    "        self.df['cluster'] = self.labels\n",
    "\n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = np.bincount(self.labels)\n",
    "\n",
    "        print(\"Cluster sizes:\")\n",
    "        for i, size in enumerate(cluster_sizes):\n",
    "            percentage = size / len(self.labels) * 100\n",
    "            print(f\"  Cluster {i}: {size} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def reconstruct_cluster_image(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the cluster assignments into the original image shape.\n",
    "\n",
    "        Returns:\n",
    "            2D array of cluster assignments with shape (height, width)\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Create empty image array\n",
    "        cluster_image = np.full((self.height, self.width), -1, dtype=int)\n",
    "\n",
    "        # Fill in cluster assignments\n",
    "        for idx, row in self.df.iterrows():\n",
    "            x, y = int(row[self.x_col]), int(row[self.y_col])\n",
    "            if 0 <= x < self.width and 0 <= y < self.height:\n",
    "                cluster_image[y, x] = row['cluster']\n",
    "\n",
    "        return cluster_image\n",
    "\n",
    "    def visualize_clusters(self,\n",
    "                          figsize: Tuple[int, int] = (12, 10),\n",
    "                          cmap: Optional[Union[str, ListedColormap]] = None,\n",
    "                          save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the clustering results by reconstructing the original image.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Create a good colormap for the number of clusters\n",
    "        if cmap is None:\n",
    "            # Choose colormap based on number of clusters\n",
    "            if n_clusters <= 10:\n",
    "                # For few clusters, use distinct colors\n",
    "                # Get a good set of distinct colors\n",
    "                tableau_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "                color_list = tableau_colors[:n_clusters]\n",
    "                cmap = ListedColormap(color_list)\n",
    "            else:\n",
    "                # For many clusters, use a continuous colormap\n",
    "                cmap = 'tab20' if n_clusters <= 20 else 'viridis'\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Display the image\n",
    "        im = ax.imshow(cluster_image, cmap=cmap)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Cluster')\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f'K-means Clustering Results ({n_clusters} clusters)')\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_separate_clusters(self,\n",
    "                                   figsize: Tuple[int, int] = (15, 10),\n",
    "                                   cmap: str = 'viridis',\n",
    "                                   n_cols: int = 3,\n",
    "                                   save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize each cluster separately.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            n_cols: Number of columns in the subplot grid\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = (n_clusters + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Create separate mask for each cluster\n",
    "        for i in range(n_clusters):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create binary mask for this cluster\n",
    "            mask = (cluster_image == i).astype(float)\n",
    "\n",
    "            # Display the mask\n",
    "            im = ax.imshow(mask, cmap=cmap)\n",
    "\n",
    "            # Set title and turn off axis labels\n",
    "            ax.set_title(f'Cluster {i}')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(n_clusters, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Individual Clusters', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved individual cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_cluster_spectra(self,\n",
    "                                 excitations: Optional[List[float]] = None,\n",
    "                                 figsize: Tuple[int, int] = (15, 10),\n",
    "                                 save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the average spectra for each cluster.\n",
    "\n",
    "        Args:\n",
    "            excitations: List of excitation wavelengths to include (if None, use all)\n",
    "            figsize: Figure size\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Get all feature columns that have the format \"emission-excitation\"\n",
    "        spectral_cols = [col for col in self.feature_cols if '-' in col]\n",
    "\n",
    "        # If no spectral columns, we can't visualize spectra\n",
    "        if not spectral_cols:\n",
    "            raise ValueError(\"No spectral columns found in DataFrame\")\n",
    "\n",
    "        # Parse excitation and emission wavelengths from column names\n",
    "        wavelengths = []\n",
    "        for col in spectral_cols:\n",
    "            try:\n",
    "                emission, excitation = map(float, col.split('-'))\n",
    "                wavelengths.append((emission, excitation, col))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping column {col} - doesn't match expected format\")\n",
    "\n",
    "        # Filter by excitation wavelengths if requested\n",
    "        if excitations:\n",
    "            wavelengths = [(em, ex, col) for em, ex, col in wavelengths if ex in excitations]\n",
    "\n",
    "        # If no wavelengths left, we can't visualize spectra\n",
    "        if not wavelengths:\n",
    "            raise ValueError(\"No valid spectral columns found after filtering\")\n",
    "\n",
    "        # Group by excitation wavelength\n",
    "        excitations = sorted(set(ex for _, ex, _ in wavelengths))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = min(3, len(excitations))\n",
    "        n_cols = (len(excitations) + n_rows - 1) // n_rows\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Get number of clusters\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Set up colors for clusters\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "        # Plot spectra for each excitation wavelength\n",
    "        for ax_idx, excitation in enumerate(excitations):\n",
    "            if ax_idx < len(axes):\n",
    "                ax = axes[ax_idx]\n",
    "\n",
    "                # Get emission wavelengths and column names for this excitation\n",
    "                excitation_data = [(em, col) for em, ex, col in wavelengths if ex == excitation]\n",
    "\n",
    "                if excitation_data:\n",
    "                    # Sort by emission wavelength\n",
    "                    excitation_data.sort()\n",
    "                    emission_wavelengths = [em for em, _ in excitation_data]\n",
    "                    columns = [col for _, col in excitation_data]\n",
    "\n",
    "                    # Plot average spectrum for each cluster\n",
    "                    for cluster_idx in range(n_clusters):\n",
    "                        # Get cluster data\n",
    "                        cluster_data = self.df[self.df['cluster'] == cluster_idx]\n",
    "\n",
    "                        # Calculate mean spectrum for this cluster\n",
    "                        mean_spectrum = cluster_data[columns].mean().values\n",
    "\n",
    "                        # Plot spectrum\n",
    "                        ax.plot(emission_wavelengths, mean_spectrum, '-',\n",
    "                                color=colors[cluster_idx], linewidth=2,\n",
    "                                label=f'Cluster {cluster_idx}')\n",
    "\n",
    "                    # Set labels and title\n",
    "                    ax.set_xlabel('Emission Wavelength (nm)')\n",
    "                    ax.set_ylabel('Mean Intensity')\n",
    "                    ax.set_title(f'Excitation {excitation} nm')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Add legend to first plot only\n",
    "                    if ax_idx == 0:\n",
    "                        ax.legend(loc='best')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(len(excitations), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Average Spectra by Cluster', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster spectra visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def save_cluster_results(self, output_file: str):\n",
    "        \"\"\"\n",
    "        Save clustering results to a file.\n",
    "\n",
    "        Args:\n",
    "            output_file: Path to save the results\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "\n",
    "        # Save based on file type\n",
    "        if ext.lower() in ['.csv']:\n",
    "            # Save to CSV (coordinates and cluster assignments only)\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.pkl', '.pickle']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_pickle(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.parquet']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_parquet(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "def run_kmeans_clustering(\n",
    "    input_file: str,\n",
    "    n_clusters: Optional[int] = None,\n",
    "    max_clusters: int = 10,\n",
    "    output_dir: Optional[str] = None,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Run K-means clustering on hyperspectral data and save results.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to input CSV/parquet/pickle file with flattened hyperspectral data\n",
    "        n_clusters: Number of clusters to use (if None, determine automatically)\n",
    "        max_clusters: Maximum number of clusters to try if determining automatically\n",
    "        output_dir: Directory to save outputs (if None, use same directory as input)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Determine file type and load data\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    if ext.lower() in ['.csv']:\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif ext.lower() in ['.pkl', '.pickle']:\n",
    "        df = pd.read_pickle(input_file)\n",
    "    elif ext.lower() in ['.parquet']:\n",
    "        df = pd.read_parquet(input_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "    # Set up output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "        if not output_dir:\n",
    "            output_dir = \".\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.basename(input_file)\n",
    "    base_name = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Initialize clustering\n",
    "    clustering = HyperspectralClustering(df)\n",
    "\n",
    "    # Preprocess the data\n",
    "    clustering.preprocess()\n",
    "\n",
    "    # If number of clusters not specified, find optimal\n",
    "    if n_clusters is None:\n",
    "        silhouette_scores, inertia_values = clustering.find_optimal_clusters(max_clusters, random_state)\n",
    "\n",
    "        # Plot metrics\n",
    "        fig = clustering.plot_cluster_metrics(silhouette_scores, inertia_values, max_clusters)\n",
    "        metrics_path = os.path.join(output_dir, f\"{base_name}_cluster_metrics.png\")\n",
    "        fig.savefig(metrics_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved cluster metrics plot to {metrics_path}\")\n",
    "\n",
    "        # Choose optimal number of clusters (highest silhouette score)\n",
    "        n_clusters = np.argmax(silhouette_scores) + 2  # Add 2 because we start from 2 clusters\n",
    "        print(f\"Selected optimal number of clusters: {n_clusters}\")\n",
    "\n",
    "    # Fit K-means with the chosen number of clusters\n",
    "    clustering.fit_kmeans(n_clusters, random_state)\n",
    "\n",
    "    # Visualize clustering results\n",
    "    fig = clustering.visualize_clusters()\n",
    "    clusters_path = os.path.join(output_dir, f\"{base_name}_kmeans_clusters.png\")\n",
    "    fig.savefig(clusters_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved cluster visualization to {clusters_path}\")\n",
    "\n",
    "    # Visualize individual clusters\n",
    "    fig = clustering.visualize_separate_clusters()\n",
    "    separate_path = os.path.join(output_dir, f\"{base_name}_separate_clusters.png\")\n",
    "    fig.savefig(separate_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved individual cluster visualization to {separate_path}\")\n",
    "\n",
    "    # Visualize cluster spectra\n",
    "    try:\n",
    "        fig = clustering.visualize_cluster_spectra()\n",
    "        spectra_path = os.path.join(output_dir, f\"{base_name}_cluster_spectra.png\")\n",
    "        fig.savefig(spectra_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved cluster spectra visualization to {spectra_path}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not visualize cluster spectra: {e}\")\n",
    "\n",
    "    # Save cluster results\n",
    "    results_path = os.path.join(output_dir, f\"{base_name}_kmeans_results.csv\")\n",
    "    clustering.save_cluster_results(results_path)\n",
    "\n",
    "    print(\"K-means clustering complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(3, 15):\n",
    "    run_kmeans_clustering(\"../Data/parquet-data/hyperspectral_2d.parquet\", n_clusters=i, output_dir=f\"KMeansResults/N-Clusters{i}\")"
   ],
   "id": "aeba7e8487b4914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(3, 15):\n",
    "    run_kmeans_clustering(\"../hy.parquet\", n_clusters=i, output_dir=f\"KMeansResults/N-Clusters{i}\")"
   ],
   "id": "57d781474fe3b120",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:15:07.566538Z",
     "start_time": "2025-04-17T22:15:07.410974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap\n",
    "from typing import Tuple, List, Optional, Dict, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class HyperspectralClustering:\n",
    "    \"\"\"\n",
    "    Class for unsupervised clustering of hyperspectral data and visualizing results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, image_shape: Optional[Tuple[int, int]] = None):\n",
    "        \"\"\"\n",
    "        Initialize with the flattened hyperspectral dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing flattened hyperspectral data with x, y coordinates\n",
    "            image_shape: Tuple of (height, width) of the original image. If None, inferred from data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        # Extract spatial coordinates and feature columns\n",
    "        self.x_col = 'x'\n",
    "        self.y_col = 'y'\n",
    "\n",
    "        # Validate that x and y columns exist\n",
    "        if self.x_col not in df.columns or self.y_col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{self.x_col}' and '{self.y_col}' columns\")\n",
    "\n",
    "        # Get feature columns (all columns except x and y)\n",
    "        self.feature_cols = [col for col in df.columns if col not in [self.x_col, self.y_col]]\n",
    "\n",
    "        if len(self.feature_cols) == 0:\n",
    "            raise ValueError(\"No feature columns found in DataFrame\")\n",
    "\n",
    "        print(f\"Found {len(self.feature_cols)} feature columns in the data\")\n",
    "\n",
    "        # Determine image shape if not provided\n",
    "        if image_shape is None:\n",
    "            self.height = int(df[self.y_col].max()) + 1\n",
    "            self.width = int(df[self.x_col].max()) + 1\n",
    "        else:\n",
    "            self.height, self.width = image_shape\n",
    "\n",
    "        print(f\"Image shape: {self.height} Ã— {self.width} pixels\")\n",
    "\n",
    "        # Initialize model and results\n",
    "        self.model = None\n",
    "        self.labels = None\n",
    "        self.feature_scaler = None\n",
    "        self.scaled_features = None\n",
    "\n",
    "    def preprocess(self, handle_nan: str = 'drop_feature'):\n",
    "        \"\"\"\n",
    "        Preprocess the data for clustering.\n",
    "\n",
    "        Args:\n",
    "            handle_nan: Strategy for handling NaN values ('drop_feature', 'fill_zero', 'fill_mean')\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Extract features\n",
    "        features = self.df[self.feature_cols].copy()\n",
    "\n",
    "        # Count missing values\n",
    "        nan_counts = features.isna().sum()\n",
    "        nan_features = nan_counts[nan_counts > 0]\n",
    "\n",
    "        if len(nan_features) > 0:\n",
    "            print(f\"Found {len(nan_features)} features with missing values\")\n",
    "            print(f\"Top 5 features with most NaNs: {nan_features.sort_values(ascending=False).head()}\")\n",
    "\n",
    "            # Handle missing values based on strategy\n",
    "            if handle_nan == 'drop_feature':\n",
    "                # Drop features with any NaN values\n",
    "                good_features = [col for col in self.feature_cols if nan_counts[col] == 0]\n",
    "                features = self.df[good_features].copy()\n",
    "                print(f\"Dropped {len(self.feature_cols) - len(good_features)} features with NaNs\")\n",
    "                self.feature_cols = good_features\n",
    "\n",
    "            elif handle_nan == 'fill_zero':\n",
    "                # Replace NaN with zeros\n",
    "                features.fillna(0, inplace=True)\n",
    "                print(\"Filled NaN values with zeros\")\n",
    "\n",
    "            elif handle_nan == 'fill_mean':\n",
    "                # Replace NaN with feature means\n",
    "                features.fillna(features.mean(), inplace=True)\n",
    "                print(\"Filled NaN values with feature means\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown handle_nan strategy: {handle_nan}\")\n",
    "\n",
    "        # Scale the features\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.scaled_features = self.feature_scaler.fit_transform(features)\n",
    "        print(f\"Scaled {self.scaled_features.shape[1]} features\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate_clustering(self, n_clusters: int, random_state: int = 42) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate clustering for a specific number of clusters.\n",
    "\n",
    "        Args:\n",
    "            n_clusters: Number of clusters\n",
    "            random_state: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit KMeans\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(self.scaled_features)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {}\n",
    "\n",
    "        # Basic metrics\n",
    "        metrics['n_clusters'] = n_clusters\n",
    "        metrics['inertia'] = kmeans.inertia_\n",
    "\n",
    "        # Silhouette score (requires at least 2 clusters)\n",
    "        if n_clusters >= 2:\n",
    "            metrics['silhouette_score'] = silhouette_score(self.scaled_features, cluster_labels)\n",
    "        else:\n",
    "            metrics['silhouette_score'] = float('nan')\n",
    "\n",
    "        # Calculate Calinski-Harabasz Index (Variance Ratio Criterion)\n",
    "        try:\n",
    "            from sklearn.metrics import calinski_harabasz_score\n",
    "            metrics['calinski_harabasz_score'] = calinski_harabasz_score(self.scaled_features, cluster_labels)\n",
    "        except:\n",
    "            metrics['calinski_harabasz_score'] = float('nan')\n",
    "\n",
    "        # Calculate Davies-Bouldin Index\n",
    "        try:\n",
    "            from sklearn.metrics import davies_bouldin_score\n",
    "            metrics['davies_bouldin_score'] = davies_bouldin_score(self.scaled_features, cluster_labels)\n",
    "        except:\n",
    "            metrics['davies_bouldin_score'] = float('nan')\n",
    "\n",
    "        # Add calculation time\n",
    "        metrics['calculation_time'] = time.time() - start_time\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def find_optimal_clusters(self, max_clusters: int = 10, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using silhouette score and inertia.\n",
    "\n",
    "        Args:\n",
    "            max_clusters: Maximum number of clusters to try\n",
    "            random_state: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (silhouette_scores, inertia_values, metrics_df)\n",
    "        \"\"\"\n",
    "        print(f\"Finding optimal number of clusters (max={max_clusters})...\")\n",
    "\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        # Try different numbers of clusters\n",
    "        silhouette_scores = []\n",
    "        inertia_values = []\n",
    "        metrics_list = []\n",
    "\n",
    "        # Start from 2 clusters (silhouette score requires at least 2)\n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            # Evaluate clustering\n",
    "            metrics = self.evaluate_clustering(n_clusters, random_state)\n",
    "            metrics_list.append(metrics)\n",
    "\n",
    "            # Extract key metrics for backward compatibility\n",
    "            silhouette_avg = metrics['silhouette_score']\n",
    "            inertia = metrics['inertia']\n",
    "\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            inertia_values.append(inertia)\n",
    "\n",
    "            print(f\"  {n_clusters} clusters: silhouette={silhouette_avg:.4f}, inertia={inertia:.4f}, \"\\\n",
    "                  f\"CH score={metrics['calinski_harabasz_score']:.2f}, \"\\\n",
    "                  f\"DB index={metrics['davies_bouldin_score']:.4f} \"\\\n",
    "                  f\"(took {metrics['calculation_time']:.2f}s)\")\n",
    "\n",
    "        # Create metrics dataframe\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "        return silhouette_scores, inertia_values, metrics_df\n",
    "\n",
    "    def plot_cluster_metrics(self,\n",
    "                             silhouette_scores: List[float],\n",
    "                             inertia_values: List[float],\n",
    "                             max_clusters: int = 10):\n",
    "        \"\"\"\n",
    "        Plot metrics to help select the optimal number of clusters.\n",
    "\n",
    "        Args:\n",
    "            silhouette_scores: List of silhouette scores for different numbers of clusters\n",
    "            inertia_values: List of inertia values for different numbers of clusters\n",
    "            max_clusters: Maximum number of clusters tried\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Plot silhouette scores\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        ax1.plot(cluster_range, silhouette_scores, 'o-', linewidth=2, markersize=8)\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Silhouette Score')\n",
    "        ax1.set_title('Silhouette Score vs Number of Clusters')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Find optimal according to silhouette\n",
    "        optimal_idx = np.argmax(silhouette_scores)\n",
    "        optimal_clusters = cluster_range[optimal_idx]\n",
    "        ax1.axvline(x=optimal_clusters, color='r', linestyle='--',\n",
    "                   label=f'Optimal: {optimal_clusters} clusters')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot inertia (elbow method)\n",
    "        ax2.plot(cluster_range, inertia_values, 'o-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Number of Clusters')\n",
    "        ax2.set_ylabel('Inertia')\n",
    "        ax2.set_title('Inertia vs Number of Clusters (Elbow Method)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def fit_kmeans(self, n_clusters: int = 5, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Fit K-means clustering on the hyperspectral data.\n",
    "\n",
    "        Args:\n",
    "            n_clusters: Number of clusters\n",
    "            random_state: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(f\"Fitting K-means with {n_clusters} clusters...\")\n",
    "\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        # Fit the model\n",
    "        self.model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        self.labels = self.model.fit_predict(self.scaled_features)\n",
    "\n",
    "        # Add cluster labels to dataframe\n",
    "        self.df['cluster'] = self.labels\n",
    "\n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = np.bincount(self.labels)\n",
    "\n",
    "        print(\"Cluster sizes:\")\n",
    "        for i, size in enumerate(cluster_sizes):\n",
    "            percentage = size / len(self.labels) * 100\n",
    "            print(f\"  Cluster {i}: {size} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def reconstruct_cluster_image(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the cluster assignments into the original image shape.\n",
    "\n",
    "        Returns:\n",
    "            2D array of cluster assignments with shape (height, width)\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Create empty image array\n",
    "        cluster_image = np.full((self.height, self.width), -1, dtype=int)\n",
    "\n",
    "        # Fill in cluster assignments\n",
    "        for idx, row in self.df.iterrows():\n",
    "            x, y = int(row[self.x_col]), int(row[self.y_col])\n",
    "            if 0 <= x < self.width and 0 <= y < self.height:\n",
    "                cluster_image[y, x] = row['cluster']\n",
    "\n",
    "        return cluster_image\n",
    "\n",
    "    def visualize_clusters(self,\n",
    "                          figsize: Tuple[int, int] = (12, 10),\n",
    "                          cmap: Optional[Union[str, ListedColormap]] = None,\n",
    "                          save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the clustering results by reconstructing the original image.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Create a good colormap for the number of clusters\n",
    "        if cmap is None:\n",
    "            # Choose colormap based on number of clusters\n",
    "            if n_clusters <= 10:\n",
    "                # For few clusters, use distinct colors\n",
    "                # Get a good set of distinct colors\n",
    "                tableau_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "                color_list = tableau_colors[:n_clusters]\n",
    "                cmap = ListedColormap(color_list)\n",
    "            else:\n",
    "                # For many clusters, use a continuous colormap\n",
    "                cmap = 'tab20' if n_clusters <= 20 else 'viridis'\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Display the image\n",
    "        im = ax.imshow(cluster_image, cmap=cmap)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Cluster')\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f'K-means Clustering Results ({n_clusters} clusters)')\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_separate_clusters(self,\n",
    "                                   figsize: Tuple[int, int] = (15, 10),\n",
    "                                   cmap: str = 'viridis',\n",
    "                                   n_cols: int = 3,\n",
    "                                   save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize each cluster separately.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            n_cols: Number of columns in the subplot grid\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = (n_clusters + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Create separate mask for each cluster\n",
    "        for i in range(n_clusters):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create binary mask for this cluster\n",
    "            mask = (cluster_image == i).astype(float)\n",
    "\n",
    "            # Display the mask\n",
    "            im = ax.imshow(mask, cmap=cmap)\n",
    "\n",
    "            # Set title and turn off axis labels\n",
    "            ax.set_title(f'Cluster {i}')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(n_clusters, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Individual Clusters', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved individual cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_cluster_spectra(self,\n",
    "                                 excitations: Optional[List[float]] = None,\n",
    "                                 figsize: Tuple[int, int] = (15, 10),\n",
    "                                 save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the average spectra for each cluster.\n",
    "\n",
    "        Args:\n",
    "            excitations: List of excitation wavelengths to include (if None, use all)\n",
    "            figsize: Figure size\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Get all feature columns that have the format \"emission-excitation\"\n",
    "        spectral_cols = [col for col in self.feature_cols if '-' in col]\n",
    "\n",
    "        # If no spectral columns, we can't visualize spectra\n",
    "        if not spectral_cols:\n",
    "            raise ValueError(\"No spectral columns found in DataFrame\")\n",
    "\n",
    "        # Parse excitation and emission wavelengths from column names\n",
    "        wavelengths = []\n",
    "        for col in spectral_cols:\n",
    "            try:\n",
    "                emission, excitation = map(float, col.split('-'))\n",
    "                wavelengths.append((emission, excitation, col))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping column {col} - doesn't match expected format\")\n",
    "\n",
    "        # Filter by excitation wavelengths if requested\n",
    "        if excitations:\n",
    "            wavelengths = [(em, ex, col) for em, ex, col in wavelengths if ex in excitations]\n",
    "\n",
    "        # If no wavelengths left, we can't visualize spectra\n",
    "        if not wavelengths:\n",
    "            raise ValueError(\"No valid spectral columns found after filtering\")\n",
    "\n",
    "        # Group by excitation wavelength\n",
    "        excitations = sorted(set(ex for _, ex, _ in wavelengths))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = min(3, len(excitations))\n",
    "        n_cols = (len(excitations) + n_rows - 1) // n_rows\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Get number of clusters\n",
    "        n_clusters = len(np.unique(self.labels))\n",
    "\n",
    "        # Set up colors for clusters\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "        # Plot spectra for each excitation wavelength\n",
    "        for ax_idx, excitation in enumerate(excitations):\n",
    "            if ax_idx < len(axes):\n",
    "                ax = axes[ax_idx]\n",
    "\n",
    "                # Get emission wavelengths and column names for this excitation\n",
    "                excitation_data = [(em, col) for em, ex, col in wavelengths if ex == excitation]\n",
    "\n",
    "                if excitation_data:\n",
    "                    # Sort by emission wavelength\n",
    "                    excitation_data.sort()\n",
    "                    emission_wavelengths = [em for em, _ in excitation_data]\n",
    "                    columns = [col for _, col in excitation_data]\n",
    "\n",
    "                    # Plot average spectrum for each cluster\n",
    "                    for cluster_idx in range(n_clusters):\n",
    "                        # Get cluster data\n",
    "                        cluster_data = self.df[self.df['cluster'] == cluster_idx]\n",
    "\n",
    "                        # Calculate mean spectrum for this cluster\n",
    "                        mean_spectrum = cluster_data[columns].mean().values\n",
    "\n",
    "                        # Plot spectrum\n",
    "                        ax.plot(emission_wavelengths, mean_spectrum, '-',\n",
    "                                color=colors[cluster_idx], linewidth=2,\n",
    "                                label=f'Cluster {cluster_idx}')\n",
    "\n",
    "                    # Set labels and title\n",
    "                    ax.set_xlabel('Emission Wavelength (nm)')\n",
    "                    ax.set_ylabel('Mean Intensity')\n",
    "                    ax.set_title(f'Excitation {excitation} nm')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Add legend to first plot only\n",
    "                    if ax_idx == 0:\n",
    "                        ax.legend(loc='best')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(len(excitations), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Average Spectra by Cluster', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster spectra visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def save_cluster_results(self, output_file: str):\n",
    "        \"\"\"\n",
    "        Save clustering results to a file.\n",
    "\n",
    "        Args:\n",
    "            output_file: Path to save the results\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_kmeans()\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "\n",
    "        # Save based on file type\n",
    "        if ext.lower() in ['.csv']:\n",
    "            # Save to CSV (coordinates and cluster assignments only)\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.pkl', '.pickle']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_pickle(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.parquet']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_parquet(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "def run_kmeans_clustering(\n",
    "    input_file: str,\n",
    "    n_clusters: Optional[int] = None,\n",
    "    max_clusters: int = 10,\n",
    "    output_dir: Optional[str] = None,\n",
    "    random_state: int = 42,\n",
    "    save_metrics_file: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run K-means clustering on hyperspectral data and save results.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to input CSV/parquet/pickle file with flattened hyperspectral data\n",
    "        n_clusters: Number of clusters to use (if None, determine automatically)\n",
    "        max_clusters: Maximum number of clusters to try if determining automatically\n",
    "        output_dir: Directory to save outputs (if None, use same directory as input)\n",
    "        random_state: Random seed for reproducibility\n",
    "        save_metrics_file: Path to save metrics across multiple runs (will append if exists)\n",
    "    \"\"\"\n",
    "    # Determine file type and load data\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    if ext.lower() in ['.csv']:\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif ext.lower() in ['.pkl', '.pickle']:\n",
    "        df = pd.read_pickle(input_file)\n",
    "    elif ext.lower() in ['.parquet']:\n",
    "        df = pd.read_parquet(input_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "    # Set up output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "        if not output_dir:\n",
    "            output_dir = \".\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.basename(input_file)\n",
    "    base_name = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Initialize clustering\n",
    "    clustering = HyperspectralClustering(df)\n",
    "\n",
    "    # Preprocess the data\n",
    "    clustering.preprocess()\n",
    "\n",
    "    # Create a metrics dictionary to store results\n",
    "    run_metrics = {}\n",
    "    run_metrics['input_file'] = input_file\n",
    "    run_metrics['base_name'] = base_name\n",
    "    run_metrics['output_dir'] = output_dir\n",
    "    run_metrics['datetime'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # If number of clusters not specified, find optimal\n",
    "    if n_clusters is None:\n",
    "        silhouette_scores, inertia_values, metrics_df = clustering.find_optimal_clusters(max_clusters, random_state)\n",
    "\n",
    "        # Plot metrics\n",
    "        fig = clustering.plot_cluster_metrics(silhouette_scores, inertia_values, max_clusters)\n",
    "        metrics_path = os.path.join(output_dir, f\"{base_name}_cluster_metrics.png\")\n",
    "        fig.savefig(metrics_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved cluster metrics plot to {metrics_path}\")\n",
    "\n",
    "        # Save detailed metrics to CSV in output directory\n",
    "        metrics_csv_path = os.path.join(output_dir, f\"{base_name}_cluster_metrics.csv\")\n",
    "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "        print(f\"Saved detailed metrics to {metrics_csv_path}\")\n",
    "\n",
    "        # Choose optimal number of clusters (highest silhouette score)\n",
    "        n_clusters = np.argmax(silhouette_scores) + 2  # Add 2 because we start from 2 clusters\n",
    "        print(f\"Selected optimal number of clusters: {n_clusters}\")\n",
    "\n",
    "        # Add metrics to our run metrics\n",
    "        run_metrics['optimal_n_clusters'] = n_clusters\n",
    "        run_metrics['max_silhouette_score'] = silhouette_scores[n_clusters-2]  # Adjust index since we start from 2\n",
    "    else:\n",
    "        # If n_clusters is specified, still calculate metrics for that number\n",
    "        print(f\"Evaluating specified clustering with {n_clusters} clusters...\")\n",
    "        metrics = clustering.evaluate_clustering(n_clusters, random_state)\n",
    "\n",
    "        # Save these metrics\n",
    "        run_metrics.update(metrics)\n",
    "\n",
    "    # Fit K-means with the chosen number of clusters\n",
    "    clustering.fit_kmeans(n_clusters, random_state)\n",
    "\n",
    "    # Add cluster size information to metrics\n",
    "    cluster_sizes = np.bincount(clustering.labels)\n",
    "    for i, size in enumerate(cluster_sizes):\n",
    "        percentage = size / len(clustering.labels) * 100\n",
    "        run_metrics[f'cluster_{i}_size'] = size\n",
    "        run_metrics[f'cluster_{i}_percentage'] = percentage\n",
    "\n",
    "    # Visualize clustering results\n",
    "    fig = clustering.visualize_clusters()\n",
    "    clusters_path = os.path.join(output_dir, f\"{base_name}_kmeans_clusters.png\")\n",
    "    fig.savefig(clusters_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved cluster visualization to {clusters_path}\")\n",
    "\n",
    "    # Visualize individual clusters\n",
    "    fig = clustering.visualize_separate_clusters()\n",
    "    separate_path = os.path.join(output_dir, f\"{base_name}_separate_clusters.png\")\n",
    "    fig.savefig(separate_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved individual cluster visualization to {separate_path}\")\n",
    "\n",
    "    # Visualize cluster spectra\n",
    "    try:\n",
    "        fig = clustering.visualize_cluster_spectra()\n",
    "        spectra_path = os.path.join(output_dir, f\"{base_name}_cluster_spectra.png\")\n",
    "        fig.savefig(spectra_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved cluster spectra visualization to {spectra_path}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not visualize cluster spectra: {e}\")\n",
    "\n",
    "    # Save cluster results\n",
    "    results_path = os.path.join(output_dir, f\"{base_name}_kmeans_results.csv\")\n",
    "    clustering.save_cluster_results(results_path)\n",
    "\n",
    "    # Save run metrics to the metrics file if specified\n",
    "    if save_metrics_file:\n",
    "        metrics_df = pd.DataFrame([run_metrics])\n",
    "\n",
    "        # Check if file exists and append if it does\n",
    "        if os.path.exists(save_metrics_file):\n",
    "            # Load existing metrics\n",
    "            try:\n",
    "                existing_metrics = pd.read_excel(save_metrics_file)\n",
    "                # Append new metrics\n",
    "                combined_metrics = pd.concat([existing_metrics, metrics_df], ignore_index=True)\n",
    "                combined_metrics.to_excel(save_metrics_file, index=False)\n",
    "                print(f\"Appended metrics to existing file: {save_metrics_file}\")\n",
    "            except:\n",
    "                # If error reading existing file, just write new file\n",
    "                metrics_df.to_excel(save_metrics_file, index=False)\n",
    "                print(f\"Created new metrics file: {save_metrics_file}\")\n",
    "        else:\n",
    "            # Create new file\n",
    "            metrics_df.to_excel(save_metrics_file, index=False)\n",
    "            print(f\"Created new metrics file: {save_metrics_file}\")\n",
    "\n",
    "    # Also save a run-specific metrics file in the output directory\n",
    "    run_metrics_path = os.path.join(output_dir, f\"{base_name}_run_metrics.csv\")\n",
    "    pd.DataFrame([run_metrics]).to_csv(run_metrics_path, index=False)\n",
    "    print(f\"Saved run metrics to {run_metrics_path}\")\n",
    "\n",
    "    print(\"K-means clustering complete!\")\n",
    "\n",
    "def run_kmeans_multiple(\n",
    "    input_file: str,\n",
    "    n_clusters_range: List[int],\n",
    "    output_base_dir: str,\n",
    "    save_metrics_file: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run K-means clustering for multiple numbers of clusters and save all metrics.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to input CSV/parquet/pickle file with flattened hyperspectral data\n",
    "        n_clusters_range: List of n_clusters values to try\n",
    "        output_base_dir: Base directory for outputs (will create subdirectories)\n",
    "        save_metrics_file: Path to save metrics across all runs (Excel file)\n",
    "    \"\"\"\n",
    "    print(f\"Running K-means clustering for {len(n_clusters_range)} different n_clusters values\")\n",
    "    print(f\"n_clusters range: {n_clusters_range}\")\n",
    "\n",
    "    # Create base output directory if it doesn't exist\n",
    "    if not os.path.exists(output_base_dir):\n",
    "        os.makedirs(output_base_dir)\n",
    "\n",
    "    # Create metrics file if not exists\n",
    "    if save_metrics_file is None:\n",
    "        save_metrics_file = os.path.join(output_base_dir, \"kmeans_metrics_summary.xlsx\")\n",
    "\n",
    "    # Run clustering for each n_clusters value\n",
    "    for n_clusters in n_clusters_range:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Running K-means with {n_clusters} clusters\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Create output directory for this run\n",
    "        output_dir = os.path.join(output_base_dir, f\"N-Clusters{n_clusters}\")\n",
    "\n",
    "        # Run K-means\n",
    "        run_kmeans_clustering(\n",
    "            input_file=input_file,\n",
    "            n_clusters=n_clusters,\n",
    "            output_dir=output_dir,\n",
    "            save_metrics_file=save_metrics_file\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCompleted all K-means clustering runs\")\n",
    "    print(f\"Summary metrics saved to: {save_metrics_file}\")"
   ],
   "id": "197c419702d55c6f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:33:13.670219Z",
     "start_time": "2025-04-17T22:15:59.396695Z"
    }
   },
   "cell_type": "code",
   "source": "run_kmeans_multiple('../Data/parquet-data/hyperspectral_2d.parquet', range(3, 5), 'KMeansResults/Test')",
   "id": "3338672667675d20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-means clustering for 2 different n_clusters values\n",
      "n_clusters range: range(3, 5)\n",
      "\n",
      "================================================================================\n",
      "Running K-means with 3 clusters\n",
      "================================================================================\n",
      "Loading data from ../hyperspectral_2d.parquet...\n",
      "Loaded dataframe with 1425408 rows and 551 columns\n",
      "Found 549 feature columns in the data\n",
      "Image shape: 1024 Ã— 1392 pixels\n",
      "Preprocessing data...\n",
      "Scaled 549 features\n",
      "Evaluating specified clustering with 3 clusters...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrun_kmeans_multiple\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../hyperspectral_2d.parquet\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../KMeansResults/Test\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 767\u001B[39m, in \u001B[36mrun_kmeans_multiple\u001B[39m\u001B[34m(input_file, n_clusters_range, output_base_dir, save_metrics_file)\u001B[39m\n\u001B[32m    764\u001B[39m     output_dir = os.path.join(output_base_dir, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mN-Clusters\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_clusters\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# Run K-means\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m767\u001B[39m     \u001B[43mrun_kmeans_clustering\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    768\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    769\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    770\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    771\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_metrics_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_metrics_file\u001B[49m\n\u001B[32m    772\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    774\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mCompleted all K-means clustering runs\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    775\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSummary metrics saved to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_metrics_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 662\u001B[39m, in \u001B[36mrun_kmeans_clustering\u001B[39m\u001B[34m(input_file, n_clusters, max_clusters, output_dir, random_state, save_metrics_file)\u001B[39m\n\u001B[32m    659\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    660\u001B[39m     \u001B[38;5;66;03m# If n_clusters is specified, still calculate metrics for that number\u001B[39;00m\n\u001B[32m    661\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEvaluating specified clustering with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_clusters\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m clusters...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m662\u001B[39m     metrics = \u001B[43mclustering\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate_clustering\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    664\u001B[39m     \u001B[38;5;66;03m# Save these metrics\u001B[39;00m\n\u001B[32m    665\u001B[39m     run_metrics.update(metrics)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 141\u001B[39m, in \u001B[36mHyperspectralClustering.evaluate_clustering\u001B[39m\u001B[34m(self, n_clusters, random_state)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;66;03m# Silhouette score (requires at least 2 clusters)\u001B[39;00m\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_clusters >= \u001B[32m2\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m     metrics[\u001B[33m'\u001B[39m\u001B[33msilhouette_score\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43msilhouette_score\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaled_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcluster_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    143\u001B[39m     metrics[\u001B[33m'\u001B[39m\u001B[33msilhouette_score\u001B[39m\u001B[33m'\u001B[39m] = \u001B[38;5;28mfloat\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mnan\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    212\u001B[39m         skip_parameter_validation=(\n\u001B[32m    213\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    214\u001B[39m         )\n\u001B[32m    215\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    219\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    222\u001B[39m     msg = re.sub(\n\u001B[32m    223\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    224\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    226\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:139\u001B[39m, in \u001B[36msilhouette_score\u001B[39m\u001B[34m(X, labels, metric, sample_size, random_state, **kwds)\u001B[39m\n\u001B[32m    137\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    138\u001B[39m         X, labels = X[indices], labels[indices]\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.mean(\u001B[43msilhouette_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m func_sig = signature(func)\n\u001B[32m    193\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:303\u001B[39m, in \u001B[36msilhouette_samples\u001B[39m\u001B[34m(X, labels, metric, **kwds)\u001B[39m\n\u001B[32m    299\u001B[39m kwds[\u001B[33m\"\u001B[39m\u001B[33mmetric\u001B[39m\u001B[33m\"\u001B[39m] = metric\n\u001B[32m    300\u001B[39m reduce_func = functools.partial(\n\u001B[32m    301\u001B[39m     _silhouette_reduce, labels=labels, label_freqs=label_freqs\n\u001B[32m    302\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m results = \u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpairwise_distances_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m intra_clust_dists, inter_clust_dists = results\n\u001B[32m    305\u001B[39m intra_clust_dists = np.concatenate(intra_clust_dists)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2252\u001B[39m, in \u001B[36mpairwise_distances_chunked\u001B[39m\u001B[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[39m\n\u001B[32m   2250\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2251\u001B[39m     X_chunk = X[sl]\n\u001B[32m-> \u001B[39m\u001B[32m2252\u001B[39m D_chunk = \u001B[43mpairwise_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2253\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (X \u001B[38;5;129;01mis\u001B[39;00m Y \u001B[38;5;129;01mor\u001B[39;00m Y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001B[32m   2254\u001B[39m     metric, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2255\u001B[39m ) \u001B[38;5;129;01mis\u001B[39;00m euclidean_distances:\n\u001B[32m   2256\u001B[39m     \u001B[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001B[39;00m\n\u001B[32m   2257\u001B[39m     \u001B[38;5;66;03m# i.e. \"l2\"\u001B[39;00m\n\u001B[32m   2258\u001B[39m     D_chunk.flat[sl.start :: _num_samples(X) + \u001B[32m1\u001B[39m] = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m func_sig = signature(func)\n\u001B[32m    193\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2480\u001B[39m, in \u001B[36mpairwise_distances\u001B[39m\u001B[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001B[39m\n\u001B[32m   2477\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001B[32m   2478\u001B[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001B[32m-> \u001B[39m\u001B[32m2480\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_parallel_pairwise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1973\u001B[39m, in \u001B[36m_parallel_pairwise\u001B[39m\u001B[34m(X, Y, func, n_jobs, **kwds)\u001B[39m\n\u001B[32m   1970\u001B[39m X, Y, dtype = _return_float_dtype(X, Y)\n\u001B[32m   1972\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(n_jobs) == \u001B[32m1\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1973\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1975\u001B[39m \u001B[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001B[39;00m\n\u001B[32m   1976\u001B[39m fd = delayed(_dist_wrapper)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m func_sig = signature(func)\n\u001B[32m    193\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:388\u001B[39m, in \u001B[36meuclidean_distances\u001B[39m\u001B[34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001B[39m\n\u001B[32m    382\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m Y_norm_squared.shape != (\u001B[32m1\u001B[39m, Y.shape[\u001B[32m0\u001B[39m]):\n\u001B[32m    383\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    384\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIncompatible dimensions for Y of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mY.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    385\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mY_norm_squared of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moriginal_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    386\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_euclidean_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:424\u001B[39m, in \u001B[36m_euclidean_distances\u001B[39m\u001B[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001B[39m\n\u001B[32m    421\u001B[39m     distances = _euclidean_distances_upcast(X, XX, Y, YY)\n\u001B[32m    422\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    423\u001B[39m     \u001B[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m424\u001B[39m     distances = -\u001B[32m2\u001B[39m * \u001B[43msafe_sparse_dot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m.\u001B[49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdense_output\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    425\u001B[39m     distances += XX\n\u001B[32m    426\u001B[39m     distances += YY\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:206\u001B[39m, in \u001B[36msafe_sparse_dot\u001B[39m\u001B[34m(a, b, dense_output)\u001B[39m\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    203\u001B[39m     ret = a @ b\n\u001B[32m    205\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m     \u001B[43msparse\u001B[49m\u001B[43m.\u001B[49m\u001B[43missparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    207\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m sparse.issparse(b)\n\u001B[32m    208\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m dense_output\n\u001B[32m    209\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ret, \u001B[33m\"\u001B[39m\u001B[33mtoarray\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    210\u001B[39m ):\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ret.toarray()\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1401\u001B[39m, in \u001B[36missparse\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m   1395\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"A namespace class to separate sparray from spmatrix\"\"\"\u001B[39;00m\n\u001B[32m   1398\u001B[39m sparray.\u001B[34m__doc__\u001B[39m = _spbase.\u001B[34m__doc__\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1401\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34missparse\u001B[39m(x):\n\u001B[32m   1402\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001B[39;00m\n\u001B[32m   1403\u001B[39m \n\u001B[32m   1404\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1430\u001B[39m \u001B[33;03m    False\u001B[39;00m\n\u001B[32m   1431\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   1432\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, _spbase)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "15938065e688b3cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
