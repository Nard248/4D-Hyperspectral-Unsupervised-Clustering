{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap\n",
    "from typing import Tuple, List, Optional, Dict, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class HyperspectralDBSCAN:\n",
    "    \"\"\"\n",
    "    Class for unsupervised DBSCAN clustering of hyperspectral data and visualizing results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, image_shape: Optional[Tuple[int, int]] = None):\n",
    "        \"\"\"\n",
    "        Initialize with the flattened hyperspectral dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing flattened hyperspectral data with x, y coordinates\n",
    "            image_shape: Tuple of (height, width) of the original image. If None, inferred from data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        # Extract spatial coordinates and feature columns\n",
    "        self.x_col = 'x'\n",
    "        self.y_col = 'y'\n",
    "\n",
    "        # Validate that x and y columns exist\n",
    "        if self.x_col not in df.columns or self.y_col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{self.x_col}' and '{self.y_col}' columns\")\n",
    "\n",
    "        # Get feature columns (all columns except x and y)\n",
    "        self.feature_cols = [col for col in df.columns if col not in [self.x_col, self.y_col]]\n",
    "\n",
    "        if len(self.feature_cols) == 0:\n",
    "            raise ValueError(\"No feature columns found in DataFrame\")\n",
    "\n",
    "        print(f\"Found {len(self.feature_cols)} feature columns in the data\")\n",
    "\n",
    "        # Determine image shape if not provided\n",
    "        if image_shape is None:\n",
    "            self.height = int(df[self.y_col].max()) + 1\n",
    "            self.width = int(df[self.x_col].max()) + 1\n",
    "        else:\n",
    "            self.height, self.width = image_shape\n",
    "\n",
    "        print(f\"Image shape: {self.height} Ã— {self.width} pixels\")\n",
    "\n",
    "        # Initialize model and results\n",
    "        self.model = None\n",
    "        self.labels = None\n",
    "        self.feature_scaler = None\n",
    "        self.scaled_features = None\n",
    "\n",
    "    def preprocess(self, handle_nan: str = 'drop_feature'):\n",
    "        \"\"\"\n",
    "        Preprocess the data for clustering.\n",
    "\n",
    "        Args:\n",
    "            handle_nan: Strategy for handling NaN values ('drop_feature', 'fill_zero', 'fill_mean')\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Extract features\n",
    "        features = self.df[self.feature_cols].copy()\n",
    "\n",
    "        # Count missing values\n",
    "        nan_counts = features.isna().sum()\n",
    "        nan_features = nan_counts[nan_counts > 0]\n",
    "\n",
    "        if len(nan_features) > 0:\n",
    "            print(f\"Found {len(nan_features)} features with missing values\")\n",
    "            print(f\"Top 5 features with most NaNs: {nan_features.sort_values(ascending=False).head()}\")\n",
    "\n",
    "            # Handle missing values based on strategy\n",
    "            if handle_nan == 'drop_feature':\n",
    "                # Drop features with any NaN values\n",
    "                good_features = [col for col in self.feature_cols if nan_counts[col] == 0]\n",
    "                features = self.df[good_features].copy()\n",
    "                print(f\"Dropped {len(self.feature_cols) - len(good_features)} features with NaNs\")\n",
    "                self.feature_cols = good_features\n",
    "\n",
    "            elif handle_nan == 'fill_zero':\n",
    "                # Replace NaN with zeros\n",
    "                features.fillna(0, inplace=True)\n",
    "                print(\"Filled NaN values with zeros\")\n",
    "\n",
    "            elif handle_nan == 'fill_mean':\n",
    "                # Replace NaN with feature means\n",
    "                features.fillna(features.mean(), inplace=True)\n",
    "                print(\"Filled NaN values with feature means\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown handle_nan strategy: {handle_nan}\")\n",
    "\n",
    "        # Scale the features\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.scaled_features = self.feature_scaler.fit_transform(features)\n",
    "        print(f\"Scaled {self.scaled_features.shape[1]} features\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate_dbscan_params(self, n_neighbors: int = 5, quantile: float = 0.95, n_samples: int = 10000):\n",
    "        \"\"\"\n",
    "        Estimate good parameters for DBSCAN using nearest neighbors distances.\n",
    "\n",
    "        Args:\n",
    "            n_neighbors: Number of neighbors to consider for distance calculation\n",
    "            quantile: Quantile of k-dist to use as eps estimate\n",
    "            n_samples: Number of samples to use for estimation (to speed up computation)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (estimated_eps, estimated_min_samples)\n",
    "        \"\"\"\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        print(f\"Estimating DBSCAN parameters using {n_samples} sample points...\")\n",
    "\n",
    "        # Use a subset of points if data is large\n",
    "        if n_samples and n_samples < len(self.scaled_features):\n",
    "            indices = np.random.choice(len(self.scaled_features), n_samples, replace=False)\n",
    "            sample_features = self.scaled_features[indices]\n",
    "        else:\n",
    "            sample_features = self.scaled_features\n",
    "\n",
    "        # Compute nearest neighbor distances\n",
    "        start_time = time.time()\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(sample_features)\n",
    "        distances, _ = nbrs.kneighbors(sample_features)\n",
    "\n",
    "        # Sort the k-distances\n",
    "        k_distances = distances[:, n_neighbors-1]\n",
    "        k_distances.sort()\n",
    "\n",
    "        # Find the \"elbow\" point in the k-distance graph\n",
    "        estimated_eps = k_distances[int(len(k_distances) * quantile)]\n",
    "\n",
    "        # Recommend min_samples based on dimensionality\n",
    "        # Common rule: min_samples = 2 * num_dimensions\n",
    "        estimated_min_samples = max(5, 2 * sample_features.shape[1])\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Parameter estimation completed in {duration:.2f}s\")\n",
    "        print(f\"Estimated parameters: eps={estimated_eps:.4f}, min_samples={estimated_min_samples}\")\n",
    "\n",
    "        # Create a plot to visualize k-distances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.arange(len(k_distances)), k_distances, 'b-')\n",
    "        plt.axhline(y=estimated_eps, color='r', linestyle='--',\n",
    "                   label=f'Suggested eps: {estimated_eps:.4f}')\n",
    "        plt.xlabel('Points sorted by distance')\n",
    "        plt.ylabel(f'Distance to {n_neighbors}th nearest neighbor')\n",
    "        plt.title('K-distance Graph for DBSCAN Parameter Estimation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return estimated_eps, estimated_min_samples\n",
    "\n",
    "    def fit_dbscan(self, eps: float = 0.5, min_samples: int = 5):\n",
    "        \"\"\"\n",
    "        Fit DBSCAN clustering on the hyperspectral data.\n",
    "\n",
    "        Args:\n",
    "            eps: The maximum distance between two samples for one to be considered a neighbor of the other\n",
    "            min_samples: The number of samples in a neighborhood for a point to be considered a core point\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(f\"Fitting DBSCAN with eps={eps}, min_samples={min_samples}...\")\n",
    "\n",
    "        # Ensure data is preprocessed\n",
    "        if self.scaled_features is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        # Track start time for performance monitoring\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        self.model = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "        self.labels = self.model.fit_predict(self.scaled_features)\n",
    "\n",
    "        # Track end time\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"DBSCAN completed in {duration:.2f}s\")\n",
    "\n",
    "        # Add cluster labels to dataframe\n",
    "        self.df['cluster'] = self.labels\n",
    "\n",
    "        # Calculate cluster sizes (including noise points labeled as -1)\n",
    "        unique_labels = np.unique(self.labels)\n",
    "        n_clusters = len(unique_labels)\n",
    "        n_noise = np.sum(self.labels == -1)\n",
    "\n",
    "        print(f\"Found {n_clusters} clusters including noise points\")\n",
    "        print(f\"Number of noise points: {n_noise} ({n_noise/len(self.labels)*100:.2f}%)\")\n",
    "\n",
    "        print(\"Cluster sizes:\")\n",
    "        cluster_sizes = np.bincount(self.labels[self.labels >= 0])\n",
    "        for i, size in enumerate(cluster_sizes):\n",
    "            percentage = size / len(self.labels) * 100\n",
    "            print(f\"  Cluster {i}: {size} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def reconstruct_cluster_image(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the cluster assignments into the original image shape.\n",
    "\n",
    "        Returns:\n",
    "            2D array of cluster assignments with shape (height, width)\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Create empty image array (default to -1 for unassigned pixels)\n",
    "        cluster_image = np.full((self.height, self.width), -1, dtype=int)\n",
    "\n",
    "        # Fill in cluster assignments\n",
    "        for idx, row in self.df.iterrows():\n",
    "            x, y = int(row[self.x_col]), int(row[self.y_col])\n",
    "            if 0 <= x < self.width and 0 <= y < self.height:\n",
    "                cluster_image[y, x] = row['cluster']\n",
    "\n",
    "        return cluster_image\n",
    "\n",
    "    def visualize_clusters(self,\n",
    "                          figsize: Tuple[int, int] = (12, 10),\n",
    "                          cmap: Optional[Union[str, ListedColormap]] = None,\n",
    "                          noise_color: str = 'black',\n",
    "                          save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the clustering results by reconstructing the original image.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            noise_color: Color to use for noise points (cluster -1)\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        unique_labels = np.unique(self.labels)\n",
    "\n",
    "        # Count number of actual clusters (excluding noise points)\n",
    "        n_clusters = len([l for l in unique_labels if l >= 0])\n",
    "\n",
    "        # Create a custom colormap that handles noise points specially\n",
    "        if cmap is None:\n",
    "            # For regular clusters (excluding noise), choose colormap based on number of clusters\n",
    "            if n_clusters <= 10:\n",
    "                # For few clusters, use distinct colors\n",
    "                tab_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "                # Add black at the beginning for noise points (label -1)\n",
    "                colors = [noise_color] + tab_colors[:n_clusters]\n",
    "                cmap = ListedColormap(colors)\n",
    "            else:\n",
    "                # For many clusters, modify a continuous colormap\n",
    "                base_cmap = plt.cm.get_cmap('tab20' if n_clusters <= 20 else 'viridis', n_clusters)\n",
    "                # Create a new colormap with black for noise points\n",
    "                colors = [noise_color]  # Start with noise color\n",
    "                colors.extend([base_cmap(i) for i in range(n_clusters)])  # Add colors for actual clusters\n",
    "                cmap = ListedColormap(colors)\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Display the image - ensure noise points (label -1) map to the first color in the colormap\n",
    "        # Add 1 to all labels to shift them up (making -1 â†’ 0, 0 â†’ 1, etc.)\n",
    "        shifted_image = cluster_image + 1\n",
    "        im = ax.imshow(shifted_image, cmap=cmap, vmin=0, vmax=n_clusters)\n",
    "\n",
    "        # Create a custom colorbar with proper labels\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Cluster')\n",
    "\n",
    "        # Adjust colorbar ticks and labels to show -1 for noise\n",
    "        tick_locs = np.arange(n_clusters + 1) + 0.5  # Center ticks\n",
    "        cbar.set_ticks(tick_locs)\n",
    "        tick_labels = ['-1 (Noise)'] + [str(i) for i in range(n_clusters)]\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f'DBSCAN Clustering Results ({n_clusters} clusters, {np.sum(cluster_image == -1)} noise points)')\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_separate_clusters(self,\n",
    "                                   figsize: Tuple[int, int] = (15, 10),\n",
    "                                   cmap: str = 'viridis',\n",
    "                                   n_cols: int = 3,\n",
    "                                   include_noise: bool = True,\n",
    "                                   save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize each cluster separately.\n",
    "\n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap to use for visualization\n",
    "            n_cols: Number of columns in the subplot grid\n",
    "            include_noise: Whether to include noise points as a separate cluster\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "\n",
    "        # Get unique cluster labels\n",
    "        unique_labels = sorted(np.unique(self.labels))\n",
    "\n",
    "        # Decide whether to include noise in visualization\n",
    "        if not include_noise and -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = (n_clusters + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Create separate mask for each cluster\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create binary mask for this cluster\n",
    "            mask = (cluster_image == label).astype(float)\n",
    "\n",
    "            # Display the mask\n",
    "            im = ax.imshow(mask, cmap=cmap)\n",
    "\n",
    "            # Set title and turn off axis labels\n",
    "            cluster_name = f'Noise Points' if label == -1 else f'Cluster {label}'\n",
    "            ax.set_title(cluster_name)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(n_clusters, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Individual Clusters', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved individual cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_cluster_spectra(self,\n",
    "                                 excitations: Optional[List[float]] = None,\n",
    "                                 figsize: Tuple[int, int] = (15, 10),\n",
    "                                 include_noise: bool = False,\n",
    "                                 save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the average spectra for each cluster.\n",
    "\n",
    "        Args:\n",
    "            excitations: List of excitation wavelengths to include (if None, use all)\n",
    "            figsize: Figure size\n",
    "            include_noise: Whether to include noise points\n",
    "            save_path: Path to save the visualization image\n",
    "\n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Get all feature columns that have the format \"emission-excitation\"\n",
    "        spectral_cols = [col for col in self.feature_cols if '-' in col]\n",
    "\n",
    "        # If no spectral columns, we can't visualize spectra\n",
    "        if not spectral_cols:\n",
    "            raise ValueError(\"No spectral columns found in DataFrame\")\n",
    "\n",
    "        # Parse excitation and emission wavelengths from column names\n",
    "        wavelengths = []\n",
    "        for col in spectral_cols:\n",
    "            try:\n",
    "                emission, excitation = map(float, col.split('-'))\n",
    "                wavelengths.append((emission, excitation, col))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping column {col} - doesn't match expected format\")\n",
    "\n",
    "        # Filter by excitation wavelengths if requested\n",
    "        if excitations:\n",
    "            wavelengths = [(em, ex, col) for em, ex, col in wavelengths if ex in excitations]\n",
    "\n",
    "        # If no wavelengths left, we can't visualize spectra\n",
    "        if not wavelengths:\n",
    "            raise ValueError(\"No valid spectral columns found after filtering\")\n",
    "\n",
    "        # Group by excitation wavelength\n",
    "        excitations = sorted(set(ex for _, ex, _ in wavelengths))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = min(3, len(excitations))\n",
    "        n_cols = (len(excitations) + n_rows - 1) // n_rows\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Get unique cluster labels (sorted, excluding noise if specified)\n",
    "        unique_labels = sorted(np.unique(self.labels))\n",
    "        if not include_noise and -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        # Set up colors for clusters - using a colormap that works well for many clusters\n",
    "        if n_clusters <= 10:\n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, max(10, n_clusters)))\n",
    "        else:\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "        # Plot spectra for each excitation wavelength\n",
    "        for ax_idx, excitation in enumerate(excitations):\n",
    "            if ax_idx < len(axes):\n",
    "                ax = axes[ax_idx]\n",
    "\n",
    "                # Get emission wavelengths and column names for this excitation\n",
    "                excitation_data = [(em, col) for em, ex, col in wavelengths if ex == excitation]\n",
    "\n",
    "                if excitation_data:\n",
    "                    # Sort by emission wavelength\n",
    "                    excitation_data.sort()\n",
    "                    emission_wavelengths = [em for em, _ in excitation_data]\n",
    "                    columns = [col for _, col in excitation_data]\n",
    "\n",
    "                    # Plot average spectrum for each cluster\n",
    "                    for i, label in enumerate(unique_labels):\n",
    "                        # Get cluster data\n",
    "                        cluster_data = self.df[self.df['cluster'] == label]\n",
    "\n",
    "                        # Skip if cluster is empty\n",
    "                        if len(cluster_data) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Calculate mean spectrum for this cluster\n",
    "                        mean_spectrum = cluster_data[columns].mean().values\n",
    "\n",
    "                        # Determine label and style\n",
    "                        if label == -1:\n",
    "                            cluster_name = \"Noise\"\n",
    "                            line_style = \":\"  # dotted line for noise\n",
    "                        else:\n",
    "                            cluster_name = f'Cluster {label}'\n",
    "                            line_style = \"-\"  # solid line for clusters\n",
    "\n",
    "                        # Plot spectrum\n",
    "                        color_idx = i if label != -1 else -1  # Use last color for noise\n",
    "                        ax.plot(emission_wavelengths, mean_spectrum,\n",
    "                                line_style, color=colors[color_idx],\n",
    "                                linewidth=2, label=cluster_name)\n",
    "\n",
    "                    # Set labels and title\n",
    "                    ax.set_xlabel('Emission Wavelength (nm)')\n",
    "                    ax.set_ylabel('Mean Intensity')\n",
    "                    ax.set_title(f'Excitation {excitation} nm')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Add legend to first plot only\n",
    "                    if ax_idx == 0:\n",
    "                        ax.legend(loc='best')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(len(excitations), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Average Spectra by Cluster', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster spectra visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def save_cluster_results(self, output_file: str):\n",
    "        \"\"\"\n",
    "        Save clustering results to a file.\n",
    "\n",
    "        Args:\n",
    "            output_file: Path to save the results\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "\n",
    "        # Save based on file type\n",
    "        if ext.lower() in ['.csv']:\n",
    "            # Save to CSV (coordinates and cluster assignments only)\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.pkl', '.pickle']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_pickle(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.parquet']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_parquet(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")"
   ],
   "id": "5a7325e6b56bbd7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def run_dbscan_clustering(\n",
    "    input_file: str,\n",
    "    eps: Optional[float] = None,\n",
    "    min_samples: Optional[int] = None,\n",
    "    auto_params: bool = True,\n",
    "    output_dir: Optional[str] = None,\n",
    "    sample_size: int = 10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Run DBSCAN clustering on hyperspectral data and save results.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to input CSV/parquet/pickle file with flattened hyperspectral data\n",
    "        eps: DBSCAN eps parameter (max distance between points to be considered neighbors)\n",
    "        min_samples: DBSCAN min_samples parameter (min points to form a dense region)\n",
    "        auto_params: Whether to automatically estimate parameters using k-dist method\n",
    "        output_dir: Directory to save outputs (if None, use same directory as input)\n",
    "        sample_size: Number of samples to use for parameter estimation (if auto_params=True)\n",
    "    \"\"\"\n",
    "    # Determine file type and load data\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    if ext.lower() in ['.csv']:\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif ext.lower() in ['.pkl', '.pickle']:\n",
    "        df = pd.read_pickle(input_file)\n",
    "    elif ext.lower() in ['.parquet']:\n",
    "        df = pd.read_parquet(input_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "    # Set up output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "        if not output_dir:\n",
    "            output_dir = \".\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.basename(input_file)\n",
    "    base_name = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Initialize clustering\n",
    "    clustering = HyperspectralDBSCAN(df)\n",
    "\n",
    "    # Preprocess the data\n",
    "    clustering.preprocess()\n",
    "\n",
    "    # If auto_params is True, estimate parameters\n",
    "    if auto_params:\n",
    "        estimated_eps, estimated_min_samples = clustering.estimate_dbscan_params(\n",
    "            n_samples=sample_size, quantile=0.95\n",
    "        )\n",
    "\n",
    "        # Save k-distance plot\n",
    "        plt.savefig(os.path.join(output_dir, f\"{base_name}_kdist_plot.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Use estimated parameters if not explicitly provided\n",
    "        if eps is None:\n",
    "            eps = estimated_eps\n",
    "        if min_samples is None:\n",
    "            min_samples = estimated_min_samples\n",
    "\n",
    "    # Use default parameters if still None\n",
    "    if eps is None:\n",
    "        eps = 0.5\n",
    "    if min_samples is None:\n",
    "        min_samples = 5\n",
    "\n",
    "    # Fit DBSCAN with the parameters\n",
    "    clustering.fit_dbscan(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Visualize clustering results\n",
    "    fig = clustering.visualize_clusters()\n",
    "    clusters_path = os.path.join(output_dir, f\"{base_name}_dbscan_clusters.png\")\n",
    "    fig.savefig(clusters_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved cluster visualization to {clusters_path}\")\n",
    "\n",
    "    # Visualize individual clusters\n",
    "    fig = clustering.visualize_separate_clusters()\n",
    "    separate_path = os.path.join(output_dir, f\"{base_name}_dbscan_separate_clusters.png\")\n",
    "    fig.savefig(separate_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved individual cluster visualization to {separate_path}\")\n",
    "\n",
    "    # Visualize cluster spectra\n",
    "    try:\n",
    "        fig = clustering.visualize_cluster_spectra()\n",
    "        spectra_path = os.path.join(output_dir, f\"{base_name}_dbscan_cluster_spectra.png\")\n",
    "        fig.savefig(spectra_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved cluster spectra visualization to {spectra_path}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not visualize cluster spectra: {e}\")\n",
    "\n",
    "    # Save cluster results\n",
    "    results_path = os.path.join(output_dir, f\"{base_name}_dbscan_results.csv\")\n",
    "    clustering.save_cluster_results(results_path)\n",
    "\n",
    "    # Save parameters used\n",
    "    with open(os.path.join(output_dir, f\"{base_name}_dbscan_params.txt\"), 'w') as f:\n",
    "        f.write(f\"DBSCAN Parameters:\\n\")\n",
    "        f.write(f\"eps: {eps}\\n\")\n",
    "        f.write(f\"min_samples: {min_samples}\\n\")\n",
    "        f.write(f\"Number of clusters: {len(np.unique(clustering.labels)) - (1 if -1 in clustering.labels else 0)}\\n\")\n",
    "        f.write(f\"Number of noise points: {np.sum(clustering.labels == -1)}\\n\")\n",
    "        f.write(f\"Percentage of noise: {np.sum(clustering.labels == -1) / len(clustering.labels) * 100:.2f}%\\n\")\n",
    "\n",
    "    print(\"DBSCAN clustering complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "run_dbscan_clustering(\n",
    "    \"../Data/Kiwi Experiment/parquests/KiwiDataMasked.parquet\",\n",
    "    auto_params=True,\n",
    "    output_dir=\"DBScanResults/Masked\",\n",
    ")"
   ],
   "id": "8b91e5802eb9da5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap\n",
    "from typing import Tuple, List, Optional, Dict, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc  # Garbage collector\n",
    "from joblib import Memory  # For caching\n",
    "\n",
    "class MemoryEfficientDBSCAN:\n",
    "    \"\"\"\n",
    "    Memory-optimized implementation of DBSCAN for large hyperspectral datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, image_shape: Optional[Tuple[int, int]] = None):\n",
    "        \"\"\"\n",
    "        Initialize with the flattened hyperspectral dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing flattened hyperspectral data with x, y coordinates\n",
    "            image_shape: Tuple of (height, width) of the original image. If None, inferred from data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        # Extract spatial coordinates and feature columns\n",
    "        self.x_col = 'x'\n",
    "        self.y_col = 'y'\n",
    "\n",
    "        # Validate that x and y columns exist\n",
    "        if self.x_col not in df.columns or self.y_col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{self.x_col}' and '{self.y_col}' columns\")\n",
    "\n",
    "        # Get feature columns (all columns except x and y)\n",
    "        self.feature_cols = [col for col in df.columns if col not in [self.x_col, self.y_col]]\n",
    "\n",
    "        if len(self.feature_cols) == 0:\n",
    "            raise ValueError(\"No feature columns found in DataFrame\")\n",
    "\n",
    "        print(f\"Found {len(self.feature_cols)} feature columns in the data\")\n",
    "\n",
    "        # Determine image shape if not provided\n",
    "        if image_shape is None:\n",
    "            self.height = int(df[self.y_col].max()) + 1\n",
    "            self.width = int(df[self.x_col].max()) + 1\n",
    "        else:\n",
    "            self.height, self.width = image_shape\n",
    "\n",
    "        print(f\"Image shape: {self.height} Ã— {self.width} pixels\")\n",
    "\n",
    "        # Initialize model and results\n",
    "        self.model = None\n",
    "        self.labels = None\n",
    "        self.feature_scaler = None\n",
    "        self.scaled_features = None\n",
    "        self.pca_model = None\n",
    "        self.reduced_features = None\n",
    "\n",
    "    def preprocess(self, handle_nan: str = 'drop_feature', n_components: Optional[int] = None,\n",
    "                  pca_variance: float = 0.95):\n",
    "        \"\"\"\n",
    "        Preprocess the data for clustering with dimensionality reduction.\n",
    "\n",
    "        Args:\n",
    "            handle_nan: Strategy for handling NaN values ('drop_feature', 'fill_zero', 'fill_mean')\n",
    "            n_components: Number of PCA components to keep (if None, determined by variance)\n",
    "            pca_variance: Proportion of variance to preserve if n_components is None\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Extract features\n",
    "        features = self.df[self.feature_cols].copy()\n",
    "\n",
    "        # Count missing values\n",
    "        nan_counts = features.isna().sum()\n",
    "        nan_features = nan_counts[nan_counts > 0]\n",
    "\n",
    "        if len(nan_features) > 0:\n",
    "            print(f\"Found {len(nan_features)} features with missing values\")\n",
    "            print(f\"Top 5 features with most NaNs: {nan_features.sort_values(ascending=False).head()}\")\n",
    "\n",
    "            # Handle missing values based on strategy\n",
    "            if handle_nan == 'drop_feature':\n",
    "                # Drop features with any NaN values\n",
    "                good_features = [col for col in self.feature_cols if nan_counts[col] == 0]\n",
    "                features = self.df[good_features].copy()\n",
    "                print(f\"Dropped {len(self.feature_cols) - len(good_features)} features with NaNs\")\n",
    "                self.feature_cols = good_features\n",
    "\n",
    "            elif handle_nan == 'fill_zero':\n",
    "                # Replace NaN with zeros\n",
    "                features.fillna(0, inplace=True)\n",
    "                print(\"Filled NaN values with zeros\")\n",
    "\n",
    "            elif handle_nan == 'fill_mean':\n",
    "                # Replace NaN with feature means\n",
    "                features.fillna(features.mean(), inplace=True)\n",
    "                print(\"Filled NaN values with feature means\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown handle_nan strategy: {handle_nan}\")\n",
    "\n",
    "        # Scale the features\n",
    "        print(\"Scaling features...\")\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.scaled_features = self.feature_scaler.fit_transform(features)\n",
    "\n",
    "        # Run garbage collection to free memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        print(f\"Applying PCA dimensionality reduction...\")\n",
    "        if n_components is None:\n",
    "            # Determine number of components to keep based on explained variance\n",
    "            self.pca_model = PCA(n_components=pca_variance, svd_solver='randomized')\n",
    "        else:\n",
    "            self.pca_model = PCA(n_components=n_components, svd_solver='randomized')\n",
    "\n",
    "        self.reduced_features = self.pca_model.fit_transform(self.scaled_features)\n",
    "\n",
    "        # Release memory from original scaled features\n",
    "        self.scaled_features = None\n",
    "        gc.collect()\n",
    "\n",
    "        components_kept = self.pca_model.n_components_\n",
    "        variance_explained = np.sum(self.pca_model.explained_variance_ratio_)\n",
    "\n",
    "        print(f\"Reduced features from {len(self.feature_cols)} to {components_kept} dimensions\")\n",
    "        print(f\"Preserved {variance_explained:.2%} of the original variance\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate_dbscan_params(self, n_samples: int = 10000, n_neighbors: int = 5, quantile: float = 0.95):\n",
    "        \"\"\"\n",
    "        Estimate good parameters for DBSCAN using nearest neighbors distances on a sample.\n",
    "\n",
    "        Args:\n",
    "            n_samples: Number of samples to use for estimation\n",
    "            n_neighbors: Number of neighbors to consider for distance calculation\n",
    "            quantile: Quantile of k-dist to use as eps estimate\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (estimated_eps, estimated_min_samples)\n",
    "        \"\"\"\n",
    "        if self.reduced_features is None:\n",
    "            raise ValueError(\"Run preprocess() before estimating parameters\")\n",
    "\n",
    "        print(f\"Estimating DBSCAN parameters using {n_samples} sample points...\")\n",
    "\n",
    "        # Use a subset of points to estimate parameters\n",
    "        if n_samples >= len(self.reduced_features):\n",
    "            sample_features = self.reduced_features\n",
    "            n_samples = len(self.reduced_features)\n",
    "            print(\"Using all available data points for parameter estimation\")\n",
    "        else:\n",
    "            # Take a random sample without replacement\n",
    "            indices = np.random.choice(len(self.reduced_features), n_samples, replace=False)\n",
    "            sample_features = self.reduced_features[indices]\n",
    "\n",
    "        # Compute nearest neighbor distances on the sample\n",
    "        start_time = time.time()\n",
    "        print(\"Computing nearest neighbors... (this may take a while)\")\n",
    "\n",
    "        # Use batch processing for k-distance calculation to save memory\n",
    "        batch_size = min(5000, n_samples)\n",
    "        k_distances = np.zeros(n_samples)\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(sample_features)\n",
    "\n",
    "        # Process in batches to avoid memory issues\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            end_idx = min(i + batch_size, n_samples)\n",
    "            batch = sample_features[i:end_idx]\n",
    "            distances, _ = nbrs.kneighbors(batch)\n",
    "            k_distances[i:end_idx] = distances[:, n_neighbors-1]\n",
    "\n",
    "            # Report progress\n",
    "            if (i // batch_size) % 5 == 0:\n",
    "                print(f\"  Processed {end_idx}/{n_samples} samples...\")\n",
    "\n",
    "        # Sort the k-distances\n",
    "        k_distances.sort()\n",
    "\n",
    "        # Find the \"elbow\" point in the k-distance graph\n",
    "        estimated_eps = k_distances[int(len(k_distances) * quantile)]\n",
    "\n",
    "        # Recommend min_samples based on dimensionality\n",
    "        # Common rule: min_samples = 2 * num_dimensions\n",
    "        estimated_min_samples = max(5, 2 * self.reduced_features.shape[1])\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Parameter estimation completed in {duration:.2f}s\")\n",
    "        print(f\"Estimated parameters: eps={estimated_eps:.4f}, min_samples={estimated_min_samples}\")\n",
    "\n",
    "        # Create a plot to visualize k-distances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.arange(len(k_distances)), k_distances, 'b-')\n",
    "        plt.axhline(y=estimated_eps, color='r', linestyle='--',\n",
    "                   label=f'Suggested eps: {estimated_eps:.4f}')\n",
    "        plt.xlabel('Points sorted by distance')\n",
    "        plt.ylabel(f'Distance to {n_neighbors}th nearest neighbor')\n",
    "        plt.title('K-distance Graph for DBSCAN Parameter Estimation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Free memory\n",
    "        del nbrs, sample_features, k_distances\n",
    "        gc.collect()\n",
    "\n",
    "        return estimated_eps, estimated_min_samples\n",
    "\n",
    "    def fit_dbscan(self, eps: float = 0.5, min_samples: int = 5, max_samples: Optional[int] = None,\n",
    "                  batch_size: int = 20000):\n",
    "        \"\"\"\n",
    "        Fit DBSCAN clustering on the hyperspectral data using memory-efficient approach.\n",
    "\n",
    "        Args:\n",
    "            eps: The maximum distance between two samples for one to be considered a neighbor\n",
    "            min_samples: The number of samples in a neighborhood for a point to be considered a core point\n",
    "            max_samples: Maximum number of samples to use for clustering (None = all)\n",
    "            batch_size: Size of batches for processing larger datasets\n",
    "\n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        if self.reduced_features is None:\n",
    "            raise ValueError(\"Run preprocess() before fitting DBSCAN\")\n",
    "\n",
    "        print(f\"Fitting DBSCAN with eps={eps}, min_samples={min_samples}...\")\n",
    "\n",
    "        # Determine if we need to sample data\n",
    "        total_points = len(self.reduced_features)\n",
    "        if max_samples is not None and max_samples < total_points:\n",
    "            print(f\"Sampling {max_samples} points from {total_points} total points\")\n",
    "            indices = np.random.choice(total_points, max_samples, replace=False)\n",
    "            features_to_cluster = self.reduced_features[indices]\n",
    "\n",
    "            # Keep track of the sampled indices for later\n",
    "            self.sampled_indices = indices\n",
    "            using_sample = True\n",
    "        else:\n",
    "            features_to_cluster = self.reduced_features\n",
    "            using_sample = False\n",
    "\n",
    "        # Track start time for performance monitoring\n",
    "        start_time = time.time()\n",
    "\n",
    "        # If the dataset is very large, use batch processing\n",
    "        if len(features_to_cluster) > batch_size and False:  # Disabling batch processing for now (can be complex to implement correctly)\n",
    "            print(f\"Using batch processing with batch size {batch_size}\")\n",
    "            # This would require a custom implementation of batch DBSCAN\n",
    "            # Which is complex and beyond the scope of this response\n",
    "            raise NotImplementedError(\"Batch processing for DBSCAN not yet implemented\")\n",
    "        else:\n",
    "            # Use standard DBSCAN implementation\n",
    "            print(f\"Using standard DBSCAN on {len(features_to_cluster)} points\")\n",
    "            self.model = DBSCAN(eps=eps, min_samples=min_samples,\n",
    "                              algorithm='kd_tree',  # More memory efficient\n",
    "                              leaf_size=40,  # Increased leaf size can reduce memory\n",
    "                              n_jobs=-1)  # Use all cores\n",
    "\n",
    "            labels = self.model.fit_predict(features_to_cluster)\n",
    "\n",
    "        # Track end time\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"DBSCAN completed in {duration:.2f}s\")\n",
    "\n",
    "        # If we used a sample, we need to propagate labels to all points\n",
    "        if using_sample:\n",
    "            print(\"Propagating labels from sample to all points...\")\n",
    "            # Create full labels array (initialize all to noise)\n",
    "            full_labels = np.full(total_points, -1)\n",
    "\n",
    "            # Assign the labels from clustering to the sampled points\n",
    "            full_labels[self.sampled_indices] = labels\n",
    "\n",
    "            # To propagate to non-sampled points, we could use nearest neighbors\n",
    "            # This is optional and would require additional processing\n",
    "\n",
    "            self.labels = full_labels\n",
    "        else:\n",
    "            self.labels = labels\n",
    "\n",
    "        # Add cluster labels to dataframe\n",
    "        self.df['cluster'] = self.labels\n",
    "\n",
    "        # Calculate cluster sizes (including noise points labeled as -1)\n",
    "        unique_labels = np.unique(self.labels)\n",
    "        n_clusters = len([l for l in unique_labels if l >= 0])\n",
    "        n_noise = np.sum(self.labels == -1)\n",
    "\n",
    "        print(f\"Found {n_clusters} clusters\")\n",
    "        print(f\"Number of noise points: {n_noise} ({n_noise/len(self.labels)*100:.2f}%)\")\n",
    "\n",
    "        print(\"Cluster sizes:\")\n",
    "        cluster_sizes = np.bincount(self.labels[self.labels >= 0])\n",
    "        for i, size in enumerate(cluster_sizes):\n",
    "            percentage = size / len(self.labels) * 100\n",
    "            print(f\"  Cluster {i}: {size} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "        # Free memory\n",
    "        gc.collect()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def reconstruct_cluster_image(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the cluster assignments into the original image shape.\n",
    "\n",
    "        Returns:\n",
    "            2D array of cluster assignments with shape (height, width)\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Create empty image array (default to -1 for unassigned pixels)\n",
    "        cluster_image = np.full((self.height, self.width), -1, dtype=int)\n",
    "\n",
    "        # Fill in cluster assignments - using vectorized approach for speed\n",
    "        x_coords = self.df[self.x_col].astype(int).values\n",
    "        y_coords = self.df[self.y_col].astype(int).values\n",
    "\n",
    "        # Filter valid coordinates within image bounds\n",
    "        valid_indices = (\n",
    "            (x_coords >= 0) & (x_coords < self.width) &\n",
    "            (y_coords >= 0) & (y_coords < self.height)\n",
    "        )\n",
    "\n",
    "        if np.any(valid_indices):\n",
    "            cluster_image[y_coords[valid_indices], x_coords[valid_indices]] = self.labels[valid_indices]\n",
    "\n",
    "        return cluster_image\n",
    "\n",
    "    # Visualization methods remain mostly unchanged\n",
    "    # ... (keep the same visualization methods as before)\n",
    "\n",
    "    def visualize_clusters(self,\n",
    "                          figsize: Tuple[int, int] = (12, 10),\n",
    "                          cmap: Optional[Union[str, ListedColormap]] = None,\n",
    "                          noise_color: str = 'black',\n",
    "                          save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the clustering results by reconstructing the original image.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "        unique_labels = np.unique(self.labels)\n",
    "\n",
    "        # Count number of actual clusters (excluding noise points)\n",
    "        n_clusters = len([l for l in unique_labels if l >= 0])\n",
    "\n",
    "        # Create a custom colormap that handles noise points specially\n",
    "        if cmap is None:\n",
    "            # For regular clusters (excluding noise), choose colormap based on number of clusters\n",
    "            if n_clusters <= 10:\n",
    "                # For few clusters, use distinct colors\n",
    "                tab_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "                # Add black at the beginning for noise points (label -1)\n",
    "                colors = [noise_color] + tab_colors[:n_clusters]\n",
    "                cmap = ListedColormap(colors)\n",
    "            else:\n",
    "                # For many clusters, modify a continuous colormap\n",
    "                base_cmap = plt.cm.get_cmap('tab20' if n_clusters <= 20 else 'viridis', n_clusters)\n",
    "                # Create a new colormap with black for noise points\n",
    "                colors = [noise_color]  # Start with noise color\n",
    "                colors.extend([base_cmap(i) for i in range(n_clusters)])  # Add colors for actual clusters\n",
    "                cmap = ListedColormap(colors)\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Display the image - ensure noise points (label -1) map to the first color in the colormap\n",
    "        # Add 1 to all labels to shift them up (making -1 â†’ 0, 0 â†’ 1, etc.)\n",
    "        shifted_image = cluster_image + 1\n",
    "        im = ax.imshow(shifted_image, cmap=cmap, vmin=0, vmax=n_clusters)\n",
    "\n",
    "        # Create a custom colorbar with proper labels\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Cluster')\n",
    "\n",
    "        # Adjust colorbar ticks and labels to show -1 for noise\n",
    "        tick_locs = np.arange(n_clusters + 1) + 0.5  # Center ticks\n",
    "        cbar.set_ticks(tick_locs)\n",
    "        tick_labels = ['-1 (Noise)'] + [str(i) for i in range(n_clusters)]\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f'DBSCAN Clustering Results ({n_clusters} clusters, {np.sum(cluster_image == -1)} noise points)')\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_separate_clusters(self,\n",
    "                                   figsize: Tuple[int, int] = (15, 10),\n",
    "                                   cmap: str = 'viridis',\n",
    "                                   n_cols: int = 3,\n",
    "                                   include_noise: bool = True,\n",
    "                                   save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize each cluster separately.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Reconstruct cluster image\n",
    "        cluster_image = self.reconstruct_cluster_image()\n",
    "\n",
    "        # Get unique cluster labels\n",
    "        unique_labels = sorted(np.unique(self.labels))\n",
    "\n",
    "        # Decide whether to include noise in visualization\n",
    "        if not include_noise and -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = (n_clusters + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Create separate mask for each cluster\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create binary mask for this cluster\n",
    "            mask = (cluster_image == label).astype(float)\n",
    "\n",
    "            # Display the mask\n",
    "            im = ax.imshow(mask, cmap=cmap)\n",
    "\n",
    "            # Set title and turn off axis labels\n",
    "            cluster_name = f'Noise Points' if label == -1 else f'Cluster {label}'\n",
    "            ax.set_title(cluster_name)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(n_clusters, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Individual Clusters', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved individual cluster visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def visualize_cluster_spectra(self,\n",
    "                                 excitations: Optional[List[float]] = None,\n",
    "                                 figsize: Tuple[int, int] = (15, 10),\n",
    "                                 include_noise: bool = False,\n",
    "                                 save_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize the average spectra for each cluster.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Get all feature columns that have the format \"emission-excitation\"\n",
    "        spectral_cols = [col for col in self.feature_cols if '-' in col]\n",
    "\n",
    "        # If no spectral columns, we can't visualize spectra\n",
    "        if not spectral_cols:\n",
    "            raise ValueError(\"No spectral columns found in DataFrame\")\n",
    "\n",
    "        # Parse excitation and emission wavelengths from column names\n",
    "        wavelengths = []\n",
    "        for col in spectral_cols:\n",
    "            try:\n",
    "                emission, excitation = map(float, col.split('-'))\n",
    "                wavelengths.append((emission, excitation, col))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping column {col} - doesn't match expected format\")\n",
    "\n",
    "        # Filter by excitation wavelengths if requested\n",
    "        if excitations:\n",
    "            wavelengths = [(em, ex, col) for em, ex, col in wavelengths if ex in excitations]\n",
    "\n",
    "        # If no wavelengths left, we can't visualize spectra\n",
    "        if not wavelengths:\n",
    "            raise ValueError(\"No valid spectral columns found after filtering\")\n",
    "\n",
    "        # Group by excitation wavelength\n",
    "        excitations = sorted(set(ex for _, ex, _ in wavelengths))\n",
    "\n",
    "        # Set up subplot grid\n",
    "        n_rows = min(3, len(excitations))\n",
    "        n_cols = (len(excitations) + n_rows - 1) // n_rows\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Get unique cluster labels (sorted, excluding noise if specified)\n",
    "        unique_labels = sorted(np.unique(self.labels))\n",
    "        if not include_noise and -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        # Set up colors for clusters - using a colormap that works well for many clusters\n",
    "        if n_clusters <= 10:\n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, max(10, n_clusters)))\n",
    "        else:\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "        # Process one excitation at a time to save memory\n",
    "        for ax_idx, excitation in enumerate(excitations):\n",
    "            if ax_idx < len(axes):\n",
    "                ax = axes[ax_idx]\n",
    "\n",
    "                # Get emission wavelengths and column names for this excitation\n",
    "                excitation_data = [(em, col) for em, ex, col in wavelengths if ex == excitation]\n",
    "\n",
    "                if excitation_data:\n",
    "                    # Sort by emission wavelength\n",
    "                    excitation_data.sort()\n",
    "                    emission_wavelengths = [em for em, _ in excitation_data]\n",
    "                    columns = [col for _, col in excitation_data]\n",
    "\n",
    "                    # Plot average spectrum for each cluster one by one\n",
    "                    for i, label in enumerate(unique_labels):\n",
    "                        # Get cluster data efficiently using boolean indexing\n",
    "                        cluster_mask = self.df['cluster'] == label\n",
    "                        cluster_size = np.sum(cluster_mask)\n",
    "\n",
    "                        # Skip if cluster is empty\n",
    "                        if cluster_size == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Calculate mean spectrum using optimized approach\n",
    "                        mean_spectrum = self.df.loc[cluster_mask, columns].mean().values\n",
    "\n",
    "                        # Determine label and style\n",
    "                        if label == -1:\n",
    "                            cluster_name = \"Noise\"\n",
    "                            line_style = \":\"  # dotted line for noise\n",
    "                        else:\n",
    "                            cluster_name = f'Cluster {label}'\n",
    "                            line_style = \"-\"  # solid line for clusters\n",
    "\n",
    "                        # Plot spectrum\n",
    "                        color_idx = i if label != -1 else -1  # Use last color for noise\n",
    "                        ax.plot(emission_wavelengths, mean_spectrum,\n",
    "                                line_style, color=colors[color_idx],\n",
    "                                linewidth=2, label=cluster_name)\n",
    "\n",
    "                    # Set labels and title\n",
    "                    ax.set_xlabel('Emission Wavelength (nm)')\n",
    "                    ax.set_ylabel('Mean Intensity')\n",
    "                    ax.set_title(f'Excitation {excitation} nm')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Add legend to first plot only\n",
    "                    if ax_idx == 0:\n",
    "                        ax.legend(loc='best')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(len(excitations), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Average Spectra by Cluster', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "        # Save figure if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved cluster spectra visualization to {save_path}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def save_cluster_results(self, output_file: str):\n",
    "        \"\"\"\n",
    "        Save clustering results to a file.\n",
    "\n",
    "        Args:\n",
    "            output_file: Path to save the results\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must fit a model first using fit_dbscan()\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Get file extension\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "\n",
    "        # Save based on file type - focusing on memory efficiency\n",
    "        print(f\"Saving results to {output_file}...\")\n",
    "\n",
    "        if ext.lower() in ['.csv']:\n",
    "            # Save to CSV (coordinates and cluster assignments only)\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.pkl', '.pickle']:\n",
    "            # Save full DataFrame with cluster assignments\n",
    "            self.df.to_pickle(output_file)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        elif ext.lower() in ['.parquet']:\n",
    "            # Save full DataFrame with clusters - parquet is more memory-efficient\n",
    "            self.df.to_parquet(output_file, index=False)\n",
    "            print(f\"Saved full DataFrame with clusters to {output_file}\")\n",
    "\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            result_df = self.df[[self.x_col, self.y_col, 'cluster']].copy()\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved cluster assignments to {output_file}\")"
   ],
   "id": "cb98151f132ede43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_memory_efficient_dbscan(\n",
    "    input_file: str,\n",
    "    eps: Optional[float] = None,\n",
    "    min_samples: Optional[int] = None,\n",
    "    n_pca_components: Optional[int] = None,\n",
    "    pca_variance: float = 0.95,\n",
    "    max_samples: Optional[int] = None,\n",
    "    output_dir: Optional[str] = None,\n",
    "    sample_size_for_estimation: int = 5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Run memory-efficient DBSCAN clustering on hyperspectral data and save results.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to input CSV/parquet/pickle file with flattened hyperspectral data\n",
    "        eps: DBSCAN eps parameter (if None, estimated automatically)\n",
    "        min_samples: DBSCAN min_samples parameter (if None, estimated automatically)\n",
    "        n_pca_components: Number of PCA components to keep (if None, based on pca_variance)\n",
    "        pca_variance: Proportion of variance to preserve if n_pca_components is None\n",
    "        max_samples: Maximum number of samples to use for clustering (None = all)\n",
    "        output_dir: Directory to save outputs (if None, use same directory as input)\n",
    "        sample_size_for_estimation: Number of samples to use for parameter estimation\n",
    "    \"\"\"\n",
    "    # Determine file type and load data\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    if ext.lower() in ['.csv']:\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif ext.lower() in ['.pkl', '.pickle']:\n",
    "        df = pd.read_pickle(input_file)\n",
    "    elif ext.lower() in ['.parquet']:\n",
    "        df = pd.read_parquet(input_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "    # Set up output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "        if not output_dir:\n",
    "            output_dir = \".\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.basename(input_file)\n",
    "    base_name = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Initialize clustering\n",
    "    clustering = MemoryEfficientDBSCAN(df)\n",
    "\n",
    "    # Preprocess the data with dimensionality reduction\n",
    "    clustering.preprocess(handle_nan='fill_mean',\n",
    "                         n_components=n_pca_components,\n",
    "                         pca_variance=pca_variance)\n",
    "\n",
    "    # If parameters not specified, estimate them\n",
    "    if eps is None or min_samples is None:\n",
    "        estimated_eps, estimated_min_samples = clustering.estimate_dbscan_params(\n",
    "            n_samples=sample_size_for_estimation\n",
    "        )\n",
    "\n",
    "        # Save k-distance plot\n",
    "        plt.savefig(os.path.join(output_dir, f\"{base_name}_kdist_plot.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Use estimated parameters if not explicitly provided\n",
    "        if eps is None:\n",
    "            eps = estimated_eps\n",
    "        if min_samples is None:\n",
    "            min_samples = estimated_min_samples\n",
    "\n",
    "    # Fit DBSCAN with the parameters\n",
    "    clustering.fit_dbscan(eps=eps, min_samples=min_samples, max_samples=max_samples)\n",
    "\n",
    "    # Visualize clustering results\n",
    "    fig = clustering.visualize_clusters()\n",
    "    clusters_path = os.path.join(output_dir, f\"{base_name}_dbscan_clusters.png\")\n",
    "    fig.savefig(clusters_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "    print(f\"Saved cluster visualization to {clusters_path}\")\n",
    "\n",
    "    # Visualize individual clusters\n",
    "    fig = clustering.visualize_separate_clusters()\n",
    "    separate_path = os.path.join(output_dir, f\"{base_name}_dbscan_separate_clusters.png\")\n",
    "    fig.savefig(separate_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "    print(f\"Saved individual cluster visualization to {separate_path}\")\n",
    "\n",
    "    # Visualize cluster spectra\n",
    "    try:\n",
    "        fig = clustering.visualize_cluster_spectra()\n",
    "        spectra_path = os.path.join(output_dir, f\"{base_name}_dbscan_cluster_spectra.png\")\n",
    "        fig.savefig(spectra_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close figure to free memory\n",
    "        print(f\"Saved cluster spectra visualization to {spectra_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not visualize cluster spectra: {e}\")\n",
    "\n",
    "    # Save cluster results\n",
    "    results_path = os.path.join(output_dir, f\"{base_name}_dbscan_results.parquet\")\n",
    "    clustering.save_cluster_results(results_path)\n",
    "\n",
    "    # Save PCA information for reference\n",
    "    if clustering.pca_model is not None:\n",
    "        pca_info = {\n",
    "            'n_components': clustering.pca_model.n_components_,\n",
    "            'explained_variance_ratio': clustering.pca_model.explained_variance_ratio_,\n",
    "            'total_variance_explained': np.sum(clustering.pca_model.explained_variance_ratio_)\n",
    "        }\n",
    "        with open(os.path.join(output_dir, f\"{base_name}_pca_info.txt\"), 'w') as f:\n",
    "            f.write(\"PCA Information:\\n\")\n",
    "            f.write(f\"Number of components: {pca_info['n_components']}\\n\")\n",
    "            f.write(f\"Total variance explained: {pca_info['total_variance_explained']:.4f}\\n\")\n",
    "            f.write(\"Component-wise variance explained:\\n\")\n",
    "            for i, var in enumerate(pca_info['explained_variance_ratio']):\n",
    "                f.write(f\"  Component {i+1}: {var:.4f}\\n\")\n",
    "\n",
    "    # Save parameters used\n",
    "    with open(os.path.join(output_dir, f\"{base_name}_dbscan_params.txt\"), 'w') as f:\n",
    "        f.write(f\"DBSCAN Parameters:\\n\")\n",
    "        f.write(f\"eps: {eps}\\n\")\n",
    "        f.write(f\"min_samples: {min_samples}\\n\")\n",
    "        if max_samples is not None:\n",
    "            f.write(f\"max_samples used: {max_samples}\\n\")\n",
    "        if n_pca_components is not None:\n",
    "            f.write(f\"PCA components: {n_pca_components}\\n\")\n",
    "        else:\n",
    "            f.write(f\"PCA variance threshold: {pca_variance}\\n\")\n",
    "\n",
    "        # Add cluster information\n",
    "        if clustering.labels is not None:\n",
    "            n_clusters = len(np.unique(clustering.labels)) - (1 if -1 in clustering.labels else 0)\n",
    "            n_noise = np.sum(clustering.labels == -1)\n",
    "            f.write(f\"Number of clusters: {n_clusters}\\n\")\n",
    "            f.write(f\"Number of noise points: {n_noise}\\n\")\n",
    "            f.write(f\"Percentage of noise: {n_noise / len(clustering.labels) * 100:.2f}%\\n\")\n",
    "\n",
    "    print(\"Memory-efficient DBSCAN clustering complete!\")"
   ],
   "id": "407ba443fa2a8fed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_memory_efficient_dbscan(\n",
    "    \"../Data/Kiwi Experiment/parquests/KiwiDataMasked.parquet\",\n",
    "    # auto_params=True,\n",
    "    output_dir=\"DBScanResults/Masked\",\n",
    ")"
   ],
   "id": "98c5ae697e2230fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6e18023bd44a532f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
