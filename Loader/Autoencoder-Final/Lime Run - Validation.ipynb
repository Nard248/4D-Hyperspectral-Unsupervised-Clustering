{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T17:42:59.706697Z",
     "start_time": "2025-04-26T17:42:47.867335Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:43:23.061630Z",
     "start_time": "2025-04-26T17:43:17.814624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File paths\n",
    "data_path = \"../Data/Lime Experiment/processed/masked_data_cutoff_30nm_exposure_max_power_min.pkl\"\n",
    "mask_path = \"../Data/Lime Experiment/processed/mask.npy\"\n",
    "output_dir = \"hyperspectral_results_lime\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading hyperspectral data...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "print(\"Data Summary:\")\n",
    "print(f\"Number of excitation wavelengths: {len(data_dict['excitation_wavelengths'])}\")\n",
    "print(f\"Excitation wavelengths: {data_dict['excitation_wavelengths']}\")\n",
    "\n",
    "# Load mask\n",
    "print(\"Loading mask...\")\n",
    "mask = np.load(mask_path)\n",
    "valid_pixels = np.sum(mask)\n",
    "total_pixels = mask.size\n",
    "print(f\"Mask loaded: {valid_pixels}/{total_pixels} valid pixels ({valid_pixels/total_pixels*100:.2f}%)\")\n",
    "\n",
    "# Create dataset\n",
    "from AutoencoderPipeline import MaskedHyperspectralDataset\n",
    "\n",
    "dataset = MaskedHyperspectralDataset(\n",
    "    data_dict=data_dict,\n",
    "    mask=mask,\n",
    "    normalize=True,\n",
    "    downscale_factor=1\n",
    ")\n",
    "\n",
    "# Get spatial dimensions\n",
    "height, width = dataset.get_spatial_dimensions()\n",
    "print(f\"Data dimensions after processing: {height}x{width}\")\n",
    "\n",
    "# Get all data\n",
    "all_data = dataset.get_all_data()"
   ],
   "id": "96e838f5b4fcc4b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hyperspectral data...\n",
      "Data Summary:\n",
      "Number of excitation wavelengths: 21\n",
      "Excitation wavelengths: [300.0, 310.0, 320.0, 330.0, 340.0, 350.0, 360.0, 370.0, 380.0, 390.0, 400.0, 410.0, 420.0, 430.0, 440.0, 450.0, 460.0, 470.0, 480.0, 490.0, 500.0]\n",
      "Loading mask...\n",
      "Mask loaded: 62953/89088 valid pixels (70.66%)\n",
      "Preparing data for 21 excitation wavelengths...\n",
      "Emission band lengths for each excitation wavelength:\n",
      "  - Excitation 300.0 nm: 24 bands\n",
      "  - Excitation 310.0 nm: 24 bands\n",
      "  - Excitation 320.0 nm: 24 bands\n",
      "  - Excitation 330.0 nm: 24 bands\n",
      "  - Excitation 340.0 nm: 24 bands\n",
      "  - Excitation 350.0 nm: 25 bands\n",
      "  - Excitation 360.0 nm: 27 bands\n",
      "  - Excitation 370.0 nm: 29 bands\n",
      "  - Excitation 380.0 nm: 31 bands\n",
      "  - Excitation 390.0 nm: 31 bands\n",
      "  - Excitation 400.0 nm: 30 bands\n",
      "  - Excitation 410.0 nm: 29 bands\n",
      "  - Excitation 420.0 nm: 28 bands\n",
      "  - Excitation 430.0 nm: 27 bands\n",
      "  - Excitation 440.0 nm: 26 bands\n",
      "  - Excitation 450.0 nm: 25 bands\n",
      "  - Excitation 460.0 nm: 24 bands\n",
      "  - Excitation 470.0 nm: 23 bands\n",
      "  - Excitation 480.0 nm: 22 bands\n",
      "  - Excitation 490.0 nm: 21 bands\n",
      "  - Excitation 500.0 nm: 20 bands\n",
      "Mask processed. Valid pixels: 62953.0/89088 (70.66%)\n",
      "Warning: 627240 NaN values detected in excitation 300.0 (29.3362% of data)\n",
      "Warning: 627240 NaN values detected in excitation 310.0 (29.3362% of data)\n",
      "Warning: 627240 NaN values detected in excitation 320.0 (29.3362% of data)\n",
      "Warning: 627240 NaN values detected in excitation 330.0 (29.3362% of data)\n",
      "Warning: 627240 NaN values detected in excitation 340.0 (29.3362% of data)\n",
      "Warning: 653375 NaN values detected in excitation 350.0 (29.3362% of data)\n",
      "Warning: 705645 NaN values detected in excitation 360.0 (29.3362% of data)\n",
      "Warning: 757915 NaN values detected in excitation 370.0 (29.3362% of data)\n",
      "Warning: 810185 NaN values detected in excitation 380.0 (29.3362% of data)\n",
      "Warning: 810185 NaN values detected in excitation 390.0 (29.3362% of data)\n",
      "Warning: 784050 NaN values detected in excitation 400.0 (29.3362% of data)\n",
      "Warning: 757915 NaN values detected in excitation 410.0 (29.3362% of data)\n",
      "Warning: 731780 NaN values detected in excitation 420.0 (29.3362% of data)\n",
      "Warning: 705645 NaN values detected in excitation 430.0 (29.3362% of data)\n",
      "Warning: 679510 NaN values detected in excitation 440.0 (29.3362% of data)\n",
      "Warning: 653375 NaN values detected in excitation 450.0 (29.3362% of data)\n",
      "Warning: 627240 NaN values detected in excitation 460.0 (29.3362% of data)\n",
      "Warning: 601105 NaN values detected in excitation 470.0 (29.3362% of data)\n",
      "Warning: 574970 NaN values detected in excitation 480.0 (29.3362% of data)\n",
      "Warning: 548835 NaN values detected in excitation 490.0 (29.3362% of data)\n",
      "Warning: 522700 NaN values detected in excitation 500.0 (29.3362% of data)\n",
      "Global data range (valid values only): [0.0000, 14665.0456]\n",
      "Data normalized to range [0, 1] using global normalization\n",
      "Data preparation complete. Spatial dimensions: 256x348\n",
      "Data dimensions after processing: 256x348\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:43:29.194596Z",
     "start_time": "2025-04-26T17:43:29.015161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from AutoencoderPipeline import HyperspectralCAEWithMasking\n",
    "\n",
    "model = HyperspectralCAEWithMasking(\n",
    "    excitations_data={ex: data.numpy() for ex, data in all_data.items()},\n",
    "    k1=20,\n",
    "    k3=20,\n",
    "    filter_size=5,\n",
    "    sparsity_target=0.1,\n",
    "    sparsity_weight=1.0,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "model = model.to(device)"
   ],
   "id": "e48fd017e671642a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 342498 parameters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:43:39.140979Z",
     "start_time": "2025-04-26T17:43:39.131976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# IMPORTANT FIX: Make sure parameter names match\n",
    "# The error you encountered was because 'chunk_overlap' should be 'overlap'\n",
    "# Let's define a function to create spatial chunks with the correct parameter names\n",
    "\n",
    "def create_chunks(data_tensor, chunk_size=64, overlap=16):\n",
    "    \"\"\"\n",
    "    Split a large spatial hyperspectral tensor into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        data_tensor: Input tensor of shape [height, width, emission_bands]\n",
    "        chunk_size: Size of each spatial chunk\n",
    "        overlap: Overlap between adjacent chunks\n",
    "\n",
    "    Returns:\n",
    "        List of chunk tensors and their positions\n",
    "    \"\"\"\n",
    "    # Determine input shape\n",
    "    if len(data_tensor.shape) == 4:  # [num_excitations, height, width, emission_bands]\n",
    "        height, width = data_tensor.shape[1], data_tensor.shape[2]\n",
    "    else:  # [height, width, emission_bands]\n",
    "        height, width = data_tensor.shape[0], data_tensor.shape[1]\n",
    "\n",
    "    # Calculate stride\n",
    "    stride = chunk_size - overlap\n",
    "\n",
    "    # Calculate number of chunks in each dimension\n",
    "    num_chunks_y = max(1, (height - overlap) // stride)\n",
    "    num_chunks_x = max(1, (width - overlap) // stride)\n",
    "\n",
    "    # Adjust to ensure we cover the entire image\n",
    "    if stride * num_chunks_y + overlap < height:\n",
    "        num_chunks_y += 1\n",
    "    if stride * num_chunks_x + overlap < width:\n",
    "        num_chunks_x += 1\n",
    "\n",
    "    # Create list to store chunks and their positions\n",
    "    chunks = []\n",
    "    positions = []\n",
    "\n",
    "    # Extract chunks\n",
    "    for i in range(num_chunks_y):\n",
    "        for j in range(num_chunks_x):\n",
    "            # Calculate start and end positions\n",
    "            y_start = i * stride\n",
    "            x_start = j * stride\n",
    "            y_end = min(y_start + chunk_size, height)\n",
    "            x_end = min(x_start + chunk_size, width)\n",
    "\n",
    "            # Handle edge cases by adjusting start positions\n",
    "            if y_end == height:\n",
    "                y_start = max(0, height - chunk_size)\n",
    "            if x_end == width:\n",
    "                x_start = max(0, width - chunk_size)\n",
    "\n",
    "            # Extract chunk based on input shape\n",
    "            if len(data_tensor.shape) == 4:  # [num_excitations, height, width, emission_bands]\n",
    "                chunk = data_tensor[:, y_start:y_end, x_start:x_end, :]\n",
    "            else:  # [height, width, emission_bands]\n",
    "                chunk = data_tensor[y_start:y_end, x_start:x_end, :]\n",
    "\n",
    "            # Add to lists\n",
    "            chunks.append(chunk)\n",
    "            positions.append((y_start, y_end, x_start, x_end))\n",
    "\n",
    "    print(f\"Created {len(chunks)} chunks of size up to {chunk_size}x{chunk_size} with {overlap} overlap\")\n",
    "    return chunks, positions"
   ],
   "id": "4a90d939feadda47",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:45:35.271784Z",
     "start_time": "2025-04-26T17:43:56.124734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now let's implement a custom training function that uses our fixed chunk creation\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dataset,\n",
    "    num_epochs=30,\n",
    "    learning_rate=0.001,\n",
    "    chunk_size=64,\n",
    "    overlap=8,\n",
    "    device='cuda'\n",
    "):\n",
    "    # Create output directory for models\n",
    "    model_dir = os.path.join(output_dir, \"model\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    # Get data and mask\n",
    "    all_data = dataset.get_all_data()\n",
    "    mask_tensor = torch.tensor(dataset.processed_mask, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Create chunks for mask\n",
    "    mask_expanded = dataset.processed_mask[..., np.newaxis]  # Add dummy dimension\n",
    "    mask_chunks, mask_positions = create_chunks(mask_expanded, chunk_size, overlap)\n",
    "    mask_chunks = [torch.tensor(chunk[..., 0], dtype=torch.float32).to(device) for chunk in mask_chunks]\n",
    "\n",
    "    # Create chunks for each excitation\n",
    "    print(\"Creating chunks for each excitation wavelength...\")\n",
    "    chunks_dict = {}\n",
    "    positions_dict = {}\n",
    "\n",
    "    for ex in all_data:\n",
    "        data_np = all_data[ex].numpy()\n",
    "        chunks, positions = create_chunks(data_np, chunk_size, overlap)\n",
    "        chunks_dict[ex] = chunks\n",
    "        positions_dict[ex] = positions\n",
    "\n",
    "    # Get number of chunks\n",
    "    num_chunks = len(next(iter(chunks_dict.values())))\n",
    "\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    mask_batches = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Data batch\n",
    "        batch = {}\n",
    "        for ex in chunks_dict:\n",
    "            chunk = chunks_dict[ex][i]\n",
    "            batch[ex] = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        batches.append(batch)\n",
    "\n",
    "        # Mask batch\n",
    "        mask_batches.append(mask_chunks[i].unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"Training for {num_epochs} epochs with {len(batches)} batches...\")\n",
    "    train_losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_recon_loss = 0.0\n",
    "        epoch_sparsity_loss = 0.0\n",
    "\n",
    "        # Train on each batch\n",
    "        for i, (batch, mask_batch) in enumerate(zip(batches, mask_batches)):\n",
    "            # Forward pass\n",
    "            output = model(batch)\n",
    "\n",
    "            # Compute masked reconstruction loss\n",
    "            recon_loss = model.compute_masked_loss(\n",
    "                output_dict=output,\n",
    "                target_dict=batch,\n",
    "                spatial_mask=mask_batch\n",
    "            )\n",
    "\n",
    "            # Compute sparsity loss\n",
    "            encoded = model.encode(batch)\n",
    "            sparsity_loss = model.compute_sparsity_loss(encoded)\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + model.sparsity_weight * sparsity_loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_sparsity_loss += sparsity_loss.item()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0 or i == len(batches) - 1:\n",
    "                print(f\"  Batch {i+1}/{len(batches)}\", end=\"\\r\")\n",
    "\n",
    "        # Record average loss\n",
    "        avg_loss = epoch_loss / len(batches)\n",
    "        avg_recon_loss = epoch_recon_loss / len(batches)\n",
    "        avg_sparsity_loss = epoch_sparsity_loss / len(batches)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Report progress\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f} \"\n",
    "              f\"(Recon: {avg_recon_loss:.4f}, Sparsity: {avg_sparsity_loss:.4f}), \"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Check if this is the best epoch\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch\n",
    "            no_improvement_count = 0\n",
    "\n",
    "            # Save best model\n",
    "            model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  New best model saved to {model_path}\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"  No improvement for {no_improvement_count} epochs (best: {best_loss:.4f} at epoch {best_epoch+1})\")\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improvement_count >= 5:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Save final model\n",
    "    model_path = os.path.join(model_dir, \"final_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Final model saved to {model_path}\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, \"best_model.pth\")))\n",
    "\n",
    "    return model, train_losses\n",
    "\n",
    "# Run training\n",
    "model, losses = train_model(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_epochs=30,\n",
    "    learning_rate=0.001,\n",
    "    chunk_size=64,\n",
    "    overlap=8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join(output_dir, \"training_loss.png\"))\n",
    "plt.close()"
   ],
   "id": "c127aa8cf9a56f5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meloy\\PycharmProjects\\Capstone\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Creating chunks for each excitation wavelength...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Training for 30 epochs with 35 batches...\n",
      "Epoch 1/30, Loss: 3.0973 (Recon: 0.0910, Sparsity: 3.0064), Time: 4.17s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 2/30, Loss: 0.1464 (Recon: 0.0106, Sparsity: 0.1358), Time: 3.28s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 3/30, Loss: 0.0079 (Recon: 0.0048, Sparsity: 0.0032), Time: 3.25s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 4/30, Loss: 0.0044 (Recon: 0.0029, Sparsity: 0.0015), Time: 3.09s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 5/30, Loss: 0.0034 (Recon: 0.0020, Sparsity: 0.0014), Time: 3.08s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 6/30, Loss: 0.0028 (Recon: 0.0016, Sparsity: 0.0012), Time: 3.03s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 7/30, Loss: 0.0026 (Recon: 0.0013, Sparsity: 0.0013), Time: 3.06s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 8/30, Loss: 0.0023 (Recon: 0.0011, Sparsity: 0.0012), Time: 3.06s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 9/30, Loss: 0.0022 (Recon: 0.0010, Sparsity: 0.0012), Time: 3.05s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 10/30, Loss: 0.0020 (Recon: 0.0009, Sparsity: 0.0011), Time: 3.04s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 11/30, Loss: 0.0018 (Recon: 0.0008, Sparsity: 0.0010), Time: 3.08s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 12/30, Loss: 0.0016 (Recon: 0.0008, Sparsity: 0.0009), Time: 3.07s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 13/30, Loss: 0.0017 (Recon: 0.0007, Sparsity: 0.0010), Time: 3.04s\n",
      "  No improvement for 1 epochs (best: 0.0016 at epoch 12)\n",
      "Epoch 14/30, Loss: 0.0015 (Recon: 0.0007, Sparsity: 0.0008), Time: 3.07s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 15/30, Loss: 0.0015 (Recon: 0.0006, Sparsity: 0.0008), Time: 3.05s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 16/30, Loss: 0.0014 (Recon: 0.0006, Sparsity: 0.0008), Time: 3.06s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 17/30, Loss: 0.0013 (Recon: 0.0006, Sparsity: 0.0007), Time: 3.04s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 18/30, Loss: 0.0012 (Recon: 0.0006, Sparsity: 0.0006), Time: 3.08s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 19/30, Loss: 0.0012 (Recon: 0.0006, Sparsity: 0.0007), Time: 3.08s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 20/30, Loss: 0.0011 (Recon: 0.0006, Sparsity: 0.0006), Time: 3.04s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 21/30, Loss: 0.0010 (Recon: 0.0005, Sparsity: 0.0005), Time: 3.09s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 22/30, Loss: 0.0010 (Recon: 0.0005, Sparsity: 0.0005), Time: 3.14s\n",
      "  No improvement for 1 epochs (best: 0.0010 at epoch 21)\n",
      "Epoch 23/30, Loss: 0.0010 (Recon: 0.0005, Sparsity: 0.0005), Time: 3.17s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 24/30, Loss: 0.0009 (Recon: 0.0005, Sparsity: 0.0004), Time: 3.11s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 25/30, Loss: 0.0009 (Recon: 0.0005, Sparsity: 0.0004), Time: 3.07s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 26/30, Loss: 0.0009 (Recon: 0.0005, Sparsity: 0.0004), Time: 3.09s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 27/30, Loss: 0.0009 (Recon: 0.0005, Sparsity: 0.0004), Time: 3.11s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 28/30, Loss: 0.0009 (Recon: 0.0005, Sparsity: 0.0004), Time: 3.27s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 29/30, Loss: 0.0008 (Recon: 0.0005, Sparsity: 0.0003), Time: 3.12s\n",
      "  New best model saved to hyperspectral_results_lime\\model\\best_model.pth\n",
      "Epoch 30/30, Loss: 0.0008 (Recon: 0.0004, Sparsity: 0.0004), Time: 3.13s\n",
      "  No improvement for 1 epochs (best: 0.0008 at epoch 29)\n",
      "Final model saved to hyperspectral_results_lime\\model\\final_model.pth\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:45:46.149680Z",
     "start_time": "2025-04-26T17:45:41.484943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_chunk_reconstructions(chunks, positions, full_height, full_width):\n",
    "    \"\"\"\n",
    "    Merge the reconstructed chunks back into a full image.\n",
    "    \"\"\"\n",
    "    # Determine shape from the first chunk\n",
    "    first_chunk = chunks[0]\n",
    "\n",
    "    if len(first_chunk.shape) == 4:  # [batch, height, width, emission_bands]\n",
    "        batch_size, _, _, num_bands = first_chunk.shape\n",
    "        merged = torch.zeros((batch_size, full_height, full_width, num_bands),\n",
    "                         device=first_chunk.device)\n",
    "        weights = torch.zeros((batch_size, full_height, full_width, num_bands),\n",
    "                          device=first_chunk.device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected chunk shape: {first_chunk.shape}\")\n",
    "\n",
    "    # Merge chunks\n",
    "    for chunk, (y_start, y_end, x_start, x_end) in zip(chunks, positions):\n",
    "        merged[:, y_start:y_end, x_start:x_end, :] += chunk\n",
    "        weights[:, y_start:y_end, x_start:x_end, :] += 1\n",
    "\n",
    "    # Average overlapping regions\n",
    "    merged = merged / torch.clamp(weights, min=1.0)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def evaluate_model(model, dataset, chunk_size=64, overlap=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the model by generating reconstructions and calculating metrics.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    eval_dir = os.path.join(output_dir, \"evaluation\")\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get data and mask\n",
    "    all_data = dataset.get_all_data()\n",
    "    mask = dataset.processed_mask\n",
    "\n",
    "    # Store results\n",
    "    results = {\n",
    "        'metrics': {},\n",
    "        'reconstructions': {}\n",
    "    }\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        overall_mse = 0.0\n",
    "        overall_mae = 0.0\n",
    "        num_excitations = 0\n",
    "\n",
    "        for ex in all_data:\n",
    "            data = all_data[ex]\n",
    "\n",
    "            # Create chunks for this excitation\n",
    "            chunks, positions = create_chunks(data.numpy(), chunk_size, overlap)\n",
    "\n",
    "            # Process chunks\n",
    "            reconstructed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Convert to tensor and add batch dimension\n",
    "                chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                # Create input dictionary for this excitation only\n",
    "                chunk_dict = {ex: chunk_tensor}\n",
    "\n",
    "                # Generate reconstruction\n",
    "                output = model(chunk_dict)\n",
    "\n",
    "                # Add to reconstructed chunks\n",
    "                if ex in output:\n",
    "                    reconstructed_chunks.append(output[ex])\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 20 == 0 or i == len(chunks) - 1:\n",
    "                    print(f\"  Chunk {i+1}/{len(chunks)} for Ex={ex}nm\", end=\"\\r\")\n",
    "\n",
    "            # Skip if no valid reconstructions\n",
    "            if not reconstructed_chunks:\n",
    "                print(f\"Warning: No valid reconstructions for excitation {ex}\")\n",
    "                continue\n",
    "\n",
    "            # Merge chunks\n",
    "            full_reconstruction = merge_chunk_reconstructions(\n",
    "                reconstructed_chunks, positions, height, width\n",
    "            )\n",
    "\n",
    "            # Remove batch dimension\n",
    "            full_reconstruction = full_reconstruction[0]\n",
    "\n",
    "            # Store reconstruction\n",
    "            results['reconstructions'][ex] = full_reconstruction\n",
    "\n",
    "            # Apply mask for metric calculation\n",
    "            if mask is not None:\n",
    "                mask_tensor = torch.tensor(mask, dtype=torch.float32, device=device)\n",
    "                mask_expanded = mask_tensor.unsqueeze(-1).expand_as(data.to(device))\n",
    "\n",
    "                # Calculate metrics only on valid pixels\n",
    "                valid_pixels = mask_expanded.sum().item()\n",
    "\n",
    "                if valid_pixels > 0:\n",
    "                    # Calculate masked metrics\n",
    "                    masked_squared_error = ((full_reconstruction - data.to(device)) ** 2) * mask_expanded\n",
    "                    masked_abs_error = torch.abs(full_reconstruction - data.to(device)) * mask_expanded\n",
    "\n",
    "                    mse = masked_squared_error.sum().item() / valid_pixels\n",
    "                    mae = masked_abs_error.sum().item() / valid_pixels\n",
    "                    psnr = 10 * np.log10(1.0 / mse) if mse > 0 else float('inf')\n",
    "\n",
    "                    results['metrics'][ex] = {\n",
    "                        'mse': mse,\n",
    "                        'mae': mae,\n",
    "                        'psnr': psnr,\n",
    "                        'valid_pixels': valid_pixels\n",
    "                    }\n",
    "\n",
    "                    overall_mse += mse\n",
    "                    overall_mae += mae\n",
    "                    num_excitations += 1\n",
    "\n",
    "                    print(f\"Excitation {ex}nm - MSE: {mse:.4f}, PSNR: {psnr:.2f} dB\")\n",
    "            else:\n",
    "                # If no mask, use all pixels\n",
    "                mse = F.mse_loss(full_reconstruction, data.to(device)).item()\n",
    "                mae = torch.mean(torch.abs(full_reconstruction - data.to(device))).item()\n",
    "                psnr = 10 * np.log10(1.0 / mse) if mse > 0 else float('inf')\n",
    "\n",
    "                results['metrics'][ex] = {\n",
    "                    'mse': mse,\n",
    "                    'mae': mae,\n",
    "                    'psnr': psnr,\n",
    "                }\n",
    "\n",
    "                overall_mse += mse\n",
    "                overall_mae += mae\n",
    "                num_excitations += 1\n",
    "\n",
    "                print(f\"Excitation {ex}nm - MSE: {mse:.4f}, PSNR: {psnr:.2f} dB\")\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        if num_excitations > 0:\n",
    "            results['metrics']['overall'] = {\n",
    "                'mse': overall_mse / num_excitations,\n",
    "                'mae': overall_mae / num_excitations,\n",
    "                'psnr': 10 * np.log10(1.0 / (overall_mse / num_excitations))\n",
    "            }\n",
    "\n",
    "            print(f\"Overall - MSE: {results['metrics']['overall']['mse']:.4f}, \"\n",
    "                  f\"PSNR: {results['metrics']['overall']['psnr']:.2f} dB\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_model(model, dataset, chunk_size=64, overlap=8, device=device)"
   ],
   "id": "4a78eb970c9d5cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 300.0nm - MSE: 0.0001, PSNR: 40.47 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 310.0nm - MSE: 0.0001, PSNR: 42.09 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 320.0nm - MSE: 0.0000, PSNR: 43.08 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 330.0nm - MSE: 0.0000, PSNR: 43.81 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 340.0nm - MSE: 0.0000, PSNR: 44.27 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 350.0nm - MSE: 0.0000, PSNR: 44.51 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 360.0nm - MSE: 0.0000, PSNR: 44.36 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 370.0nm - MSE: 0.0000, PSNR: 44.36 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 380.0nm - MSE: 0.0001, PSNR: 42.84 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 390.0nm - MSE: 0.0001, PSNR: 41.43 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 400.0nm - MSE: 0.0001, PSNR: 38.78 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 410.0nm - MSE: 0.0002, PSNR: 36.75 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 420.0nm - MSE: 0.0003, PSNR: 35.12 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 430.0nm - MSE: 0.0004, PSNR: 33.92 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 440.0nm - MSE: 0.0004, PSNR: 33.95 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 450.0nm - MSE: 0.0007, PSNR: 31.73 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 460.0nm - MSE: 0.0002, PSNR: 36.18 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 470.0nm - MSE: 0.0007, PSNR: 31.31 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 480.0nm - MSE: 0.0010, PSNR: 30.19 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 490.0nm - MSE: 0.0004, PSNR: 33.52 dB\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "Excitation 500.0nm - MSE: 0.0008, PSNR: 30.96 dB\n",
      "Overall - MSE: 0.0003, PSNR: 35.57 dB\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:45:54.339277Z",
     "start_time": "2025-04-26T17:45:48.129387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_rgb_visualization(data_dict, emission_wavelengths, mask=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create RGB visualizations from hyperspectral data.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    vis_dir = os.path.join(output_dir, \"visualizations\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    # Default RGB bands (adjust as needed)\n",
    "    r_band, g_band, b_band = 650, 550, 450\n",
    "\n",
    "    # Choose excitations to visualize (first 3 for example)\n",
    "    excitations = list(data_dict.keys())[:3]\n",
    "\n",
    "    # Create a figure for comparison\n",
    "    fig, axes = plt.subplots(1, len(excitations), figsize=(len(excitations) * 6, 5))\n",
    "    if len(excitations) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Store RGB images\n",
    "    rgb_dict = {}\n",
    "\n",
    "    # Find global min/max for consistent normalization\n",
    "    global_min, global_max = float('inf'), float('-inf')\n",
    "\n",
    "    for ex in excitations:\n",
    "        # Get data\n",
    "        if isinstance(data_dict[ex], torch.Tensor):\n",
    "            data = data_dict[ex].cpu().numpy()\n",
    "        else:\n",
    "            data = data_dict[ex]\n",
    "\n",
    "        # Get band indices (find closest to target wavelengths)\n",
    "        if ex in emission_wavelengths:\n",
    "            wavelengths = emission_wavelengths[ex]\n",
    "            r_idx = np.argmin(np.abs(np.array(wavelengths) - r_band))\n",
    "            g_idx = np.argmin(np.abs(np.array(wavelengths) - g_band))\n",
    "            b_idx = np.argmin(np.abs(np.array(wavelengths) - b_band))\n",
    "        else:\n",
    "            # Use indices proportionally if wavelengths not available\n",
    "            num_bands = data.shape[2]\n",
    "            r_idx = int(num_bands * 0.8)\n",
    "            g_idx = int(num_bands * 0.5)\n",
    "            b_idx = int(num_bands * 0.2)\n",
    "\n",
    "        # Get channel data\n",
    "        r_values = data[:, :, r_idx].flatten()\n",
    "        g_values = data[:, :, g_idx].flatten()\n",
    "        b_values = data[:, :, b_idx].flatten()\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask_flat = mask.flatten()\n",
    "            r_values = r_values[mask_flat > 0]\n",
    "            g_values = g_values[mask_flat > 0]\n",
    "            b_values = b_values[mask_flat > 0]\n",
    "\n",
    "        # Update global min/max\n",
    "        if not np.all(np.isnan(r_values)) and not np.all(np.isnan(g_values)) and not np.all(np.isnan(b_values)):\n",
    "            local_min = min(np.nanmin(r_values), np.nanmin(g_values), np.nanmin(b_values))\n",
    "            local_max = max(np.nanmax(r_values), np.nanmax(g_values), np.nanmax(b_values))\n",
    "            global_min = min(global_min, local_min)\n",
    "            global_max = max(global_max, local_max)\n",
    "\n",
    "    # Create RGB images\n",
    "    for i, ex in enumerate(excitations):\n",
    "        # Get data\n",
    "        if isinstance(data_dict[ex], torch.Tensor):\n",
    "            data = data_dict[ex].cpu().numpy()\n",
    "        else:\n",
    "            data = data_dict[ex]\n",
    "\n",
    "        # Get band indices\n",
    "        if ex in emission_wavelengths:\n",
    "            wavelengths = emission_wavelengths[ex]\n",
    "            r_idx = np.argmin(np.abs(np.array(wavelengths) - r_band))\n",
    "            g_idx = np.argmin(np.abs(np.array(wavelengths) - g_band))\n",
    "            b_idx = np.argmin(np.abs(np.array(wavelengths) - b_band))\n",
    "        else:\n",
    "            num_bands = data.shape[2]\n",
    "            r_idx = int(num_bands * 0.8)\n",
    "            g_idx = int(num_bands * 0.5)\n",
    "            b_idx = int(num_bands * 0.2)\n",
    "\n",
    "        # Create RGB image\n",
    "        rgb = np.stack([\n",
    "            data[:, :, r_idx],  # R channel\n",
    "            data[:, :, g_idx],  # G channel\n",
    "            data[:, :, b_idx]   # B channel\n",
    "        ], axis=2)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Create mask with 3 channels\n",
    "            mask_rgb = np.stack([mask, mask, mask], axis=2)\n",
    "            rgb = rgb * mask_rgb\n",
    "\n",
    "        # Normalize to [0,1] range\n",
    "        rgb_normalized = np.clip((rgb - global_min) / (global_max - global_min + 1e-8), 0, 1)\n",
    "\n",
    "        # Replace NaNs with zeros\n",
    "        rgb_normalized = np.nan_to_num(rgb_normalized, nan=0.0)\n",
    "\n",
    "        # Store and plot\n",
    "        rgb_dict[ex] = rgb_normalized\n",
    "        axes[i].imshow(rgb_normalized)\n",
    "        axes[i].set_title(f'Excitation {ex}nm')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"rgb_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save individual RGB images\n",
    "    for ex, rgb in rgb_dict.items():\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(f'Excitation {ex}nm')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(vis_dir, f\"rgb_ex{ex}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    return rgb_dict\n",
    "\n",
    "# Create visualizations for original and reconstructed data\n",
    "original_rgb = create_rgb_visualization(\n",
    "    all_data,\n",
    "    dataset.emission_wavelengths,\n",
    "    mask=dataset.processed_mask,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "reconstructions = evaluation_results['reconstructions']\n",
    "recon_rgb = create_rgb_visualization(\n",
    "    reconstructions,\n",
    "    dataset.emission_wavelengths,\n",
    "    mask=dataset.processed_mask,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Create comparison for a specific excitation\n",
    "def create_comparison(original_data, reconstructed_data, excitation, emission_wavelengths=None, mask=None):\n",
    "    \"\"\"\n",
    "    Create comparison of original vs reconstructed data.\n",
    "    \"\"\"\n",
    "    vis_dir = os.path.join(output_dir, \"visualizations\")\n",
    "\n",
    "    # Convert tensors to numpy\n",
    "    if isinstance(original_data, torch.Tensor):\n",
    "        original_data = original_data.cpu().numpy()\n",
    "    if isinstance(reconstructed_data, torch.Tensor):\n",
    "        reconstructed_data = reconstructed_data.cpu().numpy()\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # RGB comparison\n",
    "    num_bands = original_data.shape[2]\n",
    "\n",
    "    # Determine RGB indices\n",
    "    if emission_wavelengths is not None:\n",
    "        r_band, g_band, b_band = 650, 550, 450\n",
    "        r_idx = np.argmin(np.abs(np.array(emission_wavelengths) - r_band))\n",
    "        g_idx = np.argmin(np.abs(np.array(emission_wavelengths) - g_band))\n",
    "        b_idx = np.argmin(np.abs(np.array(emission_wavelengths) - b_band))\n",
    "    else:\n",
    "        r_idx = int(num_bands * 0.8)\n",
    "        g_idx = int(num_bands * 0.5)\n",
    "        b_idx = int(num_bands * 0.2)\n",
    "\n",
    "    # Create RGB images\n",
    "    rgb_original = np.stack([\n",
    "        original_data[:, :, r_idx],\n",
    "        original_data[:, :, g_idx],\n",
    "        original_data[:, :, b_idx]\n",
    "    ], axis=2)\n",
    "\n",
    "    rgb_recon = np.stack([\n",
    "        reconstructed_data[:, :, r_idx],\n",
    "        reconstructed_data[:, :, g_idx],\n",
    "        reconstructed_data[:, :, b_idx]\n",
    "    ], axis=2)\n",
    "\n",
    "    # Apply mask\n",
    "    if mask is not None:\n",
    "        mask_rgb = np.stack([mask, mask, mask], axis=2)\n",
    "        rgb_original = rgb_original * mask_rgb\n",
    "        rgb_recon = rgb_recon * mask_rgb\n",
    "\n",
    "    # Normalize\n",
    "    min_val = min(np.nanmin(rgb_original), np.nanmin(rgb_recon))\n",
    "    max_val = max(np.nanmax(rgb_original), np.nanmax(rgb_recon))\n",
    "\n",
    "    rgb_original_norm = np.clip((rgb_original - min_val) / (max_val - min_val + 1e-8), 0, 1)\n",
    "    rgb_recon_norm = np.clip((rgb_recon - min_val) / (max_val - min_val + 1e-8), 0, 1)\n",
    "\n",
    "    # Calculate difference\n",
    "    diff = np.abs(rgb_original_norm - rgb_recon_norm)\n",
    "    diff_enhanced = np.clip(diff * 5, 0, 1)  # Enhance for visibility\n",
    "\n",
    "    # Plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(rgb_original_norm)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(rgb_recon_norm)\n",
    "    plt.title('Reconstructed')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(diff_enhanced)\n",
    "    plt.title('Difference (enhanced 5x)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Reconstruction Comparison - Excitation {excitation}nm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, f\"comparison_ex{excitation}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'original': rgb_original_norm,\n",
    "        'reconstructed': rgb_recon_norm,\n",
    "        'difference': diff_enhanced\n",
    "    }\n",
    "\n",
    "# Create comparisons for first 3 excitations\n",
    "for ex in list(all_data.keys())[:3]:\n",
    "    if ex in reconstructions:\n",
    "        create_comparison(\n",
    "            all_data[ex],\n",
    "            reconstructions[ex],\n",
    "            ex,\n",
    "            emission_wavelengths=dataset.emission_wavelengths.get(ex, None),\n",
    "            mask=dataset.processed_mask\n",
    "        )"
   ],
   "id": "9e6f82d0ae220831",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:46:05.574798Z",
     "start_time": "2025-04-26T17:45:59.835511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_encoded_features(model, data_dict, mask=None, chunk_size=64, overlap=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract encoded features from the model for all excitations.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get dimensions from first excitation\n",
    "    first_ex = next(iter(data_dict.keys()))\n",
    "    height, width = data_dict[first_ex].shape[:2]\n",
    "\n",
    "    # Store features and shapes\n",
    "    encoded_features = {}\n",
    "    spatial_shapes = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ex, data in data_dict.items():\n",
    "            print(f\"Extracting features for excitation {ex}...\")\n",
    "\n",
    "            # Create chunks\n",
    "            chunks, positions = create_chunks(data.numpy(), chunk_size, overlap)\n",
    "\n",
    "            # Initialize feature maps\n",
    "            all_features = None\n",
    "\n",
    "            # Process chunks\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Convert to tensor and add batch dimension\n",
    "                chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                # Create input dictionary for this excitation only\n",
    "                chunk_dict = {ex: chunk_tensor}\n",
    "\n",
    "                # Extract encoded representation\n",
    "                encoded = model.encode(chunk_dict)\n",
    "                features = encoded.cpu().numpy()[0]  # Remove batch dimension\n",
    "\n",
    "                # Initialize feature array on first chunk\n",
    "                if all_features is None:\n",
    "                    all_features = []\n",
    "                    for feat_idx in range(features.shape[0]):\n",
    "                        all_features.append(np.zeros((height, width)))\n",
    "\n",
    "                # Store features in appropriate positions\n",
    "                y_start, y_end, x_start, x_end = positions[i]\n",
    "\n",
    "                # Remove emission dimension (which is 1)\n",
    "                spatial_features = features.squeeze(1)\n",
    "\n",
    "                # Store features\n",
    "                for feat_idx in range(spatial_features.shape[0]):\n",
    "                    feature_chunk = spatial_features[feat_idx]\n",
    "                    current = all_features[feat_idx][y_start:y_end, x_start:x_end]\n",
    "\n",
    "                    # Handle overlapping regions\n",
    "                    overlap_mask = current != 0\n",
    "\n",
    "                    # Set new areas directly\n",
    "                    new_areas = ~overlap_mask\n",
    "                    current[new_areas] = feature_chunk[new_areas]\n",
    "\n",
    "                    # Average overlapping areas\n",
    "                    if np.any(overlap_mask):\n",
    "                        current[overlap_mask] = (current[overlap_mask] + feature_chunk[overlap_mask]) / 2\n",
    "\n",
    "                    all_features[feat_idx][y_start:y_end, x_start:x_end] = current\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 20 == 0 or i == len(chunks) - 1:\n",
    "                    print(f\"  Processed {i+1}/{len(chunks)} chunks\", end=\"\\r\")\n",
    "\n",
    "            # Stack features\n",
    "            features_array = np.stack(all_features)\n",
    "\n",
    "            # Store results\n",
    "            encoded_features[ex] = features_array\n",
    "            spatial_shapes[ex] = (height, width)\n",
    "\n",
    "            print(f\"\\nExtracted {features_array.shape[0]} features for excitation {ex}\")\n",
    "\n",
    "    return encoded_features, spatial_shapes\n",
    "\n",
    "def run_kmeans_clustering(features, n_clusters=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Run K-means clustering on the extracted features.\n",
    "    \"\"\"\n",
    "    print(f\"Running K-means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "    # Get shape of features\n",
    "    n_features, height, width = features.shape\n",
    "\n",
    "    # Reshape to [pixels, features]\n",
    "    features_reshaped = features.reshape(n_features, -1).T\n",
    "    print(f\"Feature matrix shape: {features_reshaped.shape}\")\n",
    "\n",
    "    # Apply PCA if dimensionality is very high\n",
    "    if n_features > 50:\n",
    "        from sklearn.decomposition import PCA\n",
    "        print(\"Applying PCA to reduce dimensions...\")\n",
    "        pca = PCA(n_components=min(50, n_features-1))\n",
    "        features_reshaped = pca.fit_transform(features_reshaped)\n",
    "        print(f\"Reduced features shape: {features_reshaped.shape}\")\n",
    "\n",
    "    # Use MiniBatchKMeans for better performance\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        batch_size=1000,\n",
    "        max_iter=300,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Fitting K-means model...\")\n",
    "    labels = kmeans.fit_predict(features_reshaped)\n",
    "\n",
    "    # Reshape labels back to spatial dimensions\n",
    "    cluster_map = labels.reshape(height, width)\n",
    "    print(f\"Clustering complete. Found {len(np.unique(labels))} unique clusters\")\n",
    "\n",
    "    return cluster_map, kmeans\n",
    "\n",
    "# Extract features\n",
    "cluster_dir = os.path.join(output_dir, \"clustering\")\n",
    "os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting feature extraction...\")\n",
    "encoded_features, spatial_shapes = extract_encoded_features(\n",
    "    model=model,\n",
    "    data_dict=all_data,\n",
    "    mask=dataset.processed_mask,\n",
    "    chunk_size=64,\n",
    "    overlap=8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Choose excitation for clustering (use first excitation by default)\n",
    "excitation_to_use = list(encoded_features.keys())[0]\n",
    "print(f\"Using excitation {excitation_to_use} for clustering\")\n",
    "\n",
    "# Run clustering\n",
    "cluster_labels, clustering_model = run_kmeans_clustering(\n",
    "    features=encoded_features[excitation_to_use],\n",
    "    n_clusters=10\n",
    ")\n",
    "\n",
    "# Apply mask to cluster labels\n",
    "if dataset.processed_mask is not None:\n",
    "    cluster_labels[dataset.processed_mask == 0] = -1\n",
    "\n",
    "# Visualize cluster map\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cluster_labels, cmap='tab10', interpolation='nearest')\n",
    "plt.colorbar(label='Cluster ID')\n",
    "plt.title(f'Pixel-wise Clustering (Ex={excitation_to_use}nm, K=10)')\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(cluster_dir, f\"cluster_map_ex{excitation_to_use}.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save cluster labels\n",
    "np.save(os.path.join(cluster_dir, f\"cluster_labels_ex{excitation_to_use}.npy\"), cluster_labels)\n",
    "\n",
    "# Visualize cluster overlay on RGB image\n",
    "def create_cluster_overlay(cluster_labels, rgb_image, alpha=0.5, output_path=None):\n",
    "    \"\"\"\n",
    "    Create overlay of cluster labels on RGB image.\n",
    "    \"\"\"\n",
    "    # Get unique clusters (excluding -1 which is for masked areas)\n",
    "    unique_clusters = sorted([c for c in np.unique(cluster_labels) if c >= 0])\n",
    "    n_clusters = len(unique_clusters)\n",
    "\n",
    "    # Create a colormap for clusters - FIX FOR DEPRECATION WARNING\n",
    "    # Replace plt.cm.get_cmap with plt.colormaps\n",
    "    cluster_cmap = plt.colormaps['tab10'].resampled(max(10, n_clusters))\n",
    "\n",
    "    # Create empty overlay (RGBA)\n",
    "    overlay = np.zeros((*cluster_labels.shape, 4))\n",
    "\n",
    "    # Fill with cluster colors\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        mask_cluster = cluster_labels == cluster_id\n",
    "        color = cluster_cmap(i % 10)\n",
    "        overlay[mask_cluster] = (*color[:3], alpha)\n",
    "\n",
    "    # Set transparent for masked areas\n",
    "    mask = cluster_labels < 0\n",
    "    overlay[mask] = (0, 0, 0, 0)\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Show RGB image\n",
    "    plt.imshow(rgb_image)\n",
    "\n",
    "    # Add overlay\n",
    "    plt.imshow(overlay, alpha=overlay[..., 3])\n",
    "\n",
    "    # Add colorbar - FIX FOR COLORBAR ERROR\n",
    "    # Get the current axes to pass to colorbar\n",
    "    ax = plt.gca()\n",
    "    sm = plt.cm.ScalarMappable(cmap=cluster_cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, ticks=np.arange(n_clusters))\n",
    "    cbar.set_ticklabels([f'Cluster {c}' for c in unique_clusters])\n",
    "\n",
    "    plt.title('Cluster Overlay on RGB Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save figure\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return overlay\n",
    "# Create cluster overlay\n",
    "if excitation_to_use in original_rgb:\n",
    "    overlay = create_cluster_overlay(\n",
    "        cluster_labels=cluster_labels,\n",
    "        rgb_image=original_rgb[excitation_to_use],\n",
    "        alpha=0.5,\n",
    "        output_path=os.path.join(cluster_dir, \"cluster_overlay.png\")\n",
    "    )"
   ],
   "id": "a2a69cd70dcc021c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction...\n",
      "Extracting features for excitation 300.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 300.0\n",
      "Extracting features for excitation 310.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 310.0\n",
      "Extracting features for excitation 320.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 320.0\n",
      "Extracting features for excitation 330.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 330.0\n",
      "Extracting features for excitation 340.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 340.0\n",
      "Extracting features for excitation 350.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 350.0\n",
      "Extracting features for excitation 360.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 360.0\n",
      "Extracting features for excitation 370.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 370.0\n",
      "Extracting features for excitation 380.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 380.0\n",
      "Extracting features for excitation 390.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 390.0\n",
      "Extracting features for excitation 400.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 400.0\n",
      "Extracting features for excitation 410.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 410.0\n",
      "Extracting features for excitation 420.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 420.0\n",
      "Extracting features for excitation 430.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 430.0\n",
      "Extracting features for excitation 440.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 440.0\n",
      "Extracting features for excitation 450.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 450.0\n",
      "Extracting features for excitation 460.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 460.0\n",
      "Extracting features for excitation 470.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 470.0\n",
      "Extracting features for excitation 480.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 480.0\n",
      "Extracting features for excitation 490.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 490.0\n",
      "Extracting features for excitation 500.0...\n",
      "Created 35 chunks of size up to 64x64 with 8 overlap\n",
      "  Processed 35/35 chunks\r\n",
      "Extracted 20 features for excitation 500.0\n",
      "Using excitation 300.0 for clustering\n",
      "Running K-means clustering with 10 clusters...\n",
      "Feature matrix shape: (89088, 20)\n",
      "Fitting K-means model...\n",
      "Clustering complete. Found 10 unique clusters\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:46:10.375218Z",
     "start_time": "2025-04-26T17:46:08.237545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_cluster_profiles(cluster_labels, all_data, emission_wavelengths):\n",
    "    \"\"\"\n",
    "    Analyze spectral profiles for each cluster.\n",
    "    \"\"\"\n",
    "    cluster_dir = os.path.join(output_dir, \"clustering\")\n",
    "\n",
    "    # Get unique clusters (excluding -1 which is for masked areas)\n",
    "    unique_clusters = sorted([c for c in np.unique(cluster_labels) if c >= 0])\n",
    "    print(f\"Analyzing profiles for {len(unique_clusters)} clusters\")\n",
    "\n",
    "    # Create figure for profiles\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Store stats for each cluster\n",
    "    cluster_stats = {}\n",
    "\n",
    "    # Process each excitation wavelength\n",
    "    for i, ex in enumerate(all_data.keys()):\n",
    "        # Get data for this excitation\n",
    "        if isinstance(all_data[ex], torch.Tensor):\n",
    "            data = all_data[ex].cpu().numpy()\n",
    "        else:\n",
    "            data = all_data[ex]\n",
    "\n",
    "        # Get emission wavelengths\n",
    "        if ex in emission_wavelengths:\n",
    "            wavelengths = emission_wavelengths[ex]\n",
    "        else:\n",
    "            wavelengths = np.arange(data.shape[2])\n",
    "\n",
    "        # Use different markers for each excitation\n",
    "        markers = ['o', 's', '^', 'D', 'v']\n",
    "        marker = markers[i % len(markers)]\n",
    "\n",
    "        # Process each cluster\n",
    "        for cluster_id in unique_clusters:\n",
    "            # Create mask for this cluster\n",
    "            mask = cluster_labels == cluster_id\n",
    "\n",
    "            # Skip if no pixels in this cluster\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            # Get data for this cluster\n",
    "            cluster_data = data[mask]\n",
    "\n",
    "            # Calculate mean spectrum (ignore NaNs)\n",
    "            mean_spectrum = np.nanmean(cluster_data, axis=0)\n",
    "\n",
    "            # Calculate standard deviation\n",
    "            std_spectrum = np.nanstd(cluster_data, axis=0)\n",
    "\n",
    "            # Store statistics\n",
    "            if cluster_id not in cluster_stats:\n",
    "                cluster_stats[cluster_id] = {}\n",
    "\n",
    "            cluster_stats[cluster_id][ex] = {\n",
    "                'mean': mean_spectrum,\n",
    "                'std': std_spectrum,\n",
    "                'count': np.sum(mask)\n",
    "            }\n",
    "\n",
    "            # Plot mean spectrum\n",
    "            if i == 0:  # Only add to legend for first excitation\n",
    "                plt.plot(wavelengths, mean_spectrum, marker=marker,\n",
    "                         label=f\"Cluster {cluster_id}\",\n",
    "                         color=plt.cm.tab10(cluster_id % 10))\n",
    "            else:\n",
    "                plt.plot(wavelengths, mean_spectrum, marker=marker,\n",
    "                         color=plt.cm.tab10(cluster_id % 10))\n",
    "\n",
    "    # Finish plot\n",
    "    plt.xlabel('Emission Wavelength (nm)' if len(emission_wavelengths) > 0 else 'Emission Band Index')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.title('Spectral Profiles by Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(cluster_dir, \"cluster_profiles.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Create bar chart of cluster sizes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Count pixels in each cluster\n",
    "    cluster_sizes = [np.sum(cluster_labels == c) for c in unique_clusters]\n",
    "\n",
    "    # Create bar chart\n",
    "    plt.bar(\n",
    "        [f\"Cluster {c}\" for c in unique_clusters],\n",
    "        cluster_sizes,\n",
    "        color=[plt.cm.tab10(c % 10) for c in unique_clusters]\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Pixels')\n",
    "    plt.title('Cluster Sizes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(cluster_dir, \"cluster_sizes.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return cluster_stats\n",
    "\n",
    "# Analyze cluster profiles\n",
    "cluster_stats = analyze_cluster_profiles(\n",
    "    cluster_labels=cluster_labels,\n",
    "    all_data=all_data,\n",
    "    emission_wavelengths=dataset.emission_wavelengths\n",
    ")\n",
    "\n",
    "print(\"Pipeline complete! All results saved to\", output_dir)"
   ],
   "id": "5dff802643ab32fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing profiles for 10 clusters\n",
      "Pipeline complete! All results saved to hyperspectral_results_lime\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6feb82ed666f610f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
