{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Data Clustering Example\n",
    "\n",
    "This notebook demonstrates how to use the hyperspectral convolutional autoencoder for clustering hyperspectral data, visualizing clusters, and interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our custom modules\n",
    "from hyperspectral_dataset import HyperspectralDataset, load_hyperspectral_data\n",
    "from hyperspectral_models import HyperspectralCAEVariable\n",
    "from hyperspectral_clustering import (\n",
    "    run_hyperspectral_clustering,\n",
    "    visualize_clustering_results,\n",
    "    extract_encoded_features,\n",
    "    prepare_features_for_clustering,\n",
    "    cluster_features,\n",
    "    map_clusters_to_image,\n",
    "    visualize_clusters_2d,\n",
    "    visualize_cluster_maps,\n",
    "    compare_cluster_maps\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Hyperspectral Data and Trained Model\n",
    "\n",
    "First, let's load our hyperspectral data and the trained autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the path to your data file\n",
    "data_path = \"../Data/Kiwi Experiment/pickles/masked_KiwiData.pkl\"\n",
    "\n",
    "# Load the data\n",
    "data_dict = load_hyperspectral_data(data_path)\n",
    "\n",
    "# Create dataset\n",
    "dataset = HyperspectralDataset(\n",
    "    data_dict,\n",
    "    normalize=True,  # Apply global normalization to [0,1]\n",
    "    downscale_factor=1  # Use full resolution (adjust based on memory constraints)\n",
    ")\n",
    "\n",
    "# Initialize the emission_wavelengths attribute if not already done\n",
    "if not hasattr(dataset, 'emission_wavelengths'):\n",
    "    dataset.emission_wavelengths = {}\n",
    "\n",
    "# Get all processed data\n",
    "all_data = dataset.get_all_data()\n",
    "spatial_height, spatial_width = dataset.get_spatial_dimensions()\n",
    "\n",
    "print(f\"Processed data dimensions: {spatial_height}x{spatial_width}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the trained model\n",
    "model = HyperspectralCAEVariable(\n",
    "    excitations_data={ex: data.numpy() for ex, data in all_data.items()},\n",
    "    k1=20,\n",
    "    k3=20,\n",
    "    filter_size=5,\n",
    "    sparsity_target=0.1,\n",
    "    sparsity_weight=1.0,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "# Load the weights from the trained model\n",
    "try:\n",
    "    # Try loading the best model\n",
    "    model.load_state_dict(torch.load(\"best_hyperspectral_model.pth\", map_location=device))\n",
    "    print(\"Loaded best model\")\n",
    "except:\n",
    "    try:\n",
    "        # Try loading the final model\n",
    "        model.load_state_dict(torch.load(\"hyperspectral_cae_final_model.pth\", map_location=device))\n",
    "        print(\"Loaded final model\")\n",
    "    except:\n",
    "        print(\"No saved model found. Please train the model first.\")\n",
    "        \n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Encoded Features for Clustering\n",
    "\n",
    "Now, let's extract the encoded features from the trained model to use for clustering."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract encoded features\n",
    "encoded_features, spatial_shapes = extract_encoded_features(model, all_data, device)\n",
    "\n",
    "# Print encoded feature shapes\n",
    "print(\"Encoded feature shapes:\")\n",
    "for ex, features in encoded_features.items():\n",
    "    print(f\"  Excitation {ex}nm: {features.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means Clustering\n",
    "\n",
    "Let's try K-means clustering on the encoded features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Define clustering configuration for K-means\n",
    "kmeans_config = {\n",
    "    'method': 'kmeans',\n",
    "    'n_clusters': 5,\n",
    "    'combine_excitations': True,\n",
    "    'reduction_method': 'pca',\n",
    "    'n_components': 8\n",
    "}\n",
    "\n",
    "# Run the clustering pipeline\n",
    "kmeans_results = run_hyperspectral_clustering(\n",
    "    model,\n",
    "    dataset,\n",
    "    clustering_config=kmeans_config,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the K-means clustering results\n",
    "kmeans_visualizations = visualize_clustering_results(\n",
    "    kmeans_results,\n",
    "    original_data_dict=all_data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DBSCAN Clustering (Density-Based)\n",
    "\n",
    "Now let's try DBSCAN clustering, which can find clusters of arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define clustering configuration for DBSCAN\n",
    "dbscan_config = {\n",
    "    'method': 'dbscan',\n",
    "    'eps': 0.5,  # The maximum distance between two samples to be considered in the same neighborhood\n",
    "    'min_samples': 10,  # The number of samples in a neighborhood for a point to be considered a core point\n",
    "    'combine_excitations': True,\n",
    "    'reduction_method': 'pca',\n",
    "    'n_components': 8\n",
    "}\n",
    "\n",
    "# Run the clustering pipeline\n",
    "dbscan_results = run_hyperspectral_clustering(\n",
    "    model,\n",
    "    dataset,\n",
    "    clustering_config=dbscan_config,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the DBSCAN clustering results\n",
    "dbscan_visualizations = visualize_clustering_results(\n",
    "    dbscan_results,\n",
    "    original_data_dict=all_data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Mixture Model Clustering (GMM)\n",
    "\n",
    "GMM clustering assumes that the data points are generated from a mixture of several Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define clustering configuration for GMM\n",
    "gmm_config = {\n",
    "    'method': 'gmm',\n",
    "    'n_clusters': 6,  # Number of Gaussian components\n",
    "    'combine_excitations': True,\n",
    "    'reduction_method': 'pca',\n",
    "    'n_components': 8\n",
    "}\n",
    "\n",
    "# Run the clustering pipeline\n",
    "gmm_results = run_hyperspectral_clustering(\n",
    "    model,\n",
    "    dataset,\n",
    "    clustering_config=gmm_config,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the GMM clustering results\n",
    "gmm_visualizations = visualize_clustering_results(\n",
    "    gmm_results,\n",
    "    original_data_dict=all_data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Separate Clustering for Each Excitation Wavelength\n",
    "\n",
    "Instead of combining all excitations, we can also apply clustering separately to each excitation wavelength."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define clustering configuration for separate excitations\n",
    "separate_config = {\n",
    "    'method': 'kmeans',\n",
    "    'n_clusters': 4,\n",
    "    'combine_excitations': False,  # Process each excitation separately\n",
    "    'reduction_method': 'pca',\n",
    "    'n_components': 5\n",
    "}\n",
    "\n",
    "# Run the clustering pipeline\n",
    "separate_results = run_hyperspectral_clustering(\n",
    "    model,\n",
    "    dataset,\n",
    "    clustering_config=separate_config,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the separate clustering results\n",
    "separate_visualizations = visualize_clustering_results(\n",
    "    separate_results,\n",
    "    original_data_dict=all_data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Clustering Methods\n",
    "\n",
    "Let's compare the results of the different clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to print clustering metrics\n",
    "def print_clustering_metrics(results, method_name):\n",
    "    print(f\"\\n--- {method_name} Clustering Metrics ---\")\n",
    "    \n",
    "    quality_metrics = results['quality_metrics']\n",
    "    \n",
    "    if isinstance(quality_metrics, dict):\n",
    "        # Print metrics for each excitation\n",
    "        for ex, metrics in quality_metrics.items():\n",
    "            print(f\"\\nExcitation {ex}nm:\")\n",
    "            for metric, value in metrics.items():\n",
    "                if not np.isnan(value):\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        # Print overall metrics\n",
    "        for metric, value in quality_metrics.items():\n",
    "            if not np.isnan(value):\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Print metrics for each clustering method\n",
    "print_clustering_metrics(kmeans_results, \"K-means\")\n",
    "print_clustering_metrics(dbscan_results, \"DBSCAN\")\n",
    "print_clustering_metrics(gmm_results, \"GMM\")\n",
    "print_clustering_metrics(separate_results, \"Separate K-means\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a visual comparison of the different clustering methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Choose an excitation wavelength to compare\n",
    "ex_to_compare = list(all_data.keys())[0]  # First excitation\n",
    "\n",
    "# Define colormap\n",
    "cmap = 'tab10'\n",
    "\n",
    "# K-means\n",
    "if ex_to_compare in kmeans_results['cluster_maps']:\n",
    "    im1 = axes[0, 0].imshow(kmeans_results['cluster_maps'][ex_to_compare], cmap=cmap)\n",
    "    axes[0, 0].set_title(f\"K-means ({kmeans_config['n_clusters']} clusters)\")\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# DBSCAN\n",
    "if ex_to_compare in dbscan_results['cluster_maps']:\n",
    "    im2 = axes[0, 1].imshow(dbscan_results['cluster_maps'][ex_to_compare], cmap=cmap)\n",
    "    n_clusters_dbscan = len(np.unique(dbscan_results['cluster_maps'][ex_to_compare]))\n",
    "    axes[0, 1].set_title(f\"DBSCAN ({n_clusters_dbscan} clusters)\")\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# GMM\n",
    "if ex_to_compare in gmm_results['cluster_maps']:\n",
    "    im3 = axes[1, 0].imshow(gmm_results['cluster_maps'][ex_to_compare], cmap=cmap)\n",
    "    axes[1, 0].set_title(f\"GMM ({gmm_config['n_clusters']} clusters)\")\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "# Separate K-means\n",
    "if ex_to_compare in separate_results['cluster_maps']:\n",
    "    im4 = axes[1, 1].imshow(separate_results['cluster_maps'][ex_to_compare], cmap=cmap)\n",
    "    axes[1, 1].set_title(f\"Separate K-means ({separate_config['n_clusters']} clusters)\")\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle(f\"Comparison of Clustering Methods for Excitation {ex_to_compare}nm\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Dive into the Best Clustering Method\n",
    "\n",
    "Based on the metrics, let's perform a deeper analysis on the most promising clustering method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Choose the best method based on the metrics (assuming K-means for this example)\n",
    "best_results = kmeans_results\n",
    "best_method = \"K-means\"\n",
    "\n",
    "# Extract cluster maps\n",
    "cluster_maps = best_results['cluster_maps']\n",
    "\n",
    "# For each excitation, analyze the spectral signature of each cluster\n",
    "for ex in all_data.keys():\n",
    "    if ex in cluster_maps:\n",
    "        print(f\"\\n=== Analyzing Clusters for Excitation {ex}nm ===\")\n",
    "        \n",
    "        # Get original data and cluster map\n",
    "        original_data = all_data[ex].numpy()\n",
    "        cluster_map = cluster_maps[ex]\n",
    "        \n",
    "        # Get unique clusters\n",
    "        unique_clusters = np.unique(cluster_map)\n",
    "        print(f\"Found {len(unique_clusters)} unique clusters\")\n",
    "        \n",
    "        # Calculate average spectrum for each cluster\n",
    "        cluster_spectra = {}\n",
    "        cluster_sizes = {}\n",
    "        \n",
    "        for cluster_id in unique_clusters:\n",
    "            # Create mask for this cluster\n",
    "            mask = cluster_map == cluster_id\n",
    "            \n",
    "            # Count pixels in this cluster\n",
    "            size = np.sum(mask)\n",
    "            cluster_sizes[cluster_id] = size\n",
    "            \n",
    "            # Calculate mean spectrum for this cluster\n",
    "            cluster_spectrum = np.mean(original_data[mask], axis=0)\n",
    "            cluster_spectra[cluster_id] = cluster_spectrum\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {size} pixels ({size/mask.size*100:.1f}% of total)\")\n",
    "        \n",
    "        # Plot average spectra for each cluster\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for cluster_id, spectrum in cluster_spectra.items():\n",
    "            plt.plot(range(len(spectrum)), spectrum, label=f\"Cluster {cluster_id} ({cluster_sizes[cluster_id]} pixels)\")\n",
    "        \n",
    "        plt.title(f\"{best_method} Clustering: Average Spectra for Excitation {ex}nm\")\n",
    "        plt.xlabel(\"Emission Band Index\")\n",
    "        plt.ylabel(\"Mean Intensity (Normalized)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Finding Optimal Number of Clusters\n",
    "\n",
    "Let's determine the optimal number of clusters for K-means."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare features for clustering\n",
    "prepared_features, excitation_indices, excitation_wavelengths = prepare_features_for_clustering(\n",
    "    encoded_features,\n",
    "    combine_excitations=True,\n",
    "    reduction_method='pca',\n",
    "    n_components=8\n",
    ")\n",
    "\n",
    "# Try different numbers of clusters\n",
    "k_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"Testing k={k}...\")\n",
    "    # Apply K-means\n",
    "    cluster_labels, _ = cluster_features(prepared_features, method='kmeans', n_clusters=k)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    metrics = evaluate_clustering(prepared_features, cluster_labels)\n",
    "    \n",
    "    # Save scores\n",
    "    silhouette_scores.append(metrics['silhouette'])\n",
    "    davies_bouldin_scores.append(metrics['davies_bouldin'])\n",
    "    calinski_harabasz_scores.append(metrics['calinski_harabasz'])\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ax1.plot(k_range, silhouette_scores, 'o-')\n",
    "ax1.set_title('Silhouette Score (higher is better)')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(k_range, davies_bouldin_scores, 'o-')\n",
    "ax2.set_title('Davies-Bouldin Score (lower is better)')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3.plot(k_range, calinski_harabasz_scores, 'o-')\n",
    "ax3.set_title('Calinski-Harabasz Score (higher is better)')\n",
    "ax3.set_xlabel('Number of Clusters (k)')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal k based on silhouette score\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters based on Silhouette Score: {optimal_k}\")\n",
    "\n",
    "# Determine optimal k based on Davies-Bouldin score\n",
    "optimal_k_db = k_range[np.argmin(davies_bouldin_scores)]\n",
    "print(f\"Optimal number of clusters based on Davies-Bouldin Score: {optimal_k_db}\")\n",
    "\n",
    "# Determine optimal k based on Calinski-Harabasz score\n",
    "optimal_k_ch = k_range[np.argmax(calinski_harabasz_scores)]\n",
    "print(f\"Optimal number of clusters based on Calinski-Harabasz Score: {optimal_k_ch}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Clustering with Optimal Parameters\n",
    "\n",
    "Based on the analysis, let's run the clustering with the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define clustering configuration with optimal parameters\n",
    "optimal_config = {\n",
    "    'method': 'kmeans',\n",
    "    'n_clusters': optimal_k,  # Use the optimal k determined above\n",
    "    'combine_excitations': True,\n",
    "    'reduction_method': 'pca',\n",
    "    'n_components': 8\n",
    "}\n",
    "\n",
    "# Run the clustering pipeline\n",
    "optimal_results = run_hyperspectral_clustering(\n",
    "    model,\n",
    "    dataset,\n",
    "    clustering_config=optimal_config,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the optimal clustering results\n",
    "optimal_visualizations = visualize_clustering_results(\n",
    "    optimal_results,\n",
    "    original_data_dict=all_data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Cluster Maps for Further Analysis\n",
    "\n",
    "Save the cluster maps for further analysis or integration with other tools."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export cluster maps to numpy files\n",
    "output_dir = Path(\"cluster_maps\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save optimal clustering results\n",
    "for ex, cluster_map in optimal_results['cluster_maps'].items():\n",
    "    output_file = output_dir / f\"cluster_map_{ex}nm.npy\"\n",
    "    np.save(output_file, cluster_map)\n",
    "    print(f\"Saved cluster map for excitation {ex}nm to {output_file}\")\n",
    "\n",
    "# Save clustering configuration\n",
    "config_file = output_dir / \"clustering_config.npy\"\n",
    "np.save(config_file, optimal_results['config'])\n",
    "print(f\"Saved clustering configuration to {config_file}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary of Clustering Results\n",
    "\n",
    "Let's summarize the findings from our clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print summary of clustering results\n",
    "print(\"\\n=== Summary of Hyperspectral Clustering Results ===\")\n",
    "print(f\"\\nOptimal Clustering Method: K-means with {optimal_k} clusters\")\n",
    "print(f\"Dimensionality Reduction: PCA with 8 components\")\n",
    "\n",
    "# Print cluster distributions\n",
    "print(\"\\nCluster Distributions:\")\n",
    "for ex in optimal_results['cluster_maps']:\n",
    "    cluster_map = optimal_results['cluster_maps'][ex]\n",
    "    unique, counts = np.unique(cluster_map, return_counts=True)\n",
    "    percentages = counts / np.sum(counts) * 100\n",
    "    \n",
    "    print(f\"\\nExcitation {ex}nm:\")\n",
    "    for cluster_id, count, percentage in zip(unique, counts, percentages):\n",
    "        print(f\"  Cluster {cluster_id}: {count} pixels ({percentage:.1f}%)\")\n",
    "\n",
    "# Print quality metrics\n",
    "print(\"\\nClustering Quality Metrics:\")\n",
    "if isinstance(optimal_results['quality_metrics'], dict):\n",
    "    for ex, metrics in optimal_results['quality_metrics'].items():\n",
    "        print(f\"\\nExcitation {ex}nm:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if not np.isnan(value):\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "else:\n",
    "    for metric, value in optimal_results['quality_metrics'].items():\n",
    "        if not np.isnan(value):\n",
    "            print(f\"  {metric}: {value:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
